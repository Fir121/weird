[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "That’s weird! Anomaly detection using R",
    "section": "",
    "text": "Preface\n\nThis book is under development. Please don’t ask me when it will be finished — I have to fit this around my other work. In the meantime, I hope the incomplete book is useful.\n\nThis book is about tools and techniques for finding and understanding anomalies. We will begin with some simple data sets containing only one variable, and build up slowly to much more complicated data. We will cover popular but inadvisable methods to identify anomalies (pointing out their shortcomings), as well as more reliable and recommended approaches.\nThe book is written for two audiences: (1) people finding themselves doing data analysis without having had formal training in anomaly detection; and (2) students studying statistics or data science.\nI will assume a knowledge of statistics and probability at about second year undergraduate level. So if you’ve done a couple of statistics subjects, and are familiar with multiple regression, hypothesis tests and probability distributions, then you should be prepared for what follows. I will also assume some basic knowledge of matrix algebra; some of the matrix results we use are provided in Appendix A — Matrix algebra.\n\nR\nI will also assume that you already know how to use R and are familiar with the tidyverse packages such as dplyr, tidyr and ggplot2.\nAll R examples in the book assume that you have loaded the weird package first. It is available on CRAN, although many examples in the book use the latest dev version available on GitHub.\n\n# remotes::install_github(\"robjhyndman/weird-package\")\nlibrary(weird)\n\nThis will load the relevant data sets, and various other packages needed to reproduce the examples.\n \nRob J Hyndman  December 2024",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Definitions\nData analysis is about finding the stories hidden in the mass of information that make up a data set. Usually we are interested in understanding the major patterns — the relationships that hold true for most of the data. But sometimes we need to look at the weird observations — those that don’t follow the crowd and behave differently. We call these weird observations “anomalies”. They are the mavericks in the data, and in this book, they are what we find interesting.\nAnomalies are often a nuisance. For example, they may be simply recording errors that are contaminating our data. We want to find them, remove them, and get on with analysing the bulk of the data. At other times, anomalies are the observations we care about. They tell us things that might otherwise go unnoticed if we only consider how the majority of observations behave. Whether we want to remove them or study them, we first need to find them.\nAn anomaly is an observation that behaves differently from the bulk of the data. Anomalies are also called “outliers”, “novelties”, “deviants”, “abnormalities” or “discordants”. I prefer to use “anomalies” as it is a more general term than outliers, and it is more widely used than the other options.\nAn old definition due to Barnett and Lewis (1978) states\nHawkins (1980) defined it like this:\nA more mathematical definition is that anomalies are observations that come from a different probability distribution than the majority of observations. However, that does not help much as it is always possible to define a mixture probability distribution which includes several component distributions. There are also some probability distributions with “heavy tails”, so genuine observations can occur a long way from the bulk of the data.\nSo rather than trying to define anomalies more precisely, we shall deliberately proceed with the vague and subjective approach where anomalies are suspiciously different or inconsistent with the rest of the data.\nI will sometimes use the term “outlier” to mean an anomaly that lies at the outer edges of a data set. Other anomalies are actually “inliers”, and lie well within the data set. Some papers use the term “inlier” to denote any point that is not an outlier. However, we shall use the earlier definition of “inlier”, meaning an observation that is an anomaly but which is surrounded by genuine observations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#definitions",
    "href": "01-intro.html#definitions",
    "title": "1  Introduction",
    "section": "",
    "text": "an outlier in a set of data [is] an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data.\n\n\n\nAn outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\n\n\n\n\n\nRejecting anomalies\nMany analysts have thrown away apparent anomalies because they were assumed to be errors. There are even formal statistical tests to tell you which observations should be ignored. In my view, this is a dangerous practice. No anomalies should be removed unless you understand what led to their unusual behaviour in the first place. Often the most interesting observations are the ones that appear anomalous. They are the ones that tell us something new and unexpected, so we should not be too quick to reject them.\nSometimes we may want to use methods that are robust to anomalies, so that we can ignore them without worrying about their effect on the analysis. But this is not the same as removing them. Even when we use robust methods, it is still worth investigating any observations that appear anomalous, in order to fully understand the variation in the data.\n\n\nHow NASA didn’t discover the hole in the ozone layer\nThere is a widespread myth that NASA did not discover the hole in the ozone layer above the Antarctic because they had been throwing away anomalous data that would have revealed it. This is not true, but the real story is also instructive (Pukelsheim 1990; Christie 2001, 2004).\nThe “ozone hole” refers to low levels of ozone in the atmosphere above Antarctica and surrounding areas. NASA have been collecting satellite data on Antarctic ozone levels using a Total Ozone Mapping Spectrometer (TOMS) since 1979, while British scientists have collected ozone data using ground sensors at the Halley Research Station, on the edge of the Brunt Ice Shelf in Antarctica, since 1957. Figure 1.1 shows average daily values from the NASA measurements in blue, and from the British observations in orange. There is a clear downward trend in the British data, especially from the late 1970s, which is confirmed with the NASA data. So why wasn’t the “ozone hole” discovered until 1985?\n\n\nCode\ntmp &lt;- readr::read_csv(here::here(\"data/halley_toms_ozone.txt\"), skip = 2) |&gt;\n  rename(\n    Year = \"#   year\",\n    Instrument = \"instrument\",\n    Ozone = \"ozone\"\n  )\nozone &lt;- bind_rows(\n  tmp |&gt;\n    filter(!is.na(Instrument)),\n  tmp |&gt;\n    filter(is.na(Instrument), !stringr::str_detect(Year, \"^#\")) |&gt;\n    mutate(Instrument = \"Halley\")\n) |&gt;\n  mutate(\n    Year = readr::parse_integer(Year),\n    Ozone = readr::parse_number(Ozone)\n  )\nozone |&gt;\n  filter(Instrument %in% c(\"Halley\", \"TOMS\")) |&gt;\n  ggplot(aes(x = Year, y = Ozone, color = Instrument)) +\n  geom_point() +\n  geom_hline(aes(yintercept = 180), col = \"gray\") +\n  labs(y = \"Total Ozone (DU)\") +\n  scale_y_continuous(sec.axis = dup_axis(breaks = 180, name = \"\"))\n\n\n\n\n\n\n\n\nFigure 1.1: Observations of Antarctic ozone levels since 1957, measured in Dobson units (DU). Observations are mean daily values from October each year. Ground observations are from Halley Research Station, while satellite observations were obtained using a Total Ozone Mapping Spectrometer (TOMS). The satellite data were obtained from Leslie R Lait (NASA), while the Halley ground observations were obtained from Jonathan Shanklin (British Antarctic Survey). The horizontal line shows the threshold of 180 DU, used by NASA to determine when the ozone level was unusually low.\n\n\n\n\n\nThe British scientists had noticed the low ozone values as early as 1981, but it took a few years for the scientists to be convinced that the low values were real and not due to instrument problems, and then there were the usual publication delays. Eventually, the results were published in Farman, Gardiner, and Shanklin (1985).\nMeanwhile, NASA was flagging observations as anomalous when they were below 180 DU (shown as a horizontal line in Figure 1.1). As is clear from the figure, this is much lower than any of the plotted points before the early 1980s. However, the 180 threshold was used for the daily measurements, which are much more variable than the monthly averages that are plotted. Occasionally daily observations did fall below 180, and so it was a reasonable threshold for the purpose of identifying instrument problems.\nIn fact, NASA had checked the unusually low TOMS values obtained before 1985 by comparing them against other available data. But the other data available to them showed ozone values of about 300 DU, so it was assumed that the satellite sensor was malfunctioning. The British Halley data were not available to them, and only after the publication of Farman, Gardiner, and Shanklin (1985) did the NASA scientists realise that the TOMS results were accurate.\nIn 1986, NASA scientists were able to confirm the British finding, also demonstrating that the ozone hole was widespread across the Antarctic (Stolarski et al. 1986).\nThis example reveals some lessons about anomaly detection:\n\nThe NASA threshold of 180 was based on daily data, and was designed to identify instrument problems, not genuine systematic changes in ozone levels. The implicit assumption was that ozone levels varied seasonally, but that otherwise the distribution of observations was stable. All anomaly detection involves some implicit assumptions like this, and it is well to be aware of them.\nSometimes what we think are anomalies are not really anomalies, but the result of incorrect assumptions.\nOften smoothing or averaging data will help to reveal issues that are not so obvious from the original data. This reduces the variation in the data, and allows more systematic variation to be uncovered.\nAlways plot the data. In this case, a graph such as Figure 1.1 would have revealed the problem in the late 1970s, but it seems no-one was producing plots like this.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#approaches-to-anomaly-detection",
    "href": "01-intro.html#approaches-to-anomaly-detection",
    "title": "1  Introduction",
    "section": "1.2 Approaches to anomaly detection",
    "text": "1.2 Approaches to anomaly detection\nWe will take a probabilistic perspective on anomaly detection, where we aim to estimate the likelihood of each observation. Consequently, we will need to introduce (or revise) some material on probability distributions and related tools in Chapter 2 and Chapter 3.\n\nZ-scores and statistical tests\nThe early days of anomaly detection involved methods based on statistical tests. Typically, the data were assumed to come from some underlying probability distribution, and then observations were declared outliers if they appeared to be inconsistent with this assumption. By far the most popular of these methods are based on the Normal distribution and use z-scores. Even today, such methods are extremely widely used. These are discussed in Chapter 4.\n\n\nBoxplot methods\nThe second half of the 20th century witnessed the advent of computing, which allowed more sophisticated exploratory data analysis to be developed, including boxplots. Most readers will be familiar with boxplots, which provide a simple and effective way to visualise the distribution of a variable, and are often also used for anomaly detection. Such methods are discussed in Chapter 5.\n\n\nDistance and density-based methods\nModern tools for anomaly detection can be roughly divided into two groups: those based on densities and those based on distances. Distance-based tools compute the pairwise distances between observations and identify anomalies as those points that are far from other points. Density-based tools compute the probability density at each observation and identify anomalies as those points with very low density. The two ideas are closely related because density estimates are usually based on distances to other observations; for example, a density estimate at a point may be a function of the distances between that point and all other observations.\nDensity-based methods are discussed in Chapter 6, while distance-based methods are the subject of Chapter 7.\n\n\nStatistical vs machine learning approaches\nAnomaly detection has a long history in the statistics literature, including all of the approaches developed in the 20th century, and many of the newer density-based approaches.\nRecently, computer scientists have turned their attention to the problem of anomaly detection, especially as data sets have grown in size and complexity. Anomaly detection arises in cyber security systems (where intrusions are identified as anomalies), in credit-card fraud (where unusual buying patterns are anomalous) and in remote sensing (where unusual changes in land use are anomalies). Machine-learning approaches have been developed to address these problems, mostly using distanced-based methods, although some important contributions to the density-based methods have also appeared in the machine learning literature.\nIn this book, I make no attempt to distinguish methods as “statistical” or “machine-learning”. The distinction is largely about the training of the researchers, or the journals in which they publish, and has little to do with the methods themselves. In any case, the two communities have been slowly moving closer together, with computer scientists taking a more probabilistic perspective than previously, and statisticians taking a more computational and algorithmic approach than their forebears. Now there is now considerable interaction and overlap between these communities, and continuing to label methods as “statistical” or “machine-learning” is unhelpful.\n\n\nSupervised vs unsupervised approaches\nSome books and papers distinguish “supervised anomaly detection” from “unsupervised anomaly detection”. In the former case, anomalies are identified using human input and then a model or algorithm is employed to learn how to identify new anomalies contained within new data. I do not regard this as anomaly detection and it will not be covered in this book. It is possible, for example, that what a human labels as “anomalous” is not anomalous in a statistical sense. What is called “supervised anomaly detection” is actually a classification problem, where the aim is to mimic the human who labelled the training set. In that case, different tools are employed.\nIn this book, we will only consider unsupervised problems. That is, we have no idea a priori what observations are anomalous, and there is no “right” answer.\n\n\nTesting anomaly detection algorithms\nThat makes it difficult to measure how good an anomaly detection algorithm performs. If we don’t know the right answer, we can’t know if an algorithm has found the true anomalies. There are three solutions to this problem that are often employed when testing anomaly detection algorithms.\n\nSynthetic data: We can use synthetic data that has been deliberately contaminated with a small number of known anomalies. This approach allows us to study the sensitivity of an algorithm to anomalies — how anomalous does an observation have to be before it is detected? A drawback to this approach is that synthetic data is often simpler and neater than real data, making the anomaly detection task easier than it is in reality.\nHuman labelled: We can use real data where a human has labelled some of the observations as anomalous. As noted above, this only works when the human has correctly and completely labelled the anomalies. It also requires that what a human considers an anomaly is also anomalous in a statistical sense.\nDownsampled category: We can use real data that contains a categorical variable taking two label values. One of the labelled categories is downsampled and the corresponding observations form the anomalous subset. This approach is useful in obtaining large sets of realistic data, as it takes little time to create the data set. However, it requires that the observations labelled anomalies are different in some way from the other observations (apart from their label).\n\nIn this book, we will use data sets of each type when testing different anomaly detection methods. Section 1.4 discusses the main examples we will be using throughout the book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#spurious-anomalies-and-undetected-anomalies",
    "href": "01-intro.html#spurious-anomalies-and-undetected-anomalies",
    "title": "1  Introduction",
    "section": "1.3 Spurious anomalies and undetected anomalies",
    "text": "1.3 Spurious anomalies and undetected anomalies\nSpurious anomalies occur when a true observation is identified as anomalous, while undetected anomalies occur when an anomalous observation is not detected. Thus, when our purpose is anomaly detection, an undetected anomaly is a “false negative” while a spurious anomaly is a “false positive”. We can only really be sure about which observations are spurious or undetected anomalies when the data are synthetic. However, in some real data examples, there are such extreme anomalies that we will proceed under the assumption that we know that these are true anomalies.\nIn some applications, we will be less interested in detecting anomalies than in ranking observations according to their degree of “anomalousness”. For example, we may have a limited team of human analysts who can investigate a small number of observations, and we want to make sure that they investigate the most anomalous observations. Then it is important to compute an anomaly score, so we can rank the observations according to their scores, and our analysts can look at the observations with the highest scores.\nIn fact, anomaly scores often underpin the algorithms that are used to detect anomalies. The algorithms compute a score for each observation, and then identify the observations with scores above some threshold as the anomalies. So we will often be talking about anomaly scores, even when we are really interested in detecting anomalies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-data",
    "href": "01-intro.html#sec-data",
    "title": "1  Introduction",
    "section": "1.4 Data sets",
    "text": "1.4 Data sets\n\nTest cricket batting data\nCricket is a popular sport in England, India, Australia, and other countries which had strong ties to England during the 19th and 20th centuries. The cricket_batting data contains summary statistics for all 3754 men and women to have played test cricket (the traditional and long form of the game), up to 6 October 2021. Each row contains data on one player, and we will be looking for anomalies in their playing statistics.\n\ncricket_batting\n\n#&gt; # A tibble: 3,754 × 15\n#&gt;    Player       Country   Start   End Matches Innings NotOuts  Runs HighScore\n#&gt;    &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n#&gt;  1 BB Cooper    Australia  1877  1877       1       2       0    18        15\n#&gt;  2 C Bannerman  Australia  1877  1879       3       6       2   239       165\n#&gt;  3 DW Gregory   Australia  1877  1879       3       5       2    60        43\n#&gt;  4 EJ Gregory   Australia  1877  1877       1       2       0    11        11\n#&gt;  5 FR Spofforth Australia  1877  1887      18      29       6   217        50\n#&gt;  6 JM Blackham  Australia  1877  1894      35      62      11   800        74\n#&gt;  7 JR Hodges    Australia  1877  1877       2       4       1    10         8\n#&gt;  8 NFD Thomson  Australia  1877  1877       2       4       0    67        41\n#&gt;  9 TJD Kelly    Australia  1877  1879       2       3       0    64        35\n#&gt; 10 TK Kendall   Australia  1877  1877       2       4       1    39        17\n#&gt; # ℹ 3,744 more rows\n#&gt; # ℹ 6 more variables: HighScoreNotOut &lt;lgl&gt;, Average &lt;dbl&gt;, Hundreds &lt;int&gt;,\n#&gt; #   Fifties &lt;int&gt;, Ducks &lt;int&gt;, Gender &lt;chr&gt;\n\n\n\n\nOld Faithful Geyser eruptions\nData on the eruptions of the Old Faithful Geyser in Yellowstone National Park, Wyoming, USA, have been collected for about 150 years. It was named “Old Faithful” due to the relatively predictable timing and length of its eruptions. The oldfaithful data set contains data on all 2261 eruptions recorded between 1 January 2015 and 1 October 2021.\n\noldfaithful\n\n#&gt; # A tibble: 2,261 × 3\n#&gt;    time                duration waiting\n#&gt;    &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 2015-01-02 14:53:00      271    5040\n#&gt;  2 2015-01-09 23:55:00      247    6060\n#&gt;  3 2015-02-07 00:49:00      203    5460\n#&gt;  4 2015-02-14 01:09:00      195    5221\n#&gt;  5 2015-02-21 01:12:00      210    5401\n#&gt;  6 2015-02-28 01:11:00      185    5520\n#&gt;  7 2015-03-07 00:50:00      160    5281\n#&gt;  8 2015-03-13 21:57:00      226    6000\n#&gt;  9 2015-03-13 23:37:00      190    5341\n#&gt; 10 2015-03-20 22:26:00      102    3961\n#&gt; # ℹ 2,251 more rows\n\n\nThe time stamp indicates the start time of each eruption, with the other variables providing the duration of each eruption and the time to the next eruption (both in seconds).\n\n\nWine prices and quality\nThe wine_reviews data set contains data on 110,203 wines from 44 countries, taken from the Wine Enthusiast Magazine during the week of 15 June 2017.\n\nwine_reviews &lt;- fetch_wine_reviews()\n\n\nwine_reviews\n\n#&gt; # A tibble: 110,203 × 8\n#&gt;    country  state             region         winery variety points price  year\n#&gt;    &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Portugal Douro             &lt;NA&gt;           Quint… Portug…     87    15  2011\n#&gt;  2 US       Oregon            Willamette Va… Rains… Pinot …     87    14  2013\n#&gt;  3 US       Michigan          Lake Michigan… St. J… Riesli…     87    13  2013\n#&gt;  4 US       Oregon            Willamette Va… Sweet… Pinot …     87    65  2012\n#&gt;  5 Spain    Northern Spain    Navarra        Tandem Tempra…     87    15  2011\n#&gt;  6 Italy    Sicily & Sardinia Vittoria       Terre… Frappa…     87    16  2013\n#&gt;  7 France   Alsace            Alsace         Trimb… Gewürz…     87    24  2012\n#&gt;  8 Germany  Rheinhessen       &lt;NA&gt;           Heinz… Gewürz…     87    12  2013\n#&gt;  9 France   Alsace            Alsace         Jean-… Pinot …     87    27  2012\n#&gt; 10 US       California        Napa           Kirkl… Cabern…     87    19  2011\n#&gt; # ℹ 110,193 more rows\n\n\nThe points variable provides a measure of wine quality based on a taster’s assessment on a scale of 0 to 100. The price is provided in $US. Other variables indicate the location of the winery and year of harvest.\n\n\nSynthetic standard normal data\nThe n01 data set contains synthetic data on 10 variables, each generated independently from a Normal distribution with mean zero and variance one. So by definition, this data set contains no anomalies. We will add anomalies to the data when testing algorithms, in order to check that the algorithms can identify the artificial anomalies.\n\nn01\n\n#&gt; # A tibble: 1,000 × 10\n#&gt;        v1      v2      v3      v4     v5     v6      v7     v8     v9     v10\n#&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 -0.626  1.13   -0.886   0.739  -1.13  -1.52  -0.619  -1.33   0.264 -1.22  \n#&gt;  2  0.184  1.11   -1.92    0.387   0.765  0.629 -1.11    0.952 -0.829 -0.946 \n#&gt;  3 -0.836 -0.871   1.62    1.30    0.571 -1.68  -2.17    0.860 -1.46   0.0914\n#&gt;  4  1.60   0.211   0.519  -0.804  -1.35   1.18  -0.0313  1.06   1.68   0.701 \n#&gt;  5  0.330  0.0694 -0.0558 -1.60   -2.03   1.12  -0.260  -0.351 -1.54   0.673 \n#&gt;  6 -0.820 -1.66    0.696   0.933   0.590 -1.24   0.534  -0.131 -0.191  1.27  \n#&gt;  7  0.487  0.811   0.0535  1.81   -1.41  -1.23  -0.559   0.764  1.02  -1.45  \n#&gt;  8  0.738 -1.91   -1.31   -0.0565  1.61   0.598  1.61   -0.494  0.547  1.42  \n#&gt;  9  0.576 -1.25   -2.12    1.89    1.84   0.299  0.557   1.11   0.755 -1.59  \n#&gt; 10 -0.305  0.998  -0.208   1.58    1.37  -0.110  0.186   1.46  -0.420  0.246 \n#&gt; # ℹ 990 more rows",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-scatterplots",
    "href": "01-intro.html#sec-scatterplots",
    "title": "1  Introduction",
    "section": "1.5 Strip plots and scatterplots",
    "text": "1.5 Strip plots and scatterplots\nWhen there are no more than a few thousand observations, and only one or two variables, it is useful to start with simple plots that allow us to look at the observations directly with no processing or modelling to hide what is going on.\nLet’s start with some examples using only one or two variables. These examples will be revisited in the next couple of chapters as we introduce new anomaly detection tools and graphics.\n\nExample: Don Bradman’s batting averages\nDon Bradman was an Australian cricketer in the first half of the 20th century, and is renowned as the best batter to ever play the game. The most common measure of a batter’s ability is their career average — the total number of runs made divided by the number of times they were dismissed. Figure 1.2 shows a strip plot of the career averages for all 1138 men and women to have played test cricket and batted more than 20 times.\n\n\nCode\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  ggplot(aes(x = Average, y = 1)) +\n  geom_jitter(width = 0, alpha = 0.5) +\n  scale_y_discrete() +\n  labs(y = \"\", x = \"Career batting average\")\n\n\n\n\n\n\n\n\nFigure 1.2: Career batting averages for all men and women to have played test cricket and batted more than 20 times. The anomaly is Don Bradman, who averaged 99.94 over his career.\n\n\n\n\n\nThe points are “jittered” vertically to reduce overplotting, and made slightly transparent to show where overplotting occurs. There is an obvious anomaly on the right of the plot; this point is Don Bradman who averaged 99.94 over his career. Clearly, Don Bradman was much better than any of the other men and women who have played test cricket.\n\n\nExample: Old Faithful eruption durations\nSome anomalies are not so obvious, and occur within the range of the rest of the data. A good example of this is the duration of eruptions of the Old Faithful Geyser. Figure 1.3 shows the duration of all eruptions in the data set.\n\n\nCode\noldfaithful |&gt;\n  ggplot(aes(x = duration, y = 1)) +\n  geom_jitter(width = 0, alpha = 0.5) +\n  labs(y = \"\", x = \"Duration (seconds)\") +\n  scale_y_discrete()\n\n\n\n\n\n\n\n\nFigure 1.3: Old Faithful eruption durations since 2015. The large anomaly is an eruption recorded on 7 December 2015 which apparently lasted nearly two hours.\n\n\n\n\n\nThe big anomaly on the right was an eruption recorded as lasting nearly two hours on 7 December 2015. (I can find no record of this in any news media, so perhaps it is an error in the data.) This makes it difficult to see any detail in the rest of the data, so let’s omit it so we can look at the remaining data more clearly.\n\n\nCode\noldfaithful |&gt;\n  filter(duration &lt; 7000) |&gt;\n  ggplot(aes(x = duration, y = 1)) +\n  geom_jitter(width = 0, alpha = 0.5) +\n  labs(y = \"\", x = \"Duration (seconds)\") +\n  scale_y_discrete()\n\n\n\n\n\n\n\n\nFigure 1.4: Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours.\n\n\n\n\n\nWe see that almost all of the remaining eruptions were between 100 and 300 seconds in length, with one extremely short anomalous duration of 1 second. (Again, perhaps this is an error in the recorded data.) However, the plot reveals another feature of the data. The majority of observations are between 200 and 300 seconds, with a smaller group at around 120 seconds. Few observations fall between 140 and 180 seconds. Those that do can be considered “inliers” — anomalous points that lie within the range of the rest of the data but in regions of low density.\nThe two clusters of observations characterise two eruption modes for the Old Faithful geyser: short durations (around 2 minutes in length) and long durations (between 3 and 5 minutes in length). There was an earthquake in 1998 which changed the distribution of durations. Before 1998, the durations were more evenly split between the two clusters, but currently the long durations are much more common.\nThe oldfaithful data set also contains the time between eruptions. For each row, waiting gives the time to the following eruption. This is used to predict when the next eruption will happen, based on the duration of the most recent eruption.\n\n\nCode\noldfaithful |&gt;\n  filter(duration &lt; 7200, waiting &lt; 7200) |&gt;\n  ggplot(aes(x = duration, y = waiting)) +\n  geom_point(alpha = 0.5) +\n  labs(y = \"Waiting time to next eruption (seconds)\", x = \"Duration (seconds)\")\n\n\n\n\n\n\n\n\nFigure 1.5: Old Faithful eruption durations and waiting times since 2015, omitting waiting times and durations of two hours or more.\n\n\n\n\n\nIn Figure 1.5, we have plotted the duration of each eruption and the waiting time to the next eruption, after filtering some extreme waiting times (which are probably due to incomplete records), and the two-hour duration we have seen previously.\nThe resulting plot shows some anomalies that occur due to the combination of the two variables, although they do not appear anomalous within the data on any one of the variables. For example, there is one eruption of 120 seconds, followed by a waiting time of over 6000 seconds. Neither the waiting time nor the duration are unusual on their own, but the combination is anomalous. Similarly, there are two eruptions of 170 seconds, followed by waiting times of under 4000 seconds. Those waiting times are not particularly unusual compared to other waiting times, but they are unusual given the durations of the previous eruptions.\n\n\nExample: Wine prices and quality\nFigure 1.6 shows a scatterplot of data on 4,496 Syrah wines (also known as Shiraz). The review points is a measure of the quality of the wine (at least according to one taster’s palate). As expected, the price of the wine increases with the quality, although there is considerable variation, and some very expensive wines are rated of relatively low quality, while there are a few exceptional wines at bargain prices.\n\n\nCode\nwine_reviews |&gt;\n  filter(variety %in% c(\"Shiraz\", \"Syrah\")) |&gt;\n  select(points, price) |&gt;\n  ggplot(aes(y = price, x = points)) +\n  geom_jitter(height = 0, width = 0.3, alpha = 0.5) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nFigure 1.6: Wine prices and review points for Shiraz and Syrah wines.\n\n\n\n\n\nThe jittering in the horizontal direction helps reduce overplotting as points is always an integer value. This adds a small amount of random noise to the points variable, but not so much that it overlaps with the neighbouring values. The price is shown on a log scale to allow all the points to be seen more clearly.\nHere the most interesting observations are the ones that have an unusual price given their points value. For example, the lowest priced wine above 95 points is a 2007 Syrah from the Rulo vineyard in the Columbia Valley, Washington. It is an anomaly given its high points value and low price, although neither the price nor the points value are particularly unusual. There are also two very expensive wines that do not have a rating to match, with points values in the low 90s. These can also be considered anomalies.\n\nwine_reviews |&gt;\n  filter(\n    variety %in% c(\"Shiraz\", \"Syrah\"),\n    points &gt; 95\n  ) |&gt;\n  filter(price == min(price)) |&gt;\n  select(country, state, region, winery, variety, year, points, price)\n\n#&gt; # A tibble: 1 × 8\n#&gt;   country state      region          winery variety  year points price\n#&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 US      Washington Columbia Valley Rulo   Syrah    2007     96    20\n\n\nA similar phenomenon occurs whenever you consider additional variables. An anomaly in three dimensions may not appear anomalous in any of the 2-dimensional sets of variables. As the number of dimensions increases, there are more ways for observations to be anomalous, but it is increasingly difficult to find them.\nWhile strip plots and scatterplots are useful for finding anomalies in small- to moderate-sized data sets with one or two numerical variables, we will need alternative tools once we have three or more variables, or a large data set, or non-numerical data.\nWe will consider the problem of high dimensions in Chapter 8, while various types of non-numerical data are considered in the later chapters of the book.\n\n\nExample: Synthetic standard normal data\nWhen we have many variables, showing the pairwise scatterplots in a matrix is a useful way to see a lot of information quickly. This is called a scatterplot matrix, and can be produced using the GGally package.\nFigure 1.7 show the scatterplot matrix of all 10 variables in the n01 data set. Some transparency has been used to help reduce the overplotting, but with 1000 observations in each small point cloud, it is hard to see any detail other than around the edges.\n\n\nCode\nn01 |&gt;\n  GGally::ggpairs(mapping = aes(alpha = 0.02))\n\n\n\n\n\n\n\n\nFigure 1.7: Scatterplots of all pairs of the synthetic n01 data, where each variable has been generated from independent standard Normal distributions.\n\n\n\n\n\nAny points that appear slightly outside the main clouds of points are not genuine anomalies, because they were generated from the same N(0,1) distribution as the rest of the data.\n\n\n\n\nBarnett, V, and T Lewis. 1978. Outliers in Statistical Data. John Wiley & Sons.\n\n\nChristie, M. 2001. The Ozone Layer: A Philosophy of Science Perspective. Cambridge, UK: Cambridge University Press.\n\n\n———. 2004. “Data Collection and the Ozone Hole: Too Much of a Good Thing?” History of Meteorology 1: 99–105.\n\n\nFarman, J C, B G Gardiner, and J D Shanklin. 1985. “Large Losses of Total Ozone in Antarctica Reveal Seasonal ClO_x/NO_x Interaction.” Nature 315 (6016): 207–10.\n\n\nHawkins, D M. 1980. Identification of Outliers. Springer.\n\n\nPukelsheim, F. 1990. “Robustness of Statistical Gossip and the Antarctic Ozone Hole.” The IMS Bulletin 19 (4): 540–45.\n\n\nStolarski, R S, A J Krueger, M R Schoeberl, R D McPeters, P A Newman, and J C Alpert. 1986. “Nimbus 7 Satellite Measurements of the Springtime Antarctic Ozone Decrease.” Nature 322 (6082): 808–11.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-univariate.html",
    "href": "02-univariate.html",
    "title": "2  Univariate probability distributions",
    "section": "",
    "text": "2.1 Common distributions\nIn this book, we take a probabilistic perspective on anomaly detection. That is, we are interested in the likelihood of each observation. So before we discuss any anomaly detection methods, we first need to discuss probability distributions. Some of this material (especially Section 2.1) should be familiar to most readers, but is covered here as a refresher, and to introduce some notation that will be used in later chapters.\nWe are interested in data obtained from a range of possible sample spaces in this book, but for now we will assume that all data are real numbers, and come from continuous probability distributions. Later we will consider other types of data, including discrete data, data that are functions, images, video, networks, text, or even probability distributions as data objects. But for the first half of the book, we will only deal with numerical data.\nLet Y denote a continuous random variable taking values in \\mathbb{R} (the real numbers), with its cumulative probability distribution (cdf) given by F(y) = \\text{Pr}(Y \\le y).\n\\tag{2.1} This function is monotonically increasing in y, and takes values between 0 and 1. Because Y is a continuous random variable, F(y) is a continuous function of y. The probability density function (pdf) is defined as the derivative of the cdf, so f(y) = F'(y), assuming the derivative exists at y. The pdf is non-negative, and integrates to 1.\nThe probability that Y lies in the interval [a,b] is given by \\text{Pr}(a \\leq Y \\leq b) = \\int_a^b f(y)dy = F(b) - F(a).\nThe expected value (or mean) of Y is given by \n\\text{E}(Y) = \\int_{-\\infty}^\\infty y f(y)dy,\n and the variance is given by \n\\text{Var}(Y) = \\text{E}[(Y-\\text{E}(Y))^2] = \\int_{-\\infty}^\\infty (y-\\text{E}(Y))^2 f(y)dy.\n The standard deviation is the square root of the variance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-common",
    "href": "02-univariate.html#sec-common",
    "title": "2  Univariate probability distributions",
    "section": "",
    "text": "Uniform distribution\nThe simplest continuous distribution is the Uniform distribution, which takes two parameters, a and b, and has pdf given by \n  f(y;a,b) = \\begin{cases}\n    \\frac{1}{b-a} & \\text{if } a \\leq y \\leq b, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}\n Thus, it can only take values between a and b, and every value in that range is equally likely. We refer to a Uniform random variable with parameters a and b as Y \\sim \\text{U}(a,b). It has mean (a+b)/2 and variance (b-a)^2/12.\nThe U(0,1) distribution is shown in Figure 2.1.\n\n\nCode\ndist_uniform(0,1) |&gt;\n  gg_density() +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\")\n\n\n\n\n\n\n\n\nFigure 2.1: Probability density function for a U(0,1) distribution. The mean is 0.5 and the variance is 1/12.\n\n\n\n\n\n\n\nNormal (Gaussian) distribution\nThe most widely used continuous distribution is the Normal distribution, also known as the Gaussian distribution, named after the German mathematician Carl Friedrich Gauss. It takes two parameters, the mean \\mu, and the variance, \\sigma^2. The pdf is given by \n  f(y; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right).\n We refer to a Normal random variable with mean \\mu and variance \\sigma^2 as Y \\sim \\text{N}(\\mu, \\sigma^2). When \\mu=0 and \\sigma^2=1, we call it the standard Normal distribution N(0,1), and denote the pdf as \\phi(y) and the cdf as \\Phi(y).\nThree Normal distributions are shown in Figure 2.2.\n\n\nCode\ndist_normal(c(0, 0, 1), c(1, 2, 1)) |&gt;\n  gg_density() +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\")\n\n\n\n\n\n\n\n\nFigure 2.2: Probability density functions for three Normal distributions. The mean \\mu controls the location of the distribution, while the variance \\sigma^2 controls the spread.\n\n\n\n\n\n\n\n\\chi^2 distribution\nThe \\chi^2 (or chi-squared) distribution takes one parameter, the degrees of freedom k. The pdf is given by \n  f_k(y) = \\frac{y^{k/2-1}e^{-y/2}}{2^{k/2}\\Gamma(k/2)},\n where \\Gamma(u) = \\int_0^\\infty x^{u-1}e^{-x}dx is the gamma function. For positive integers u, \\Gamma(u) = (u-1)!. The \\chi^2 distribution arises naturally as the sum of the squares of k independent standard Normal random variables. We refer to a \\chi^2 random variable with k degrees of freedom as Y \\sim \\chi^2_k. It has mean k and variance 2k.\nThree \\chi^2 distributions are shown in Figure 2.3.\n\n\nCode\ndist_chisq(c(1, 2, 5)) |&gt;\n  gg_density() +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  scale_color_discrete(labels = c(\n    latex2exp::TeX(\"$\\\\chi^{~2}_{~~1}$\"),\n    latex2exp::TeX(\"$\\\\chi^{~2}_{~~2}$\"),\n    latex2exp::TeX(\"$\\\\chi^{~2}_{~~5}$\")\n  ))\n\n\n\n\n\n\n\n\nFigure 2.3: Probability density functions for three \\chi^2 distributions. As the degrees of freedom k increases, the distribution becomes more symmetric, and converges to the Normal distribution N(k,2k).\n\n\n\n\n\n\n\nt distribution\nThe t distribution was first described by the English statistician and chemist, William Gosset, who worked for the Guinness brewery in Dublin, Ireland, and published under the pseudonym “Student”. It arises naturally as the ratio Z/\\sqrt{S/k}, where Z\\sim \\text{N}(0,1) and S\\sim \\chi^2_k are independent, and is commonly used in variations of t-tests. It takes one parameter, the degrees of freedom k. The pdf is given by \n  f_k(y) = \\frac{\\Gamma\\left(\\frac{k+1}{2}\\right)}{\\sqrt{k\\pi}\\Gamma\\left(\\frac{k}{2}\\right)}\n  \\left(1 + \\frac{y^2}{k}\\right)^{-\\frac{k+1}{2}}.\n We refer to a t random variable with k degrees of freedom as Y \\sim \\text{t}_k. The t distribution is similar in shape to the standard Normal distribution, but has heavier tails. When k=1, the t distribution is known as the “Cauchy” distribution (after the French mathematician Augustin-Louis Cauchy), which has undefined mean and variance. For k=2, it has mean 0 and infinite variance. For k &gt; 2, it has mean 0 and variance k/(k-2). When k\\rightarrow\\infty, the t distribution converges to the standard Normal distribution.\nThree t distributions are shown in Figure 2.4.\n\n\nCode\ndist_student_t(c(1, 5, 99)) |&gt;\n  gg_density(ngrid = 10001) +\n  coord_cartesian(xlim = c(-8,8)) +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\") +\n  scale_color_discrete(labels = c(\n    latex2exp::TeX(\"$t_{~1}$\"),\n    latex2exp::TeX(\"$t_{~5}$\"),\n    latex2exp::TeX(\"$t_{~99}$\")\n  )) +\n  theme(legend.text = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nFigure 2.4: Probability density functions for three t distributions. As the degrees of freedom k increases, the distribution converges to a N(0,1) distribution.\n\n\n\n\n\n\n\n\\Gamma distribution\nThe \\Gamma (or Gamma) distribution is a generalization of the \\chi^2 distribution, and is used in many statistical models. There are several parameterizations. Here, we use the shape/rate parameterization which uses the shape \\alpha and the rate \\beta. The pdf is given by \n  f(y; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} y^{\\alpha-1}e^{-\\beta y}.\n We refer to a \\Gamma random variable with shape \\alpha and rate \\beta as Y \\sim \\Gamma(\\alpha, \\beta). The mean and variance of a \\Gamma random variable are \\alpha/\\beta and \\alpha/\\beta^2 respectively. The \\chi^2_k distribution is a special case of the \\Gamma distribution, with \\alpha = k/2 and \\beta = 1/2. The Exponential distribution is another special case of the \\Gamma distribution, with \\alpha = 1. The sum of k independent Exponential random variables with rate \\beta has a \\Gamma(k, \\beta) distribution.\nThree \\Gamma distributions are shown in Figure 2.5.\n\n\nCode\ndist_gamma(c(1, 1, 2), c(1, 2, 1)) |&gt;\n  gg_density() +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\")\n\n\n\n\n\n\n\n\nFigure 2.5: Probability density functions for three \\Gamma distributions. As \\alpha increases, the distribution to a N(\\alpha/\\beta, \\alpha/\\beta^2) distribution.\n\n\n\n\n\n\n\nMixture distributions\nA mixture distribution is a distribution that is formed by mixing together other distributions. That is, a sample value is drawn from a mixture distribution by first randomly selecting one of the component distributions with some specified probability, and then sampling from the selected component distribution.\nThe pdf of a mixture distribution is given by the weighted sum of the pdfs of the component distributions, where the weights correspond to the probability of sampling from each component. For example, suppose Y is a mixture of two Normal distributions, with parameters (\\mu_1,\\sigma_1^2) and (\\mu_2,\\sigma_2^2), where the first distribution has weight p and the second has weight 1-p. Then the resulting pdf of the mixture distribution is given by \n  f(y) = pf(y;\\mu_1,\\sigma_1^2) + (1-p)f(y;\\mu_2,\\sigma_2^2).\n The mean and variance of the mixture distribution are given by \n  \\text{E}(Y) = p\\mu_1 + (1-p)\\mu_2,\n and \n\\text{Var}(Y) = p\\sigma_1^2 + (1-p)\\sigma_2^2 + p(1-p)(\\mu_1 - \\mu_2)^2,\n respectively.\nAn example is shown in Figure 2.6, where \\mu_1 = -2, \\mu_2 = 2, \\sigma_1^2 = \\sigma_2^2 = 1, and p=1/3.\n\n\nCode\ndist_mixture(\n  dist_normal(-2, 1),\n  dist_normal(2, 1),\n  weights = c(1/3, 2/3)\n) |&gt;\n  gg_density() +\n  gg_density_layer(dist_normal(-2,1), linetype = \"dashed\", scale = 1/3) +\n  gg_density_layer(dist_normal(2,1), linetype = \"dashed\", scale = 2/3) +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\")\n\n\n\n\n\n\n\n\nFigure 2.6: Probability density function for a mixture of two Normal distributions. The dashed lines show the (scaled) component densities, and the solid line shows the mixture density, equal to the sum of the dashed lines.\n\n\n\n\n\nThis is an example of a multimodal density function, with modes at -2 and 2. The mixture distribution is not Normal, even though both component distributions are Normal. The mixture distribution is also not symmetric, even though both component distributions are symmetric.\n\n\nFurther reading\nA good introduction to other probability distributions on real sample spaces is provided by Forbes et al. (2011), with more advanced treatments in Johnson, Kotz, and Balakrishnan (1994) and Johnson, Kotz, and Balakrishnan (1995).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-clt",
    "href": "02-univariate.html#sec-clt",
    "title": "2  Univariate probability distributions",
    "section": "2.2 Central limit theorem",
    "text": "2.2 Central limit theorem\nThe central limit theorem (CLT) is one of the most important results in statistics. There are several variations of the theorem; the simplest version states that the sum of a large number of iid random variables, with finite mean and variance, is approximately Normally distributed.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nIf Y_1,\\dots,Y_n are iid random variables with mean \\mu and variance \\sigma^2 &lt; \\infty, then \n  \\frac{\\sum_{i=1}^n Y_i - n\\mu}{\\sigma\\sqrt{n}} \\longrightarrow \\text{N}(0,1).\n\n\n\nThis is a remarkable result as it does not require any assumptions about the underlying distribution of the random variables Y_1,\\dots,Y_n, other than that they have finite mean and variance. It is also a useful result, as it allows us to use the Normal distribution to approximate the distribution of many statistics.\nFor example, the sample mean (or average), is simply the sum of the observations divided by the sample size. So by the CLT, for a large sample size, the sample mean will have a Normal distribution. That is, if the observations come from a distribution with mean \\mu and variance \\sigma^2, then the sample mean will have distribution \\text{N}(\\mu, \\sigma^2/n) when n is large, regardless of the underlying distribution of the observations.\nVariations on the theorem allow the variables Y_1,\\dots,Y_n to be independent but not necessarily identically distributed, or to be weakly dependent.\nIt is because of the CLT that a \\chi^2_k distribution is approximately Normal for large k. (Recall that the sum of k independent N(0,1) random variables has a \\chi^2_k distribution.)\nSimilarly, a \\Gamma(\\alpha,\\beta) distribution is approximately Normal for large \\alpha. (Recall that the sum of \\alpha independent \\Gamma(1,\\beta), or Exponential, random variables has a \\Gamma(\\alpha,\\beta) distribution.)\nWasserman (2004) provides some further details and examples of the CLT.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-quantiles",
    "href": "02-univariate.html#sec-quantiles",
    "title": "2  Univariate probability distributions",
    "section": "2.3 Quantiles",
    "text": "2.3 Quantiles\nA quantile of a continuous univariate distribution is a point that divides the sample space into two regions, based on the probability of observations falling below or above the quantile value. If Q(p) denotes the quantile corresponding to probability p, then P(Y \\le Q(p)) = p and P(Y &gt; Q(p)) = 1-p.\n\n\nCode\ndist &lt;- dist_chisq(5)\nq80 &lt;- quantile(dist, p = 0.8)\nx5 &lt;- tibble(\n  x = seq(0, 19, l = 501),\n  y = unlist(density(dist, at = x))\n)\nx5 |&gt;\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  geom_area(data = x5 |&gt; filter(x &lt; q80), fill = \"#0072B2\") +\n  geom_area(data = x5 |&gt; filter(x &gt;= q80), fill = \"#D55E00\") +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\") +\n  scale_x_continuous(\n    breaks = c(seq(0, 20, by = 5), q80),\n    labels = c(seq(0, 20, by = 5), paste0(round(q80, 2), \"\\nQ(0.8)\")),\n    minor_breaks = NULL\n  ) +\n  geom_label(\n    data = tibble(x = c(4, 10), y = 0.01),\n    aes(x = x, y = y, label = paste(\"Prob =\", c(0.8, 0.2)))\n  )\n\n\n\n\n\n\n\n\nFigure 2.7: The 0.8 quantile of a \\chi^2_5 distribution is the value Q(0.8) = 7.29 such that P(Y \\le 7.29) = 0.8. The shaded area is 0.8, and the probability of observing a value greater than 7.29 is 0.2.\n\n\n\n\n\nThe quantiles of a continuous distribution are given by the inverse of the cumulative distribution function, Q(p) = F^{-1}(p).\nSome quantiles are given special names. The p=0.5 quantile is called the median. The p=0.25 and p=0.75 quantiles are called the lower and upper quartiles respectively. The deciles are defined when p=0.1,0.2,\\dots,0.9, while the percentiles are defined with p=0.01,0.02,\\dots,0.99.\nThe “interquartile range” (IQR) is the difference between the upper and lower quartiles. It is a measure of the spread of the distribution, and is often used as a robust alternative to the standard deviation. For a Normal distribution, the standard deviation is equal to 1.349 times the IQR.\nThe quantile functions for a standard Normal distribution, a \\text{t}_5 distribution, and a \\chi^2_5 distribution are shown in Figure 2.8.\n\n\nCode\np &lt;- seq(0, 1, l = 501)\nc(dist_normal(0, 1), dist_student_t(5), dist_chisq(5)) |&gt;\n  quantile(p = p) |&gt;\n  as_tibble(.name_repair = \"unique\") |&gt;\n  mutate(p = p) |&gt;\n  tidyr::pivot_longer(-p, names_to = \"Distribution\", values_to = \"Q\") |&gt;\n  ggplot(aes(x = p, y = Q, col = Distribution)) +\n  geom_line() +\n  labs(x = \"p\", y = \"Quantile function: Q(p)\") +\n  scale_color_discrete(labels = c(\n    latex2exp::TeX(\"N(0,1)\"),\n    latex2exp::TeX(\"$t_{~5}$\"),\n    latex2exp::TeX(\"$\\\\chi^{~2}_{~~5}$\")\n  )) +\n  theme(legend.text = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nFigure 2.8: Quantile function for a standard Normal distribution, a \\text{t}_5 distribution, and a \\chi^2_5 distribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-sample-quantiles",
    "href": "02-univariate.html#sec-sample-quantiles",
    "title": "2  Univariate probability distributions",
    "section": "2.4 Sample quantiles",
    "text": "2.4 Sample quantiles\nThe sample quantiles of a data set are estimates of the quantiles of the underlying distribution. Suppose we have data on a single variable, \\{y_1,\\dots,y_n\\}, and we want to estimate a quantile Q(p). There are a surprising number of ways this can be done, and there is no accepted standard approach.\n\nThe quantile() function\nThe R function quantile() includes nine variations, based on Hyndman and Fan (1996). Which variation you use makes little difference except for tiny data sets where n is small, or when you are interested in the tails when p is close to 0 or 1. However, for anomaly detection, we are usually interested in the tails, so we will need to use a method that works well for extreme values of p.\nLet’s compare two of the possible variations available in quantile(): type = 7 which is the default method, and type = 8 which was the recommendation of Hyndman and Fan (1996) due to it having smaller bias.\nLet y_{(k)} denote the kth largest observation, k=1,\\dots,n. These ordered values are known as “order statistics”. Then the sample quantile \\hat{Q}(p) is given by linear interpolation between consecutive order statistics. That is, if k^+= p(n+1-2a) +a for some constant a, then \n  \\hat{Q}(p) = (1-\\gamma) y_{(k)} + \\gamma y_{(k+1)},\n where k = \\lfloor k^+ \\rfloor is the integer part of k^+ and \\gamma = k^+ - k is the remainder. Most of the different methods for estimating quantiles differ only in the value of a.\n\nWhen the default type = 7 is used, a = 1, so k^+ = p(n-1) + 1.\nWhen type = 8 is used, a= 1/3, so k^+ = p(n+1/3) + 1/3.\n\nFor any value of a, if p=0.5 then k^+ = (n+1)/2, and we obtain the sample median:\n\nWhen n is even, then k=n/2 and \\gamma = 1/2, so \\hat{Q}(0.5) is the average of the middle two observations.\nWhen n is odd, then k=(n+1)/2 and \\gamma = 0, so \\hat{Q}(0.5) is the middle observation.\n\nFor large n, the sample quantile should provide a good estimate of the true quantile, so that \\hat{Q}(p) \\approx Q(p). We can study the bias of the estimator by simulating from a known distribution, and comparing the sample quantiles to the true quantiles. In Figure 2.9, we do this for samples drawn from a N(0,1) distribution. The estimated bias is based on repeating the exercise 10,000 times and averaging the results.\n\n\nCode\navequantile &lt;- function(n, p, m) {\n  Q7 &lt;- Q8 &lt;- numeric(length(p))\n  for (i in seq(m)) {\n    random_sample &lt;- rnorm(n)\n    Q7 &lt;- Q7 + quantile(random_sample, prob = p, type = 7) / m\n    Q8 &lt;- Q8 + quantile(random_sample, prob = p, type = 8) / m\n  }\n  tibble(n = n, p = p, Q7 = Q7, Q8 = Q8)\n}\nqbias &lt;- tibble(n = c(100, 1000)) |&gt;\n  purrr::pmap_dfr(~ avequantile(\n    n = .x,\n    p = seq(0.002, 0.998, by = 0.001), m = 10000\n  )) |&gt;\n  tidyr::pivot_longer(Q7:Q8, names_to = \"type\", values_to = \"Q\") |&gt;\n  mutate(\n    type = paste(\"type = \", readr::parse_number(type)),\n    Qtrue = qnorm(p),\n    bias = Q - qnorm(p)\n  )\nqbias |&gt;\n  ggplot(aes(x = p, group = type)) +\n  geom_line(aes(y = bias, col = type, linetype = \"n = 100\"),\n    data = qbias |&gt; filter(n == 100)\n  ) +\n  geom_line(aes(y = bias, col = type, linetype = \"n = 1000\"),\n    data = qbias |&gt; filter(n == 1000)\n  ) +\n  labs(\n    x = \"Probability p\",\n    y = \"Bias of sample quantile\",\n  ) +\n  guides(\n    linetype = guide_legend(title = \"Sample size\"),\n    col = guide_legend(title = \"Quantile method\")\n  )\n\n\n\n\n\n\n\n\nFigure 2.9: Bias of the sample quantile for a N(0,1) distribution. The orange line shows the bias of the default type = 7 quantile method, while the blue line shows the bias of the preferred type = 8 quantile method.\n\n\n\n\n\nClearly, the type = 8 sample quantile gives much better results than the default type = 7 method, especially when p is close to 0 or 1. Consequently, we will always use type = 8 when computing sample quantiles in the remainder of this book.\n\n\nExample: Test cricket batting averages\nWe can calculate some of the sample quantiles for Test cricket batting averages, considering only those batters who have played at least 20 innings. These values are easily computed using the quantile() function with argument type = 8.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  pull(Average) |&gt;\n  quantile(prob = c(0, 0.25, 0.50, 0.75, 0.99, 1), type = 8)\n\n#&gt;    0%   25%   50%   75%   99%  100% \n#&gt;  2.00 16.59 26.63 36.72 58.50 99.94\n\n\n\n\nCode\nbatave &lt;- cricket_batting |&gt;\n  filter(Innings &gt; 20)\np &lt;- c(0.25, 0.5, 0.75, 0.99)\nQs &lt;- quantile(batave$Average, prob = p, type = 8)\nbatave |&gt;\n  ggplot(aes(x = Average, y = 1)) +\n  geom_jitter(width = 0, alpha = 0.5) +\n  geom_vline(xintercept = Qs, col = \"#D55E00\") +\n  scale_y_discrete() +\n  scale_x_continuous(\n    breaks = c(0, Qs, 100),\n    labels = c(\"0\", sprintf(\"%2.2f\", Qs), \"100\"),\n  ) +\n  scale_x_continuous(\n    breaks = c(0, Qs, 100),\n    labels = c(\"0\", sprintf(\"%2.2f\", Qs), \"100\"),\n    sec.axis = dup_axis(\n      name = \"\",\n      breaks = Qs,\n      labels = latex2exp::TeX(sprintf(\"\\\\hat{Q}(%2.2f)\", p))\n    )\n  ) +\n  labs(y = \"\", x = \"Career batting average\")\n\n\n\n\n\n\n\n\nFigure 2.10: Some sample quantiles of career batting averages for all men and women to have played test cricket and batted more than 20 times.\n\n\n\n\n\nHalf of all batters average above 26.63, while only 1 in 100 batters have an average above 58.50.\n\n\nSample quantile distribution\nWith a large sample size n, the sample quantile \\hat{Q}(p) has an approximate Normal distribution, \n\\hat{Q}(p) \\approx \\text{N}\\left(Q(p), \\frac{p(1-p)}{n f^2(Q(p))}\\right)\n\\tag{2.2} where f(y) is the density of the underlying distribution (Van der Vaart 2000, p308). This is an asymptotic result, that assumes np is large, and holds for all types of sample quantiles.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-hdr",
    "href": "02-univariate.html#sec-hdr",
    "title": "2  Univariate probability distributions",
    "section": "2.5 Highest density regions",
    "text": "2.5 Highest density regions\nA highest density region is defined as the region of the sample space where the density is higher than a given threshold (Hyndman 1996). It is commonly specified using the probability mass that the HDR contains. For example, a 95% HDR is the region of the sample space that contains 95% of the probability mass, and where every point inside the region has higher density than any point outside the region.\nFor the univariate random variable Y, with a smooth, continuous density function f, the 100(1-\\alpha)% HDR is the set \n  R_\\alpha = \\{y: f(y) \\ge f_\\alpha\\}\n\\tag{2.3} where P(Y \\in R_\\alpha) = 1-\\alpha.\nA useful property of the HDR is that it is the smallest region containing 1-\\alpha of the probability mass.\nFor a symmetric unimodal distribution such as the Normal or t distributions, the HDR is an interval centred on the mean (which is equal to the median), with the ends given by the \\alpha/2 and 1-\\alpha/2 quantiles. For a skewed unimodal distribution such as the \\chi^2 or Gamma distributions, the HDR is also an interval, but it is not centred on the mean (or median), and the ends are not given by symmetric quantiles. For example, Figure 2.11 shows two 90% regions for a \\chi^2_5 distribution. The one shown in blue is based on the 0.05 and 0.95 quantiles, while the orange region is the 90% HDR. Notice that both are intervals, and that the orange interval is smaller than the blue interval. The value of the density at the ends of the HDR is the same, and is given by f_\\alpha.\n\n\nCode\ndist &lt;- dist_chisq(5)\ninterval1 &lt;- quantile(dist, p = c(0.05, 0.95)) |&gt; unlist()\ninterval2 &lt;- hdr(dist, size = 90) |&gt; unlist()\nfalpha &lt;- density(dist, at = interval2[1])\ngg_density(dist) +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\") +\n  geom_line(\n    data = tibble(x = interval1), aes(x = x, y = -0.002),\n    col = \"#0072B2\", linewidth = 2\n  ) +\n  geom_line(\n    data = tibble(x = interval2[1:2]), aes(x = x, y = 0.001),\n    col = \"#D55E00\", linewidth = 2\n  ) +\n  geom_line(\n    data = tibble(x = rep(interval2[1], 2), y = c(0, falpha)),\n    aes(x = x, y = y), col = \"#D55E00\", linetype = \"dashed\"\n  ) +\n  geom_line(\n    data = tibble(x = rep(interval2[2], 2), y = c(0, falpha)),\n    aes(x = x, y = y), col = \"#D55E00\", linetype = \"dashed\"\n  ) +\n  geom_hline(aes(yintercept = falpha), col = \"#D55E00\", linetype = \"dashed\") +\n  scale_y_continuous(\n    breaks = c(seq(0, 0.15, by = 0.05), falpha),\n    minor_breaks = NULL,\n    labels = latex2exp::TeX(c(seq(0, 0.15, by = 0.05), \"$f_\\\\alpha$\")),\n  )\n\n\n\n\n\n\n\n\nFigure 2.11: Highest density region for a \\chi^2_5 distribution. The blue bar shows the 90% interval based on the 0.05 and 0.95 quantiles. The orange bar shows the 90% HDR, which is the smallest region containing 90% of the probability mass.\n\n\n\n\n\nFor a multimodal distribution, the HDR may be a union of intervals. For example, the HDR for the mixture shown in Figure 2.6 is displayed in Figure 2.12.\n\n\nCode\ndist &lt;- dist_mixture(dist_normal(-2,1), dist_normal(2, 1), weights = c(1/3, 2/3))\ninterval1 &lt;- quantile(dist, p = c(0.05, 0.95)) |&gt; unlist()\ninterval2 &lt;- hdr(dist, size = 90) |&gt; unlist()\nhdrinterval1 &lt;- interval2[c(\"lower1\", \"upper1\")]\nhdrinterval2 &lt;- interval2[c(\"lower2\", \"upper2\")]\nfalpha &lt;- density(dist, at = hdrinterval2[1])\np &lt;- gg_density(dist) +\n  labs(x = \"y\", y = \"Probability Density Function: f(y)\") +\n  geom_line(\n    data = tibble(x = interval1), aes(x = x, y = -0.0028),\n    col = \"#0072B2\", linewidth = 2\n  ) +\n  geom_line(\n    data = tibble(x = hdrinterval1), aes(x = x, y = 0.0018),\n    col = \"#D55E00\", linewidth = 2\n  ) +\n  geom_line(\n    data = tibble(x = hdrinterval2), aes(x = x, y = 0.0018),\n    col = \"#D55E00\", linewidth = 2\n  )\nfor (edge in interval2[1:4]) {\n  p &lt;- p + geom_line(\n    data = tibble(x = rep(edge, 2), y = c(0, falpha)),\n    aes(x = x, y = y), col = \"#D55E00\", linetype = \"dashed\"\n  )\n}\np +\n  geom_hline(aes(yintercept = falpha), col = \"#D55E00\", linetype = \"dashed\") +\n  scale_y_continuous(\n    breaks = c(seq(0, 0.2, by = 0.1), falpha),\n    minor_breaks = NULL,\n    labels = latex2exp::TeX(c(\n      seq(0, 0.2, by = 0.1),\n      \"$f_\\\\alpha$\"\n    )),\n  )\n\n\n\n\n\n\n\n\nFigure 2.12: Highest density region for the mixture distribution shown in Figure 2.6. The blue bar shows the 90% interval based on the 0.05 and 0.95 quantiles. The orange bar shows the 90% HDR, which is the smallest region containing 90% of the probability mass.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-robust-univariate",
    "href": "02-univariate.html#sec-robust-univariate",
    "title": "2  Univariate probability distributions",
    "section": "2.6 Robust estimation",
    "text": "2.6 Robust estimation\nMany anomaly detection methods will need an estimate of the centre and the spread of the distribution of data. The usual estimators are the sample mean and sample standard deviation, but these are sensitive to outliers, so we will need to use alternatives.\nFor univariate data, the sample median provides a convenient robust estimator of the centre of the distribution. A simple robust estimator of the spread is the IQR, defined as \n  \\text{IQR} = \\hat{Q}(0.75) - \\hat{Q}(0.25).\n\\tag{2.4} Another commonly used robust estimator of spread is the median absolute deviation (MAD), given by \n  \\text{MAD} = \\text{median}(|y_i - \\text{median}(y)|).\n\\tag{2.5} If our data come from a Normal distribution, then the standard deviation is given by s = 0.7413\\times\\text{IQR} = 1.4826\\times\\text{MAD}.\nA third robust estimator of the spread is the Q_n estimator (Rousseeuw and Croux 1993), based on pairwise differences between observations. Let \\Delta_{ij} = |y_i-y_j| denote the absolute difference between the ith and jth observations, for 1 \\le i &lt; j \\le n. There are n(n-1)/2 such differences. Let \\Delta_{(k)} be the kth largest of these differences, where k = {\\lfloor n/2\\rfloor + 1 \\choose 2} and \\lfloor n/2\\rfloor is the integer part of n/2. Thus, \\Delta_{(k)} is approximately equal to the first quartile of the absolute pairwise differences. Then Q_n = d_n\\Delta_{(k)}, where d_n is a constant that depends on n and is designed to scale Q_n so that it provides an estimate of the standard deviation for a Normal distribution. For large n, d_n = 2.219. While the IQR and MAD estimators are somewhat simpler and easier to understand, the Q_n estimator has some useful properties (as discussed in Rousseeuw and Croux 1993).\nAll three of these robust estimators of the standard deviation can be computed using the robustbase package, with the functions s_IQR(), s_mad(), and s_Qn() respectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-kde",
    "href": "02-univariate.html#sec-kde",
    "title": "2  Univariate probability distributions",
    "section": "2.7 Univariate kernel density estimation",
    "text": "2.7 Univariate kernel density estimation\nUnlike the examples discussed in Section 2.1, we will not normally know what distribution our data come from, and they will almost never be from a standard parametric distribution such as a Normal, t, \\chi^2 or Gamma distribution. Instead, we will need to estimate the probability density function from the data. Kernel density estimation is the most popular method for nonparametric estimation of a probability density function.\n\nKernel density estimates\nSuppose we have n univariate observations, \\{y_1,\\dots,y_n\\}, which are independent draws from a probability distribution, and we want to estimate the underlying probability density function. The kernel density estimate (see Wand and Jones 1995) is given by \n  \\hat{f}(y) = \\frac{1}{n} \\sum_{i=1}^n K_h(y-y_i),\n\\tag{2.6} where K_h is a “kernel” function and h is a bandwidth to be determined. We will use kernel functions that are themselves probability density functions with mean 0 and standard deviation h. Thus, a kernel density estimate is a mixture distribution with n components, each of which is a kernel function centred at one of the observations. For example, the “Gaussian” kernel is K_h(u) = \\exp(-u^2/h^2)/(h\\sqrt{2\\pi}), equal to the Normal density function with mean zero and standard deviation h. Another popular kernel is the quadratic Epanechnikov kernel given by K_h(u) = [1-u^2/(5h^2)]_+ / (h4\\sqrt{5}/3), where x_+ = \\max(x,0). In our examples, we will use the Gaussian kernel.\nNow we will apply Equation 2.6 to the first ten Old Faithful eruption durations from 2021 that are in the oldfaithful data set. The kernel density estimate can be visualized as a sum of kernel functions centred over each observation with width determined by h, and height given by K_h(0)/n.\nLet’s suppose K_h is a Gaussian kernel and let h = 16 (I will explain this choice below). Then we get the following set of kernel functions.\n\n\n\n\n\n\n\n\nFigure 2.13: Kernel functions centred over the observations.\n\n\n\n\n\nThe vertical ticks show the location of the ten observations, while the grey lines show \\frac{1}{n}K_h(y - y_i) for i=1,\\dots,n.\nThese functions are then added together, as in Equation 2.6, to give the density estimate.\n\n\n\n\n\n\n\n\nFigure 2.14: Density estimate formed by summing the kernel functions centred on the observations.\n\n\n\n\n\nWe made two choices when producing this estimate: the value of h and the type of kernel K_h. If either was replaced with a different choice, the estimate would be different. For large data sets, it does not make much difference which kernel function is used.\nThe choice of h is more difficult and will change the shape of the density estimate substantially. Here are three versions of the density estimate with bandwidths given by h=5, h=15 and h=40.\n\n\n\n\n\n\n\n\nFigure 2.15: Kernel density estimates based on different bandwidth values h.\n\n\n\n\n\nWhen h is too small, the density estimate is too rough, with many peaks and troughs. When h is too large, the density estimate is too smooth and we fail to see any features in the data. A popular choice for h is the “normal reference rule” (Silverman 1986, p45) given by \n  h = 1.059 s n^{-1/5}\n\\tag{2.7} but because we want our results to be robust to outliers, we will replace s by the robust Q_n estimate, resulting in \n  h = 1.059 Q_n n^{-1/5}.\n\\tag{2.8} We will call this the “robust normal reference rule”; it is implemented in the function kde_bandwidth(). For the 10 observations in the example above, it gives h = 16, which is the value we used.\nThis example using only 10 observations was purely to illustrate the method. Let’s now estimate a kernel density estimate for the full data set (other than that one pesky duration of 2 hours).\nWe will use the dist_kde() function, which uses the Gaussian kernel, and by default sets the bandwidth using the robust normal reference rule (Equation 2.8) via the kde_bandwidth() function.\n\nof &lt;- oldfaithful |&gt;\n  filter(duration &lt; 7000)\nof_density &lt;- dist_kde(of$duration)\nof_density |&gt;\n  gg_density() +\n  geom_rug(aes(x = duration), of) +\n  labs(x = \"Duration (seconds)\")\n\n\n\n\n\n\n\nFigure 2.16: Kernel density estimate of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours.\n\n\n\n\n\nAs this is a much bigger data set (with 2260 observations), the selected bandwidth is smaller and is now h = 5.01. Here the estimate has clearly identified the two groups of eruptions, one much larger than the other. The extreme observation of 1 second, and the unusual observations between 140 and 180 seconds are in the areas of low density. Later we will use density estimates at each observation to identify anomalous points.\n\n\nStatistical properties\nThe statistical properties of the kernel density estimator (Equation 2.6) have been extensively studied, and are described in several books including Wand and Jones (1995) and Scott (2015).\nAn important asymptotic result is that the mean square error (MSE) of \\hat{f}(y) is \n  \\text{E}\\left[(\\hat{f}(y) - f(y))^2\\right] \\approx\n  \\frac{1}{4}h^4[f''(y)]^2 + \\frac{f(y)R(K)}{nh} ,\n\\tag{2.9} where R(K) = \\int K^2(u)du is the “roughness” of the kernel function. An estimator is “consistent” if the MSE goes to zero as the sample size n goes to infinity. So Equation 2.6 gives a consistent estimator of the underlying density f when both terms in Equation 2.9 go to zero. That is, \n  \\lim_{n\\rightarrow\\infty} h = 0\n  \\qquad\\text{and}\\qquad\n  \\lim_{n\\rightarrow\\infty} nh = \\infty,\n\\tag{2.10} and so h should decrease slowly as n increases. Note that these conditions hold for Equation 2.8.\nIf we integrate the MSE given by Equation 2.9 over y (assuming f is sufficiently smooth for the integral to exist), we obtain the mean integrated squared error (MISE) given by \n  \\text{E}\\int \\left[(\\hat{f}(y) - f(y))^2\\right] dy \\approx\n  \\frac{1}{4}h^4R(f'') + \\frac{R(K)}{nh} ,\n\\tag{2.11} where R(f'') = \\int [f''(y)]^2 dy is the roughness of the second derivative of the underlying density. This gives us the average squared error over the whole range of y.\n\n\nBandwidth selection\nThe optimal overall bandwidth is obtained by minimizing the MISE. This can be calculated by differentiating Equation 2.11 with respect to h and setting the derivative to zero, yielding \n  h = \\left(\\frac{R(K)}{R(f'')n}\\right)^{1/5}.\n\\tag{2.12} So the optimal h is proportional to n^{-1/5}, which clearly satisfies the conditions given by Equation 2.10.\nHowever, this value of h depends on the underlying density f which we don’t know. If f is a Normal density with variance \\sigma^2, then R(f) = \\frac{1}{2\\sqrt{\\pi}}\\sigma^{-1} and R(f'') = \\frac{3}{8\\sqrt{\\pi}}\\sigma^{-5}. So if we use a Gaussian kernel, and assume the underlying density has the same roughness as a Normal density, we obtain \n  h = \\sigma \\left(\\frac{4}{3n}\\right)^{1/5}.\n\\tag{2.13} Replacing \\sigma by the sample standard deviation gives us the normal reference rule Equation 2.7; while replacing \\sigma by the robust estimate Q_n gives use the robust normal reference rule Equation 2.8.\nSeveral other related bandwidth selection methods are widely used. For example, instead of using Q_n to estimate s, we might use the minimum of the sample standard deviation and the robust estimate based on the IQR, giving \n  h = 1.06 \\min(s, 0.7413\\text{IQR}) n^{-1/5}.\n\\tag{2.14} This is what is computed by the bw.nrd() function, and is available in geom_density() or density() by setting bw = \"nrd\".\nSilverman’s “rule-of-thumb” is similar, but it replaces 1.06 in Equation 2.14 by 0.9, because Silverman argued that most densities are not as smooth as a Normal density, and so a smaller bandwidth is required. This variation is computed using the bw.nrd0() function, and is the default bandwidth used by geom_density() and density().\nAnother popular bandwidth choice is the “plug-in” bandwidth (Sheather and Jones 1991), obtained by replacing R(f'') in Equation 2.13 by an estimate based on the data. This is computed by bw.SJ() and can be used with geom_density() and density() by setting bw = \"SJ\". For most data sets, this will give a smaller value than Equation 2.7.\nBandwidths obtained in this way are designed to give a good overall estimate of the underlying density, but may not be optimal for any particular point of the density. Our goal is to find anomalies in the data, rather than find a good representation for the rest of the data, and so we are interested in the regions of low density.\nIf we optimized MSE (Equation 2.9) rather than MISE (Equation 2.11), we would obtain \nh = \\left(\\frac{f(y)R(K)}{n[f''(y)]^2}\\right)^{1/5}.\n This shows that larger bandwidths are required when f(y)/[f''(y)]^2 is relatively large, which often occurs in the extreme tails of a distribution. So bandwidths for anomaly detection tend to be larger than bandwidths for other purposes. For that reason, we will sometimes use twice the value of h given by Equation 2.8, which is available in the kde_bandwidth() function using multiplier = 2.\n\n\nHighest kernel density regions\nThe kernel density estimate can be used to identify regions of highest density in the data. We can apply the hdr() function to the estimated distribution to find the HDR regions with specified probability coverage. For example, we can compute the 90% HDR for the oldfaithful data set as follows:\n\nhdr &lt;- hdr(of_density, size = 90)\nhdr\n\n#&gt; &lt;hdr[1]&gt;\n#&gt; [1] [105.5, 127.1][202.2, 282.2]90\n\n\nThis shows that the 90% HDR comprises two intervals, corresponding to the short and long duration events.\nThe resulting estimate is shown in Figure 2.17, along with the 10% of observations that lie outside the 90% HDR.\n\n\nCode\nof_density |&gt;\n  gg_density(hdr = \"fill\", prob = 0.9, show_points = TRUE, jitter = TRUE, alpha = 0.5) +\n  labs(x = \"Duration (seconds)\")\n\n\n\n\n\n\n\n\nFigure 2.17: 90% highest density region for the oldfaithful data set.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "02-univariate.html#sec-evt",
    "href": "02-univariate.html#sec-evt",
    "title": "2  Univariate probability distributions",
    "section": "2.8 Extreme value theory",
    "text": "2.8 Extreme value theory\nExtreme Value Theory is used to model rare, extreme events and is useful in anomaly detection. Suppose we have n independent and identically distributed random variables Y_1, \\dots, Y_n with a cdf F(y) = P\\{Y \\leq y\\}. Then the maximum of these n random variables is M_n = \\max \\{Y_1, \\dots, Y_n\\}. If F is known, the cdf of M_n is given by P\\{M_n \\leq z \\} = P\\{Y_1 \\leq z, \\dots, Y_n \\leq z\\} = \\left(F(z)\\right)^n. However, F is often not known in practice. This gap is filled by Extreme Value Theory, which studies approximate families of models for F^n so that extremes can be modeled and their uncertainty quantified.\nIt is well known, due to the central limit theorem, that the average of a set of iid random variables will converge to the Normal distribution if the mean and variance both exist and are finite. The Fisher-Tippett-Gnedenko (FTG) Theorem provides an analogous result for the maximum. It was developed in a series of papers by Fréchet (1927), Fisher and Tippett (1928), and Gnedenko (1943). Independently, von Mises (1936) proposed a similar result. The FTG Theorem states that if the maximum can be scaled so that it converges, then the scaled maximum will converge to either a Gumbel, Fréchet or Weibull distribution (Coles 2001, p46); no other limits are possible.\n\n\n\n\n\n\nFisher-Tippett-Gnedenko Theorem\n\n\n\nIf there exist sequences \\{a_n\\} and \\{b_n\\} such that \n  P\\left\\{ \\frac{(M_n - a_n)}{b_n} \\leq z \\right\\} \\rightarrow G(z) \\quad \\text{as} \\quad n \\to \\infty,\n where G is a non-degenerate cumulative distribution function, then G belongs to one of the following families: \\begin{align*}\n  &\\text{Gumbel} :  && G(z) = \\exp\\left(-\\exp \\left[- \\Big(\\frac{z-b}{a}\\Big) \\right] \\right), \\quad -\\infty &lt; z &lt; \\infty , \\\\\n  &\\text{Fréchet} : && G(z) =\n    \\begin{cases}\n      0 ,                                                           & z \\leq b  , \\\\\n      \\exp \\left( - \\left( \\frac{z-b}{a}\\right)^{-\\alpha} \\right) , & z &gt; b    ,\n    \\end{cases}                     \\\\\n  &\\text{Weibull} : && G(z) =\n    \\begin{cases}\n      \\exp \\left( - \\left(- \\left[\\frac{z-b}{a}\\right]\\right)^{\\alpha} \\right) , & z &lt; b  ,    \\\\\n      1 ,                                                                        & z \\geq b  ,\n    \\end{cases}\n\\end{align*} for parameters a, b and \\alpha where a, \\alpha &gt;0.\n\n\nThe three types of limits correspond to different forms of the tail behaviour of F.\n\nWhen F has a finite upper bound, such as with a uniform distribution, then G is a Weibull distribution.\nWhen F has exponential tails, such as with a Normal distribution or a Gamma distribution, then G is a Gumbel distribution.\nWhen F has heavy tails including polynomial decay, then G is a Fréchet distribution. One example is when F itself is a Fréchet distribution with F(y)= e^{-1/y}, y&gt;0.\n\nIf we take the negative of the random variables Y_1,\\dots,Y_n, it becomes clear that a similar result holds for the minimum.\nThese three families of distributions can be further combined into a single family by using the following cdf known as the Generalized Extreme Value (GEV) distribution, \n  G(z) = \\exp\\left\\{ -\\left[ 1 + \\xi\\Big(\\frac{z - \\mu}{\\sigma} \\Big)\\right]^{-1/\\xi} \\right\\} ,\n\\tag{2.15} where the domain of the function is \\{z: 1 + \\xi (z - \\mu)/\\sigma &gt;0 \\}. The location parameter is \\mu\\in\\mathbb{R}, \\sigma&gt;0 is the scale parameter, while \\xi\\in\\mathbb{R} is the shape parameter. When \\xi = 0 we obtain a Gumbel distribution with exponentially decaying tails. When \\xi &lt; 0 we get a Weibull distribution with a finite upper end, and when \\xi &gt; 0 we get a Fréchet family of distributions with heavy tails including polynomial tails.\nTo illustrate, suppose F is a standard Normal distribution N(0,1) and we have n=1000 observations, Y_1,\\dots,Y_n. Then we know that the distribution of the maximum is given by \n  P\\left\\{ \\max \\{Y_1, \\dots, Y_n\\} \\leq z \\right\\} = \\left[ \\Phi(z) \\right]^n,\n so the density of the maximum is given by \n  f(z) = n \\left[ \\Phi(z) \\right]^{n-1} \\phi(z).\n\\tag{2.16}\nAccording to the FTG theorem, the distribution of the maximum can be approximated by a Gumbel distribution. We can find the parameters of the Gumbel distribution by estimating them from simulated maximums. We simulate 2000 maximums (each from 1000 N(0,1) draws), and fit a Gumbel distribution to the resulting data. The resulting Gumbel distribution is shown in Figure 2.18 (in blue), along with the true distribution of the maximum given by Equation 2.16 (in black). The approximation is so good, that it is hard to distinguish the two distributions.\n\n\nCode\nn &lt;- 1000\nmaximums &lt;- replicate(2000, max(rnorm(n)))\n# Fit GEV distribution\ngev_fit &lt;- evd::fgev(maximums)$estimate\nx &lt;- seq(0, 8, l=500)\n# Plot true distribution and GEV distribution\nc(\n  Exact = dist_density(x = x, density = n * pnorm(x)^(n-1) * dnorm(x)),\n  `GEV approximation` = dist_gev(location = gev_fit[\"loc\"], scale = gev_fit[\"scale\"], shape = gev_fit[\"shape\"])\n) |&gt;\n  gg_density()\n\n\n\n\n\n\n\n\nFigure 2.18: Distribution of the maximum of 1000 N(0,1) draws. Here we have simulated 2000 such maximums and shown the estimated GEV distribution (in black), along with the true distribution (in blue). The approximation is so good, that it is hard to distinguish the two distributions.\n\n\n\n\n\n\nThe Generalized Pareto Distribution\nThe Peaks Over Threshold (POT) approach regards extremes as observations greater than a threshold u. The probability distribution of exceedances above a specified threshold u can be expressed as the conditional distribution \n  H(y) = P\\left \\{Y \\leq u + y \\mid Y &gt; u \\right \\}\n  = \\frac{ F(u+y) - F(u)}{1 - F(u)}.\n\\tag{2.17} When the distribution F satisfies the FTG theorem, then (Coles 2001, p75) H is a Generalized Pareto Distribution (GPD) defined by \n    H(y) \\approx 1 - \\Big( 1 + \\frac{\\xi y}{\\sigma_u} \\Big)^{-1/\\xi} ,\n   \\tag{2.18} where the domain of H is \\{y: y &gt;0 \\text{ and } (1 + \\xi y)/\\sigma_u &gt;0 \\}, and \\sigma_u = \\sigma + \\xi(u- \\mu). The GPD parameters are determined from the associated GEV parameters. In particular, the shape parameter \\xi is the same in both distributions.\nContinuing the previous example, we now look at the probability distribution of exceedances above 3 from a N(0,1) distribution. We simulate 100,000 N(0,1) values and only keep those above 3. Then two GPD estimates are drawn. The blue GPD uses the parameters obtained previously from the GEV estimate, while the orange GPD estimates the parameters from the exceedances.\n\n\nCode\n# Generate random normal data above 3 and fit GPD distribution\ndf &lt;- tibble(y = rnorm(1e5)) |&gt; filter(y &gt;= 3)\ngpd_fit &lt;- evd::fpot(df$y, 3)$estimate\n# Show both distributions\nc(\n  `GPD_from_GEV` = dist_gpd(location = 3, scale = gev_fit[\"scale\"], shape = gev_fit[\"shape\"]),\n  `GPD_estimate` = dist_gpd(location = 3, scale = gpd_fit[\"scale\"], shape = gpd_fit[\"shape\"])\n) |&gt;\n  gg_density() +\n  geom_rug(data = df, aes(x = y))\n\n\n\n\n\n\n\n\nFigure 2.19: Conditional distribution of exceedances above 3 from N(0,1) draws. Here we have simulated 100,000 values and only kept those above 3. The density implied by the GEV distribution is shown in blue, and an estimated GPD distribution is shown in orange.\n\n\n\n\n\nThe GEV (blue) estimate is better because it is based on more information (2000 maximums rather than 151 exceedances).\n\n\n\n\nColes, S. 2001. An Introduction to Statistical Modeling of Extreme Values. Springer Series in Statistics. London UK: Springer.\n\n\nFisher, R A, and L H C Tippett. 1928. “Limiting Forms of the Frequency Distribution of the Largest or Smallest Member of a Sample.” Mathematical Proceedings of the Cambridge Philosophical Society 24 (2): 180–90.\n\n\nForbes, C S, M Evans, N Hastings, and B Peacock. 2011. Statistical Distributions. 4th ed. Hoboken, NJ: Wiley-Blackwell. https://doi.org/10.1002/9780470627242.\n\n\nFréchet, M. 1927. “Sur La Loi de Probabilité de l’écart Maximum.” Annales de La Société Polonaise de Mathématique 6: 93–116.\n\n\nGnedenko, B. 1943. “Sur La Distribution Limite Du Terme Maximum d’une Serie Aleatoire.” Annals of Mathematics, 423–53.\n\n\nHyndman, R J. 1996. “Computing and Graphing Highest Density Regions.” The American Statistician 50 (2): 120–26. http://www.jstor.org/stable/2684423.\n\n\nHyndman, R J, and Y Fan. 1996. “Sample Quantiles in Statistical Packages.” The American Statistician 50 (4): 361–65.\n\n\nJohnson, N L, S Kotz, and N Balakrishnan. 1994. Continuous Univariate Distributions. 2nd ed. Vol. 1. New York, USA: John Wiley & Sons.\n\n\n———. 1995. Continuous Univariate Distributions. 2nd ed. Vol. 2. New York, USA: John Wiley & Sons.\n\n\nRousseeuw, Peter J, and Christophe Croux. 1993. “Alternatives to the Median Absolute Deviation.” Journal of the American Statistical Association 88: 1273–83. https://doi.org/10.1080/01621459.1993.10476408.\n\n\nScott, D W. 2015. Multivariate Density Estimation: Theory, Practice, and Visualization. 2nd ed. Wiley.\n\n\nSheather, S J, and M C Jones. 1991. “A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation.” Journal of the Royal Statistical Society. Series B 53 (3): 683–90.\n\n\nSilverman, B W. 1986. Density Estimation for Statistics and Data Analysis. Chapman; Hall.\n\n\nVan der Vaart, A W. 2000. Asymptotic Statistics. Cambridge University Press.\n\n\nvon Mises, R. 1936. “La Distribution de La Plus Grande de n Valeurs.” Rev. Math. Union Interbalcanique 1: 141–60.\n\n\nWand, M P, and M C Jones. 1995. Kernel Smoothing. New York: Chapman & Hall.\n\n\nWasserman, L. 2004. All of Statistics: A Concise Course in Statistical Inference. Springer Texts in Statistics. New York: Springer. https://doi.org/10.1007/978-0-387-21736-9.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Univariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html",
    "href": "03-multivariate.html",
    "title": "3  Multivariate probability distributions",
    "section": "",
    "text": "3.1 Hiding in high-dimensional space\nWhile we will cover anomaly detection in univariate data, most of the methods we will discuss are for multivariate data. We will therefore need to understand some basic concepts of multivariate probability distributions.\nWhen first thinking about multivariate probability distributions, it can take some time to develop an intuition for how they behave. One way to start developing this intuition is to plot data with increasing numbers of dimensions, starting with the univariate case.\nLet’s take the n01 data set, which contains 10 variables, each from independent standard Normal distributions. We will consider only the first three variables, but to make the example more interesting, we will consider cumulative sums of these variables. We will also add a couple of anomalies, and then see if we can find them using some data visualizations.\n# Construct example data set\ndf &lt;- n01 |&gt;\n  transmute(\n    V1 = v1,\n    V2 = v1 + v2,\n    V3 = v1 + v2 + v3\n  )\n# Add anomalies\ndf[999, ] &lt;- tibble(V1 = -2, V2 = 3.4, V3 = 1.3)\ndf[1000, ] &lt;- tibble(V1 = 1.2, V2 = -1.8, V3 = 1.1)\ndf$anomaly &lt;- c(rep(FALSE, 998), TRUE, TRUE)\nFirst, we can do a strip plot of each variable to visualize the data from a univariate perspective.\ndf |&gt;\n  tidyr::pivot_longer(V1:V3) |&gt;\n  ggplot(aes(x = value, y = 0, col = anomaly)) +\n  geom_jitter(width = 0) +\n  facet_grid(name ~ .) +\n  scale_y_discrete() +\n  labs(x = \"\", y = \"\") +\n  scale_color_manual(values = c(\"#999999\", \"#D55E00\"))\n\n\n\n\n\n\n\nFigure 3.1: Strip plots of each of the three variables, with anomalies shown in orange.\nThe only obvious feature of these graphs is that the variance increases in successive panels. This is because the variance of the sum of independent random variables is the sum of their variances. The two anomalies are plotted in each panel, but they are not obviously different from the other observations.\nNow we will plot pairwise scatterplots of the four variables, giving the bivariate perspective.\nGGally::ggpairs(df[, 1:3],\n  lower = list(\n    continuous = GGally::wrap(\"points\"),\n    mapping = aes(color = df$anomaly)\n  )\n) +\n  scale_color_manual(values = c(\"#999999\", \"#D55E00\"))\n\n\n\n\n\n\n\nFigure 3.2: Pairwise scatterplots of the three variables, with anomalies shown in orange.\nThis shows that there are positive relationships between the variables, with the strongest between V2 and V3. One of the anomalies is now clearly separate from the other observations in the plot of V2 vs V1 (the top left scatterplot). For this anomaly, the value of V1 is -2, which is not particularly unusual compared to the other observations. Similarly, the value of V2 is 3.4, which is also not particularly unusual. But because of their positive correlation, the combination of these two values is unusual, so it shows up in the 2-dimensional scatterplot. The other anomaly does not look particularly unusual in any of the plots.\nTo visualise trivariate relationships, we need to use a 3d-scatterplot. It is easiest to see this if it can spin. Try dragging the plot around with your mouse to view if from different angles. See if you can find a viewpoint where the second anomaly is clearly separated from the other observations.\nThis second anomaly is not particularly unusual in any combination of two variables, but it is unusual in the combination of all three variables.\nWhen looking for anomalies, it is important to consider all the variables together, rather than looking at each variable in isolation, or even looking at all the pairwise relationships. It is possible for an observation to be unusual in d dimensions, but not unusual in any of the lower-dimensional subsets of variables. In other words, there are more places for anomalies to hide in higher dimensions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#sec-hiding",
    "href": "03-multivariate.html#sec-hiding",
    "title": "3  Multivariate probability distributions",
    "section": "",
    "text": "# rgl::plot3d(df[, 1:3],\n#   size = 5,\n#   col = c(rep(\"#999999\", 998), rep(\"#D55E00\", 2))\n# )\n\n\n\n\nFigure 3.3: 3d scatterplot of the three variables, with anomalies shown in orange.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#sec-curse",
    "href": "03-multivariate.html#sec-curse",
    "title": "3  Multivariate probability distributions",
    "section": "3.2 The curse of dimensionality",
    "text": "3.2 The curse of dimensionality\nThe “curse of dimensionality” refers to the increasing sparseness of space as the number of dimensions increases.\nSuppose we observed a high-dimensional data set of independent U(-1,1) variables, and consider the proportion of points that lie outside the unit sphere (in the corners of the space) as the number of dimensions increases. First, let’s visualise what this means in 1, 2 and 3 dimensions. In 1 dimension, the unit sphere is just the interval [-1,1], so no points lie outside this interval. In 2 dimensions, the unit sphere is the circle with radius 1, sitting snugly within a square of side length 2, as shown in Figure 3.4.\n\n\nCode\ntibble(\n  x = runif(2000, -1, 1),\n  y = runif(2000, -1, 1),\n  within_sphere = (x^2 + y^2) &lt; 1\n) |&gt;\n  ggplot(aes(x = x, y = y, color = within_sphere)) +\n  geom_point() +\n  coord_fixed() +\n  scale_color_manual(values = c(\"#D55E00\", \"#777777\")) +\n  guides(col = \"none\")\n\n\n\n\n\n\n\n\nFigure 3.4: 2000 simulated observations from two independent U(-1,1) distributions, with points within the unit circle shown in grey, and points outside the unit circle shown in orange.\n\n\n\n\n\nThe proportion of points lying outside the circle in the corners of the space (shown in orange) is 21.5%.\nThe equivalent plot in 3-dimensions is shown in Figure 3.5. The unit sphere is now a ball sitting within a box, and 47.6% of points lie outside the ball.\n\nCode\ndf &lt;- tibble(\n  x = runif(2000, -1, 1),\n  y = runif(2000, -1, 1),\n  z = runif(2000, -1, 1),\n  within_sphere = (x^2 + y^2 + z^2) &lt; 1,\n  color = if_else(within_sphere, \"#777777\", \"#D55E00\")\n)\n# rgl::plot3d(df[, 1:3],\n#   size = 5,\n#   col = df$color\n# )\n\n\n\n\n\n\n\n\n\nFigure 3.5: 2000 simulated observations from three independent U(-1,1) distributions, with points within the unit sphere shown in grey, and points outside the unit sphere shown in orange.\n\n\n\nAs the number of dimensions grows, it becomes increasingly likely that points will lie in the corners of the space. In fact, the proportion of points lying outside the unit sphere in d dimensions is 1 minus the ratio of the volume of the unit sphere to the volume of the whole space. This is given by \n1 - \\frac{\\pi^{d/2}}{2^d\\Gamma(d/2 + 1)}\n where \\Gamma is the gamma function, and is plotted in Figure 3.6.\n\n\nCode\ntibble(d = seq(10)) |&gt;\n  mutate(outside = 100 * (1 - (pi^(d / 2) / (2^d * gamma(d / 2 + 1))))) |&gt;\n  ggplot(aes(x = d, y = outside)) +\n  geom_line() +\n  labs(x = \"Dimensions (d)\", y = \"Percentage of points outside unit sphere\") +\n  scale_x_continuous(breaks = seq(10), minor_breaks = NULL)\n\n\n\n\n\n\n\n\nFigure 3.6: For observations from a d-dimensional U(-1,1) distribution, the percentage of points lying outside the unit sphere in the corners of the space.\n\n\n\n\n\nRemarkably, by 10 dimensions, almost all points lie outside the unit sphere, and live in the corners of the space. Almost no points are in the centre of the space. This occurs because, with enough variables, at least one of the univariate observations is going to lie in the tails of the distribution, and so the multivariate observation will not lie near the centre of the space.\nAnother way to think about this is to consider hypercubes that contain 50% of the observations. These must have sides of length 2^{1-1/d}, in order for the volume of the hypercube to be 2^{d-1}, exactly half of the entire space. For d=1, the hypercube is a unit interval. For d=2, it is a square of side length \\sqrt{2} \\approx 1.41 as shown in Figure 3.7. For d=3, the central cube must have side length 2^{2/3} \\approx 1.59, and so on.\n\n\nCode\ntibble(\n  x = runif(2000, -1, 1),\n  y = runif(2000, -1, 1),\n  within_cube = (abs(x) &lt; 2^(-1 / 2)) & (abs(y) &lt; 2^(-1 / 2))\n) |&gt;\n  ggplot(aes(x = x, y = y, color = within_cube)) +\n  geom_point() +\n  coord_fixed() +\n  scale_color_manual(values = c(\"#D55E00\", \"#777777\")) +\n  guides(col = \"none\")\n\n\n\n\n\n\n\n\nFigure 3.7: 2000 simulated observations from two independent U(-1,1) distributions, with the central square containing 50% of the observations.\n\n\n\n\n\nAs the dimension d increases, the size of the hypercube containing 50% of the observations must also increase. Figure 3.8 shows the side length of these hypercubes plotted against the dimension d.\n\n\nCode\ntibble(d = seq(20)) |&gt;\n  mutate(length = 2^(1 - 1 / d)) |&gt;\n  ggplot(aes(x = d, y = length)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Dimensions (d)\", y = \"Side length of hypercube containing 50% of points\") +\n  scale_x_continuous(breaks = seq(2, 20, by = 2), minor_breaks = NULL) +\n  ylim(1, 2)\n\n\n\n\n\n\n\n\nFigure 3.8: For observations from a d-dimensional U(-1,1) distribution, the side length of a hypercube which contains 50% of points.\n\n\n\n\n\nThe side lengths of these hypercubes increasingly approaches the maximum of 2 as the number of dimensions increases. So the size of the neighbourhood containing 50% of the observations becomes almost as large as the whole space. Therefore, we cannot use methods that rely on local neighbourhoods in high dimensions, as these neighbourhoods must become so large as to no longer be “local”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#sec-joint",
    "href": "03-multivariate.html#sec-joint",
    "title": "3  Multivariate probability distributions",
    "section": "3.3 Joint probability distributions",
    "text": "3.3 Joint probability distributions\nNow we have a sense of what it means to have data in many dimensions, let’s consider how we can describe the joint probability distribution of multiple variables.\n\nStatistical definitions\nSuppose \\bm{Y} = [Y_1,\\dots,Y_n]' is a random variable taking values in \\mathbb{R}^d, the d-dimensional real numbers. Then the joint distribution of \\bm{Y} is defined by the joint cdf \nF(\\bm{y}) = \\text{Pr}(\\bm{Y} \\le \\bm{y}),\n while the joint density function is given by \nf(\\bm{y}) = \\frac{\\partial^n F(\\bm{y})}{\\partial y_1 \\dots \\partial y_d}.\n\nThe marginal cdfs are defined by F_i(y) = \\text{Pr}(Y_i \\le y), with corresponding marginal pdfs given by f_i(y) = F_i'(y), i=1,\\dots,d.\nIf the variables are independent, then the joint pdf is the product of the marginal pdfs, f(\\bm{y}) = \\prod_{i=1}^d f_i(y_i).\nThe expected value of \\bm{y} is given by \n\\text{E}(\\bm{Y}) = \\int_{\\mathbb{R}^d} \\bm{y} f(\\bm{y})d\\bm{y},\n and the covariance matrix is given by \n\\text{Var}(\\bm{Y}) = \\text{E}[(\\bm{Y}-\\text{E}(\\bm{Y}))(\\bm{Y}-\\text{E}(\\bm{Y}))'].\n The covariance matrix is a d\\times d matrix, with (i,j)th element given by \\text{Cov}(Y_i,Y_j) = \\text{E}[(Y_i-\\text{E}(Y_i))(Y_j-\\text{E}(Y_j))]. The diagonal elements are the variances of the individual variables, while the off-diagonal elements are the covariances between the variables.\n\n\nMultivariate Normal distribution\nIf random variable \\bm{Y} has a multivariate Normal distribution, we write \\bm{Y} \\sim \\text{N}(\\bm{\\mu}, \\bm{\\Sigma}), where \\bm{\\mu} is the mean and \\bm{\\Sigma} is the covariance matrix.\nThe multivariate Normal distribution has pdf given by \nf(\\bm{y}; \\bm{\\mu}, \\bm{\\Sigma}) = (2\\pi)^{-d/2}|\\bm{\\Sigma}|^{-1/2} \\exp\\left\\{-\\frac{1}{2}(\\bm{y}-\\bm{\\mu})'\\bm{\\Sigma}^{-1}(\\bm{y}-\\bm{\\mu})\\right\\}.\n The notation |\\bm{\\Sigma}| denotes the determinant of the matrix \\bm{\\Sigma}.\nMultivariate Normal distributions have the interesting property that the marginal distributions are also Normal.\n\n\nFurther reading\nA good reference on multivariate probability distributions is Kotz, Balakrishnan, and Johnson (2000).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#sec-mhdr",
    "href": "03-multivariate.html#sec-mhdr",
    "title": "3  Multivariate probability distributions",
    "section": "3.4 Highest density regions",
    "text": "3.4 Highest density regions\nAs with univariate distributions, a highest density region for a multivariate distribution is defined as the region of the sample space where the density is higher than a given threshold. Suppose we have a multivariate random variable \\bm{Y} with a smooth, continuous density function f. Then the 100(1-\\alpha)% HDR is the set \n  R_\\alpha = \\{\\bm{y}: f(\\bm{y}) \\ge f_\\alpha\\}\n\\tag{3.1} where P(\\bm{Y} \\in R_\\alpha) = 1-\\alpha.\nHDRs are equivalent to level sets of the density function, and so can be plotted as contours for bivariate density functions. For example, the bivariate Normal distribution with mean (0,0) and covariance matrix \\Sigma = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} is shown in Figure 3.9 as a series of HDR contours, each containing an additional 10% of the probability mass.\n\n\nCode\nmu &lt;- c(0, 0)\nSigma &lt;- matrix(c(1, 0.5, 0.5, 1), 2, 2)\ndistributional::dist_multivariate_normal(list(mu), list(Sigma)) |&gt;\n  gg_density(hdr = \"fill\") +\n  labs(\n    x = latex2exp::TeX(\"$y_1$\"), y = latex2exp::TeX(\"$y_2$\"),\n    title = latex2exp::TeX(\"Contours of $f(y_1,y_2)$\")\n  )\n\n\n\n\n\n\n\n\nFigure 3.9: Bivariate Normal distribution with mean (0,0) and covariance matrix \\Sigma = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}. The HDR contours cover 10%, 20%, \\dots, 90% of the probability mass.\n\n\n\n\n\nSimilarly, we can obtain HDRs for a mixture distribution. Suppose we had two bivariate Normal distributions with means (0,0) and (3,1), and covariance matrices equal to \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} and the identity matrix \\bm{I}_2 respectively. Then the HDRs for an equal mixture of these two distributions is shown in Figure 3.10.\n\n\nCode\nmu1 &lt;- c(0, 0)\nmu2 &lt;- c(3, 1)\nSigma1 &lt;- rbind(c(1, 0.5), c(0.5, 1))\nSigma2 &lt;- diag(2)\nmixture_density &lt;- dist_mixture(\n  dist_multivariate_normal(mu = list(mu1), sigma = list(Sigma1)),\n  dist_multivariate_normal(mu = list(mu2), sigma = list(Sigma2)),\n  weights = c(0.5, 0.5)\n)\nmixture_plot &lt;- mixture_density |&gt;\n  gg_density(hdr = \"fill\") +\n  labs(\n    x = latex2exp::TeX(\"$y_1$\"), y = latex2exp::TeX(\"$y_2$\"),\n    title = latex2exp::TeX(\"Contours of $f(y_1,y_2)$\")\n  )\nmixture_plot\n\n\n\n\n\n\n\n\nFigure 3.10: Bivariate mixture distribution of two equally weighted Normal components with means (0,0) and (3,1), and covariance matrices \\Sigma = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} and \\bm{I}_2 respectively. The HDR contours cover 10%, 20%, \\dots, 90% of the probability mass.\n\n\n\n\n\nHere, the 10%, 20%, 30% and 40% HDRs contain disconnected regions, but for the larger HDRs, there is just one region for each.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#multivariate-quantiles",
    "href": "03-multivariate.html#multivariate-quantiles",
    "title": "3  Multivariate probability distributions",
    "section": "3.5 Multivariate quantiles",
    "text": "3.5 Multivariate quantiles\nUnlike the univariate case, there is no unique definition of a multivariate quantile. There are many different definitions, and each has its own advantages and disadvantages.\nIn this book, we are mostly concerned with sample multivariate quantiles, and one useful definition for sample quantiles is based on data depth. We will discuss this approach in Section 5.5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#robust-covariance-estimation",
    "href": "03-multivariate.html#robust-covariance-estimation",
    "title": "3  Multivariate probability distributions",
    "section": "3.6 Robust covariance estimation",
    "text": "3.6 Robust covariance estimation\nThe sample covariance matrix is a useful measure of the spread of a multivariate distribution, given by \n\\bm{S} = \\frac{1}{n-1} \\sum_{i=1}^n (\\bm{y}_i - \\bar{\\bm{y}})(\\bm{y}_i - \\bar{\\bm{y}})',\n\\tag{3.2} However, it is sensitive to outliers, and so is not suitable for our purposes. There have been many robust estimators of covariance proposed in the literature, but we will discuss only one, relatively simple, estimator known as the “orthogonalized Gnanadesikan/Kettenring” (OGK) estimator (Gnanadesikan and Kettenring 1972; Maronna and Zamar 2002).\nSuppose we have two random variables X and Y. Then the variance of their sum and difference is given by \\begin{align*}\n  \\text{Var}(X+Y) &= \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y) \\\\\n  \\text{Var}(X-Y) &= \\text{Var}(X) + \\text{Var}(Y) - 2\\text{Cov}(X,Y).\n\\end{align*} The difference between these two expressions is \n  \\text{Var}(X+Y) - \\text{Var}(X-Y) = 4\\text{Cov}(X,Y),\n so that the covariance can be expressed as \n\\text{Cov}(X,Y) = \\frac{1}{4} \\left[ \\text{Var}(X+Y) - \\text{Var}(X-Y)\\right].\n Now we can use a robust estimate of variance, such as the one based on Qn (Section 2.6), to estimate the two variances on the right hand side, giving \n\\hat{s}(X,Y) = \\frac{1}{4} \\left[ s_{\\text{Qn}}^2(X+Y) - s_{\\text{Qn}}^2(X-Y)\\right].\n We can repeat this for each pair of variables, to obtain a robust estimate of the covariance matrix, \\bm{S}^*. The diagonals can be obtained using the same robust measure of variance. This is known as the Gnanadesikan-Kettenring estimator. The resulting matrix is symmetric, but not necessarily positive definite, which is a requirement of a covariance matrix. So some additional iterative steps are applied to “orthogonalize” it.\n\nCompute the eigenvector decomposition of \\bm{S^*}, so that \\bm{S}^* = \\bm{U}\\bm{\\Lambda}\\bm{U}^{-1}.\nProject the data onto the basis eigenvectors\nEstimate the variances (robustly) in the coordinate directions.\nThen the robust covariance matrix is given by \n\\bm{S}_{\\text{OGK}} = \\bm{U}\\bm{\\Lambda}^*\\bm{U}^{-1},\n   \\tag{3.3} where \\bm{\\Lambda}^* is a diagonal matrix with the robust variances on the diagonal.\n\nThese orthogonalization steps are usually repeated one more time.\nThis procedure is implemented in the covOGK function in the robustbase package (Maechler et al. 2023).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#multivariate-kernel-density-estimation",
    "href": "03-multivariate.html#multivariate-kernel-density-estimation",
    "title": "3  Multivariate probability distributions",
    "section": "3.7 Multivariate kernel density estimation",
    "text": "3.7 Multivariate kernel density estimation\nSuppose our observations are d-dimensional vectors, \\bm{y}_1,\\dots,\\bm{y}_n. Then the multivariate version of Equation 2.6 is given by (Scott 2015) \n  \\hat{f}(\\bm{y}) = \\frac{1}{n} \\sum_{i=1}^n K_H(\\bm{y} - \\bm{y}_i),\n\\tag{3.4} where K_H is a multivariate probability density with covariance matrix \\bm{H}. In this book, we will use a multivariate Gaussian kernel given by \n  K_H(\\bm{u}) = (2\\pi)^{-d/2} |\\bm{H}|^{-1/2} \\exp \\{-\\textstyle\\frac12 \\bm{u}'\\bm{H}^{-1}\\bm{u} \\}.\n\n\nBivariate kernel density estimation\nTo illustrate the idea, consider a simple bivariate example of 10 observations: the first 10 eruption durations from 2021 that are in the oldfaithful data set, along with the corresponding waiting times until the following eruption. These are shown in the figure below along with the contours of bivariate kernels placed over each observation. Here we have used a bivariate Gaussian kernel with bandwidth matrix given by \\bm{H} = \\left[\\begin{array}{rr}265 & 4139 \\\\ 4139 & 152871\\end{array}\\right].\n\n\nCode\nh &lt;- sqrt(diag(H))\nk &lt;- tidyr::expand_grid(\n  x = seq(-3 * h[1], 3 * h[1], l = 100),\n  y = seq(-3 * h[2], 3 * h[2], l = 100)\n) |&gt;\n  mutate(z = mvtnorm::dmvnorm(x = cbind(x, y), sigma = H))\nof2021kde &lt;- of2021 |&gt;\n  mutate(k = list(k)) |&gt;\n  tidyr::unnest(k) |&gt;\n  mutate(x = x + duration, y = y + waiting)\nggplot() +\n  geom_contour(\n    data = of2021kde, aes(x = x, y = y, group = eruption, z = z),\n    bins = 4, col = \"gray\"\n  ) +\n  geom_point(data = of2021, mapping = aes(x = duration, y = waiting)) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 3.11: Contours of bivariate kernels centred over the first ten observations in 2021 from the oldfaithful data set.\n\n\n\n\n\nIf we add the bivariate kernel functions as in Equation 3.4, we obtain the bivariate kernel density estimate shown below. The contours shown correspond to the 10%, 20%, \\dots, 90% highest density regions (HDRs) of the density estimate.\n\n\nCode\noldfaithful |&gt;\n  filter(as.Date(time) &gt; \"2021-01-01\") |&gt;\n  head(10) |&gt;\n  select(duration, waiting) |&gt;\n  dist_kde() |&gt;\n  gg_density(show_points = TRUE) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 3.12: Bivariate kernel density estimate formed by summing the kernels shown in Figure 3.11.\n\n\n\n\n\nNow we will apply the method to the full data set, other than the 2 hour eruption and observations where the subsequent waiting time is more than 2 hours (which are likely to be data errors).\nWe will use the dist_kde() function, which uses a bivariate Gaussian kernel, and the bandwidth matrix given by the kde_bandwidth() function.\n\noldfaithful |&gt;\n  filter(duration &lt; 7000, waiting &lt; 7000) |&gt;\n  select(duration, waiting) |&gt;\n  dist_kde() |&gt;\n  gg_density(show_points = TRUE) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\nFigure 3.13: Bivariate kernel density estimate with default bandwidths.\n\n\n\n\n\nHere we see that the short durations tended to be followed by a short waiting time until the next duration, while the long durations tend to be followed by a long waiting time until the next duration. There are two anomalous eruptions: the one with the 1 second duration, and one where a short duration was followed by a long waiting time. The unusual durations between 150 and 180 seconds can be followed by either short or long durations.\n\n\nBandwidth matrix selection\nThe optimal bandwidth matrix (minimizing the mean integrated squared error between the true density and its estimate) is of the order n^{-2/(d+4)}. If such a bandwidth matrix is used, then the estimator converges at rate n^{-4/(d+4)}, implying that kernel density estimation becomes increasingly difficult as the dimension d increases. This is to be expected given the curse of dimensionality (Section 3.2), as the number of observations required to obtain a good estimate increases exponentially with the dimension. In practice, we rarely attempt to estimate a density in more than d=3 dimensions.\nIf the underlying density is Normal with mean \\bm{\\mu} and variance \\bm{\\Sigma}, then the optimal bandwidth matrix is given by \n  \\bm{H} = \\left(\\frac{4}{d+2}\\right)^{2/(d+4)} n^{-2/(d+4)} \\bm{\\Sigma}.\n\\tag{3.5} Notice that in the univariate case, when d=1, this rule gives the same bandwidth as the rule of thumb given by Equation 2.7.\nReplacing \\bm{\\Sigma} by the robust covariance matrix \\bm{S}_{\\text{OGK}} (Equation 3.3), we obtain a robust normal reference rule, calculated by kde_bandwidth(). Figure 3.13 shows a bivariate kernel density estimate computed using this approach.\nA more data-driven approach, is the “plug-in” estimator (Chacón and Duong 2018) implemented by ks::Hpi(), which is a generalization of the plug-in bandwidths popular for univariate kernel density estimation Section 2.7. While this typically leads to better estimates of the density as a whole, it can lead to worse estimates in the tails of the distribution where anomalies tend to lie.\nA common approach is to specify \\bm{H} to be diagonal, with elements equal to the squares of univariate bandwidth rules. (The univariate bandwidths are usually defined as the standard deviation of the kernel, while multivariate bandwidth matrices correspond to the covariance matrix of the kernel. Hence, the univariate bandwidths need to be squared if used in a bandwidth matrix.) That is, if h_1,\\dots,h_d are the univariate bandwidths for each of the variables, then the corresponding diagonal bandwidth matrix is given by \\bm{H} = \\text{diag}(h_1^2,\\dots,h_d^2). For example, geom_density_2d() uses a bivariate Gaussian kernel with diagonal bandwidth matrix where the diagonal values are given by the squares of Equation 2.14. However, this approach leads to bandwidths that are too small, as it ignores the convergence properties of the multivariate estimator. Additionally, any diagonal bandwidth matrix implicitly assumes that the variables are uncorrelated, and leads to more biased estimators.\nAs with univariate density estimation, when estimating the density in the tails of the distribution, it is usually better to have a larger bandwidth than those given by the rules above. By default, the kde_bandwidth() function will return the bandwidth matrix given by the normal reference rule (Equation 3.5) with \\bm{\\Sigma} estimated using Equation 3.3. When method = \"double\", it will return the same matrix multiplied by 4 (because it is a variance, doubling the scale increases the matrix by a factor of four).\n\n\nFurther reading\nThere is a rich literature on multivariate kernel density estimation. Good starting points are Scott (2015) or Chacón and Duong (2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#conditional-probability-distributions",
    "href": "03-multivariate.html#conditional-probability-distributions",
    "title": "3  Multivariate probability distributions",
    "section": "3.8 Conditional probability distributions",
    "text": "3.8 Conditional probability distributions\nA fundamental concept in statistics is a conditional probability distribution; that is, the distribution of a random variable conditional on the values of other (possibly random) variables.\nAlmost all statistical modelling involves the estimation of conditional distributions. For example, a regression is a model for the conditional distribution of a response variable given the values of a set of predictor variables. In its simplest form, we assume the conditional distribution is normal, with constant variance, and mean equal to a linear function of the predictor values. Generalized linear models allow for non-normal conditional distributions, while generalized additive models allow for non-linear relationships between the response and the predictors.\nThe conditional cdf of Y given X_1,\\dots,X_n is defined by the conditional cdf \nF(y\\mid x_1,\\dots,x_n) = \\text{Pr}(Y \\le y \\mid  X_1 = x_1,\\dots,X_n = x_n).\n The conditional pdf is given by \nf(y \\mid  x_1, \\dots, x_n) = \\frac{f(y,x_1,\\dots,x_n)}{f(x_1,\\dots,x_n)}.\n The conditional pdf can be thought of as slices of the joint pdf, with the values of x_1,\\dots,x_n fixed, rescaled to ensure the conditional pdfs integrate to 1. For example, f(y_1 | y_2) is equal to a scaled slice of the joint pdf f(y_1,y_2) at y_2. Figure 3.14 shows some examples for the distribution shown at Figure 3.10 at several values of y_2. The left plot shows the joint density, with horizontal lines indicating where conditioning (or slicing) occurs at different values of y_2. The right plot shows the resulting conditional density functions.\n\n\nCode\n# Conditioning points\ny2_slice &lt;- -2:3\n# Compute bivariate density over a grid\ndf &lt;- expand.grid(\n  y1 = seq(-3, 6, length = 100),\n  y2 = y2_slice\n)\ndf$density &lt;- density(mixture_density, at = as.matrix(df))[[1]]\n# Scaling factor for each density\nscale_cond_density &lt;- df |&gt;\n  summarise(scale = sum(density), .by = y2) |&gt;\n  mutate(scale = scale / max(scale))\n# Scale each conditional density\ndf &lt;- df |&gt;\n  left_join(scale_cond_density, by = \"y2\") |&gt;\n  mutate(\n    density = density / scale,\n    density = density / max(density) * 0.9\n  )\n# Joint density plot\nplot1 &lt;- mixture_plot +\n  guides(fill = \"none\") +\n  scale_y_continuous(breaks = y2_slice, minor_breaks = NULL) +\n  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6)) +\n  geom_hline(aes(yintercept = y2), data = tibble(y2 = y2_slice), color = \"#4c93bb\")\n# Conditional density plots\nplot2 &lt;- df |&gt;\n  ggplot(aes(x = y1, y = density + y2, group = y2)) +\n  geom_ribbon(aes(ymin = y2, ymax = density + y2, xmin = -2, xmax = 4),\n    col = \"#4c93bb\", fill = \"#4c93bb\"\n  ) +\n  labs(\n    x = latex2exp::TeX(\"$y_1$\"), y = latex2exp::TeX(\"$y_2$\"),\n    title = latex2exp::TeX(\"Conditional densities: $f(y_1|y_2)$\")\n  ) +\n  scale_y_continuous(minor_breaks = NULL, breaks = y2_slice) +\n  coord_cartesian(xlim = c(-2.6, 5.5), ylim = c(-2.6, 3.6))\n# Show plots side by side\npatchwork::wrap_plots(plot1, plot2, nrow = 1)\n\n\n\n\n\n\n\n\nFigure 3.14: Conditional distribution of Y_2|Y_1, where (Y_1,Y_2) has the joint distribution plotted in Figure 3.10. The left plot shows the joint density with the values of y_2 where we will condition, while the right plot shows conditional density functions at different values of y_2.\n\n\n\n\n\nAnother neat property of Normal distributions is that the conditional distribution of a subset of variables is also Normal. For example, suppose \\bm{Y} = (Y_1,Y_2,Y_3) is a multivariate Normal random variable. Then the conditional distribution of Y_1 given Y_2 and Y_3 is also Normally distributed.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "03-multivariate.html#sec-scaling",
    "href": "03-multivariate.html#sec-scaling",
    "title": "3  Multivariate probability distributions",
    "section": "3.9 Multivariate scaling",
    "text": "3.9 Multivariate scaling\nSome anomaly detection methods will require the data to have been scaled first, to prevent the variables with the largest ranges from dominating.\n\nZ-scores\nOne common approach is to apply univariate scaling on each variable, by subtracting its mean and dividing by its standard deviation. This gives z-scores.\nIf our data are given by y_1,\\dots,y_n, then their z scores are given by z_i = (y_i - \\bar{y})/s_y where \\bar{y} is the mean and s_y is the standard deviation of the observations. These are easily computed using the scale() function.\nHowever, the mean and standard deviation may be affected by anomalies, and it would be better to use an approach that was robust to anomalies.\n\n\nRobust z-scores\nThe simplest way to make the scaling robust to anomalies is to use the median instead of the mean, and to use a robust estimate of the standard deviation such as s_{\\text{Qn}} (Section 2.6). Robust z-scores computed in this way can be obtained using the mvscale() function with the argument cov = NULL.\n\n\nMultivariate z-scores\nIt is sometimes useful to scale a set of multivariate data, so that the scaled variables are uncorrelated with each other, and have mean zero and unit variance.\nSuppose each observation is a vector \\bm{y}, with most observations coming from a distribution with mean \\bm{\\mu} and covariance matrix \\bm{\\Sigma}. Let \\bm{\\Sigma}^{-1} = \\bm{U}' \\bm{U} be the Cholesky decomposition (see Section A.6) of \\bm{\\Sigma}^{-1}. Then the data can be scaled by \\bm{U}, giving \\bm{z} = \\bm{U} (\\bm{y} - \\bm{\\mu}), so that the covariance matrix of the scaled data is the identity matrix. This can be thought of as both scaling and rotating the data, so that the variables are uncorrelated with mean zero and unit variance. It is the multivariate equivalent of calculating z-scores. They can be computed using the mvscale() function with the arguments center = mean and cov = stats::cov.\n\n\nRobust multivariate z-scores\nWhen there may be anomalies in the data set, we can replace the mean by the pointwise median (i.e., the vector of medians for each variable), and estimate \\bm{\\Sigma} using the robust covariance matrix \\bm{S}_{\\text{OGK}} (Equation 3.3). The mvscale() function in the weird package will do this for us, by default.\nLet’s consider the oldfaithful data again, where durations are much longer than waiting times. Figure 3.15 shows the data before and after scaling. The scaling has rotated the data a little, and the variables are now less correlated, and centered on zero. The right plot still appears to show some positive relationship between the variables, because the covariance matrix used for scaling was computed robustly, and so was largely based on the upper cluster of points.\n\n\nCode\nof &lt;- oldfaithful |&gt;\n  filter(duration &lt; 7000, waiting &lt; 7000)\np1 &lt;- of |&gt;\n  ggplot(aes(x = duration, y = waiting)) +\n  geom_point(alpha = 0.2) +\n  labs(\n    title = \"Original data\",\n    x = \"Duration of eruption (seconds)\",\n    y = \"Waiting time to next eruption (seconds)\"\n  )\np2 &lt;- mvscale(of, warning = FALSE) |&gt;\n  ggplot(aes(x = z1, y = z2)) +\n  geom_point(alpha = 0.2) +\n  labs(\n    title = \"Scaled data\",\n    x = latex2exp::TeX(\"$z_1$\"), y = latex2exp::TeX(\"$z_2$\")\n  )\npatchwork::wrap_plots(p1, p2, nrow = 1)\n\n\n\n\n\n\n\n\nFigure 3.15: Old Faithful duration and waiting time data, before and after scaling.\n\n\n\n\n\n\n\n\n\nChacón, J E, and T Duong. 2018. Multivariate Kernel Smoothing and Its Applications. Boca Raton, Florida: CRC Press.\n\n\nGnanadesikan, R., and J. R. Kettenring. 1972. “Robust Estimates, Residuals, and Outlier Detection with Multiresponse Data.” Biometrics 28 (1): 81–124.\n\n\nKotz, S, N Balakrishnan, and N L Johnson. 2000. Continuous Multivariate Distributions: Models and Applications. 2nd ed. Vol. 1. New York, USA: John Wiley & Sons.\n\n\nMaechler, M, P Rousseeuw, C Croux, V Todorov, A Ruckstuhl, M Salibian-Barrera, T Verbeke, M Koller, E L T Conceicao, and M A di Palma. 2023. robustbase: Basic Robust Statistics. http://robustbase.r-forge.r-project.org/.\n\n\nMaronna, R. A., and R. H. Zamar. 2002. “Robust Estimates of Location and Dispersion of High-Dimensional Datasets.” Technometrics 44 (4): 307–17.\n\n\nScott, D W. 2015. Multivariate Density Estimation: Theory, Practice, and Visualization. 2nd ed. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate probability distributions</span>"
    ]
  },
  {
    "objectID": "04-tests.html",
    "href": "04-tests.html",
    "title": "4  Statistical tests",
    "section": "",
    "text": "4.1 Examples\nThe early history of anomaly detection method involved statistical tests and assumptions about the distribution of the data. There are much better methods available these days, but these old methods are still widely used. So they are included here for historical interest, and to point out their flaws to those still using them. There is nothing in later chapters that assumes knowledge of this chapter, so if you want to skip on to more useful methods, head to Chapter 5.\nWe will only consider univariate methods in this chapter. There have been multivariate parametric methods for anomaly detection, but they are much less widely used and are also best avoided.\nWe will test the methods using six examples: two involving real data, and four involving simulated data.\nCode\nn01b &lt;- tibble(y = c(n01$v2[1:18], 4, 4.5))\nn01b |&gt;\n  ggplot(aes(x = y, y = 1)) +\n  geom_jitter(width = 0, alpha = 0.5) +\n  scale_y_discrete() +\n  labs(y = \"\", x = \"Synthetic data\")\n\n\n\n\n\n\n\n\nFigure 4.1: Eighteen observations from a N(0,1) distribution along with two anomalies.\nThe other three simulated examples will use 1000 observations from each of the following distributions:\nThe density functions for these three distributions are shown in Figure 4.2.\nCode\nc(t3 = dist_student_t(3), X4 = dist_chisq(4), N01 = dist_normal()) |&gt;\n  gg_density() +\n  labs(y = \"Probability density\") +\n  scale_color_discrete(\n    breaks = c(\"t3\", \"X4\", \"N01\"),\n    labels = c(\n      latex2exp::TeX(\"$t_{~3}$\"),\n      latex2exp::TeX(\"$\\\\chi^{~2}_{~~4}$\"),\n      \"N(0,1)\"\n    )\n  ) +\n  theme(legend.text = element_text(hjust = 0)) +\n  coord_cartesian(xlim = c(-5, 10))\n\n\n\n\n\n\n\n\nFigure 4.2: Three distributions we will use to test various anomaly detection methods.\nData for the last two examples are generated below.\nset.seed(1)\nt3 &lt;- tibble(y = rt(1000, df = 3))\nchisq4 &lt;- tibble(y = rchisq(1000, df = 4))\nAll of the methods considered in this chapter assume the underlying data follow a Normal distribution. In our examples, only #4 is from a Normal distribution, so the methods should work well in that case, but perhaps not in the other cases.\nGood anomaly detection methods should pick up one anomaly in the cricket batting example, at least two anomalies in the Old Faithful example, and the two anomalies in the n01b example. They should identify no anomalies in the remaining examples.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#sec-examples",
    "href": "04-tests.html#sec-examples",
    "title": "4  Statistical tests",
    "section": "",
    "text": "The cricket batting averages from the cricket_batting data set. As discussed in Section 1.5, these appear to contain one genuine anomaly, the batter Don Bradman.\nOld Faithful eruption durations since 2015, also discussed in Section 1.5. For these data, there were two extreme anomalies — one at nearly 2 hours and one at 1 second — and several “inliers” in a area of low density between 140 and 180 seconds.\nThe first 18 rows of the second variable in the n01 data, along with the values 4.0 and 4.5. The latter two are anomalies as they are unlikely to arise from the N(0,1) distribution. We save this data set as n01b. The plot below shows the data for this example.\n\n\n\n\na N(0,1) distribution (we will use the first variable in the n01 data set);\na \\text{t}_3 distribution, which has heavy tails (but finite variance);\na \\chi^2_4 distribution, which is positively skewed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#sec-zscores",
    "href": "04-tests.html#sec-zscores",
    "title": "4  Statistical tests",
    "section": "4.2 Z scores",
    "text": "4.2 Z scores\nMany parametric methods are based on z-scores (Section 3.9) and assume that the data come from a Normal distribution. Recall that if our data are given by y_1,\\dots,y_n, then their z scores are given by z_i = (y_i - \\bar{y})/s_y where \\bar{y} is the mean and s_y is the standard deviation of the observations.\nSome books recommend that observations be identified as anomalies when the absolute value of the corresponding z is above some threshold (usually 3). This is a bad idea for several reasons.\n\nIf the data really do come from a Normal distribution that has been contaminated with anomalies, then the estimated mean (\\bar{y}) and standard deviation s_y will also be affected by those anomalies. Any anomaly detection method should be relatively robust to the anomalies in the data.\nHaving a fixed threshold regardless of sample size means that the probability of a spurious anomaly increases with the sample size.\nThe assumption of normality is unlikely to be satisfied with most real data.\n\n\nExamples\nLet’s see what happens when we apply this test to the six examples.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  mutate(z = (Average - mean(Average)) / sd(Average)) |&gt;\n  filter(abs(z) &gt; 3) |&gt;\n  select(Player, Average, z)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   Player     Average     z\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 DG Bradman    99.9  5.55\n\noldfaithful |&gt;\n  mutate(z = (duration - mean(duration)) / sd(duration)) |&gt;\n  filter(abs(z) &gt; 3)\n\n#&gt; # A tibble: 1 × 4\n#&gt;   time                duration waiting     z\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2015-12-07 00:09:00     7200    3420  45.7\n\nn01b |&gt;\n  mutate(z = (y - mean(y)) / sd(y)) |&gt;\n  filter(abs(z) &gt; 3)\n\n#&gt; # A tibble: 0 × 2\n#&gt; # ℹ 2 variables: y &lt;dbl&gt;, z &lt;dbl&gt;\n\nn01 |&gt;\n  select(v1) |&gt;\n  mutate(z = (v1 - mean(v1)) / sd(v1)) |&gt;\n  filter(abs(z) &gt; 3)\n\n#&gt; # A tibble: 1 × 2\n#&gt;      v1     z\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  3.81  3.69\n\nt3 |&gt;\n  mutate(z = (y - mean(y)) / sd(y)) |&gt;\n  filter(abs(z) &gt; 3)\n\n#&gt; # A tibble: 18 × 2\n#&gt;         y     z\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  -7.40 -4.78\n#&gt;  2   5.53  3.49\n#&gt;  3  10.5   6.66\n#&gt;  4  -5.82 -3.77\n#&gt;  5  -5.23 -3.39\n#&gt;  6   6.20  3.92\n#&gt;  7   4.88  3.08\n#&gt;  8  -4.77 -3.10\n#&gt;  9   5.99  3.79\n#&gt; 10  -5.72 -3.70\n#&gt; 11  -4.70 -3.05\n#&gt; 12  -5.37 -3.48\n#&gt; 13  -6.61 -4.27\n#&gt; 14   5.02  3.17\n#&gt; 15 -11.4  -7.33\n#&gt; 16   7.65  4.85\n#&gt; 17   5.32  3.36\n#&gt; 18   6.38  4.04\n\nchisq4 |&gt;\n  mutate(z = (y - mean(y)) / sd(y)) |&gt;\n  filter(abs(z) &gt; 3)\n\n#&gt; # A tibble: 15 × 2\n#&gt;        y     z\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  14.4  3.74\n#&gt;  2  14.1  3.63\n#&gt;  3  13.0  3.23\n#&gt;  4  13.5  3.42\n#&gt;  5  13.7  3.48\n#&gt;  6  13.3  3.35\n#&gt;  7  12.5  3.04\n#&gt;  8  17.0  4.68\n#&gt;  9  13.7  3.48\n#&gt; 10  12.9  3.20\n#&gt; 11  14.0  3.60\n#&gt; 12  15.2  4.04\n#&gt; 13  14.1  3.62\n#&gt; 14  13.9  3.55\n#&gt; 15  12.7  3.12\n\n\n\nDon Bradman is correctly identified as the only outlier in the cricket batting averages.\nOnly the extreme 2-hour duration is identified. We already know there was an additional strange 1-second eruption, plus several unusual eruptions between 140 and 180 seconds in length.\nNeither of the two anomalies added to the 18 N(0,1) observations has been correctly identified.\nOne spurious anomaly has been identified in the 1000 N(0,1) observations.\nEighteen spurious anomalies have been identified in the 1000 \\text{t}_3 observations.\nFifteen spurious anomalies have been identified in the 1000 \\chi^2_4 observations.\n\n\n\nProbability of spurious anomalies\nEven if the data did come from a Normal distribution, the probability of an observation being more than 3 standard deviations from the mean is 0.0027, so we would expect to see 1 in every 1/ 0.0027 = 370 regular observations being identified as an anomaly using this approach.\nIn fact, we can be more accurate than this. If our n observations come from a Normal distribution, then the z-scores follow (approximately) a \\text{t}_{n-1} distribution. (The approximation arises because we replace the mean and standard deviations by their sample estimate.) When we identify anomalies as those points with |z|&gt;c, then the probability of finding at least one spurious anomaly is the probability of the maximum being above c or the minimum being below -c. Since the distribution is symmetric, these probabilities are the same. Let M_n be the maximum of n z-scores computed from the data; then P(M_n \\le c) = (F_{t}(c; n-1))^n where F_{t}(y; n-1) is the cumulative distribution function of a t distribution with n-1 degrees of freedom. So the probability of at least one spurious anomaly is \n  1 - (F_{t}(c; n-1))^{2n}.\n\\tag{4.1} This probability is accurate for large n, but not for small sample sizes. So, instead, we will use simulation to compute the probabilities; these are plotted in Figure 4.3 for several values of c and n.\n\n\n\n\n\n\n\n\nFigure 4.3: The probability of at least one spurious anomaly using z-scores and with a threshold c for normally distributed data.\n\n\n\n\n\nLet’s also compute the probability of spurious anomalies for different data distributions — the \\text{t}_3 and \\chi^2_4 distributions shown in Figure 4.2.\n\n\n\n\n\n\n\n\nFigure 4.4: The probability of at least one spurious anomaly using z-scores for different data distributions and with a threshold of c=3.\n\n\n\n\n\nNotice how the departures from normality, either via skewness (for the \\chi^2 distribution) or with heavier tails (for the t distribution), lead to much greater probabilities for spurious anomalies. Any tests for anomaly detection that assume an underlying data distribution will be sensitive to the shape of that distribution.\nEven if we were prepared to believe that the data come from a Normal distribution, we need to adjust the threshold to allow for the sample size. This is the idea behind the next methods we will consider.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#peirces-and-chauvenets-criteria",
    "href": "04-tests.html#peirces-and-chauvenets-criteria",
    "title": "4  Statistical tests",
    "section": "4.3 Peirce’s and Chauvenet’s criteria",
    "text": "4.3 Peirce’s and Chauvenet’s criteria\n\nPeirce’s criterion\nBenjamin Peirce was a Harvard mathematician in the mid 1800s who worked with astronomical data which were prone to anomalous observations. He proposed the first known test based on what are now called z-scores (Peirce 1852). His criterion was that any observations with |z| &gt; c should be “rejected”, where c is a complicated function depending on the sample size n and the number of suspected anomalies.\nFigure 4.5 shows the value of c as a function of sample size, when there is only one suspected anomaly.\n\n\n\n\n\n\n\n\nFigure 4.5: Criteria for anomalies based on z-scores.\n\n\n\n\n\nThe threshold increases with sample size n to allow for the increasing likelihood of observations falling in the extreme tails of the distribution.\nThe peirce_anomalies() function returns a logical vector indicating which observations are anomalous under this criterion. Let’s apply it to the six examples.\n\ncricket_batting |&gt; filter(peirce_anomalies(Average))\n\n#&gt; # A tibble: 0 × 15\n#&gt; # ℹ 15 variables: Player &lt;chr&gt;, Country &lt;chr&gt;, Start &lt;int&gt;, End &lt;int&gt;,\n#&gt; #   Matches &lt;int&gt;, Innings &lt;int&gt;, NotOuts &lt;int&gt;, Runs &lt;int&gt;, HighScore &lt;dbl&gt;,\n#&gt; #   HighScoreNotOut &lt;lgl&gt;, Average &lt;dbl&gt;, Hundreds &lt;int&gt;, Fifties &lt;int&gt;,\n#&gt; #   Ducks &lt;int&gt;, Gender &lt;chr&gt;\n\noldfaithful |&gt; filter(peirce_anomalies(duration))\n\n#&gt; # A tibble: 1 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-12-07 00:09:00     7200    3420\n\nn01 |&gt;\n  select(v1) |&gt;\n  filter(peirce_anomalies(v1))\n\n#&gt; # A tibble: 1 × 1\n#&gt;      v1\n#&gt;   &lt;dbl&gt;\n#&gt; 1  3.81\n\nn01b |&gt; filter(peirce_anomalies(y))\n\n#&gt; # A tibble: 2 × 1\n#&gt;       y\n#&gt;   &lt;dbl&gt;\n#&gt; 1   4  \n#&gt; 2   4.5\n\nt3 |&gt; filter(peirce_anomalies(y))\n\n#&gt; # A tibble: 10 × 1\n#&gt;         y\n#&gt;     &lt;dbl&gt;\n#&gt;  1  -7.40\n#&gt;  2  10.5 \n#&gt;  3  -5.82\n#&gt;  4   6.20\n#&gt;  5   5.99\n#&gt;  6  -5.72\n#&gt;  7  -6.61\n#&gt;  8 -11.4 \n#&gt;  9   7.65\n#&gt; 10   6.38\n\nchisq4 |&gt; filter(peirce_anomalies(y))\n\n#&gt; # A tibble: 6 × 1\n#&gt;       y\n#&gt;   &lt;dbl&gt;\n#&gt; 1  14.4\n#&gt; 2  14.1\n#&gt; 3  17.0\n#&gt; 4  14.0\n#&gt; 5  15.2\n#&gt; 6  14.1\n\n\n\nWhen applied to the test cricket batting averages, it doesn’t even find the obvious anomaly of Don Bradman.\nWhen applied to the Old Faithful eruption durations, it finds only the most extreme duration.\nBoth anomalies have been correctly spotted amongst the 18 N(0,1) observations.\nOne spurious anomaly is identified in the 1000 N(0,1) observations.\nTen spurious anomalies have been identified in the 1000 \\text{t}_3 observations.\nSix spurious anomalies have been identified in the 1000 \\chi^2_4 observations.\n\n\n\nChauvenet’s criterion\nPeirce’s proposal was largely superseded by an alternative proposed by the astrophysicist William Chauvenet, which was much simpler to describe. He suggested (Chauvenet 1863) replacing the threshold c by the 1-0.25/n quantile from the standard Normal distribution. This threshold is also shown in Figure 4.5. A consequence of this choice is that Chauvenet’s criterion will reject, on average, half an observation of genuine data from a Normal distribution regardless of the value of n. However, for non-normal data, there is no such guarantee that genuine observations will not be detected as anomalies. Despite its flaws, the method is still widely used in some disciplines, especially engineering.\nThe chauvenet_anomalies() function can be used to implement this test. For our six examples, it gives similar results to those above for peirce_anomalies() (with two additional spurious anomalies for the t_2 example, and one additional spurious anomaly for the \\chi^2_4 example.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#grubbs-test",
    "href": "04-tests.html#grubbs-test",
    "title": "4  Statistical tests",
    "section": "4.4 Grubbs’ test",
    "text": "4.4 Grubbs’ test\nBy the 20th century, the concept of hypothesis testing had been developed, and the t-distribution had been discovered, and both were applied to the identification of anomalies using z-scores. Many tests were developed under different assumptions about the underlying distribution and what was assumed to be known (Hawkins 1980). We will mention just two of them here, as they are the most widely used.\nEgon Pearson and Chandra Sekar proposed (Pearson and Sekar 1936) that an observation be considered an anomaly if |z_i|&gt; c_\\alpha, where the critical value is given by \nc_\\alpha = t_{\\alpha/n, n-2} \\sqrt{\\frac{n-1}{n-2+t^2_{\\alpha/n, n-2}}}\n and t_{p, k} is the 1-p quantile of the t distribution with k degrees of freedom. Later, this was extended by Frank Grubbs (Grubbs 1950) who proposed using \nc_\\alpha = \\frac{(n-1)t_{\\alpha/2n, n-2}}{\\sqrt{n(n-2 + t^2_{\\alpha/2n, n-2})}}.\n\nFigure 4.6 shows the critical values at \\alpha=0.05 for these tests, along with the corresponding value from Chauvenet (1863) for comparison.\n\n\n\n\n\n\n\n\nFigure 4.6: Critical values for maximum z-score tests with \\alpha=0.05.\n\n\n\n\n\nFrom these, we can compute the probability of a spurious anomaly in a Normal distribution, using Equation 4.1. This probability should be equal to \\alpha=0.05.\n\n\n\n\n\n\n\n\nFigure 4.7: True size of the tests (i.e., the probability of a spurious anomaly) for normally distributed data with \\alpha=0.05.\n\n\n\n\n\nFigure 4.7 shows that only Grubbs’ test gives reasonable results, with the others finding too many anomalies except in small samples.\nHowever, when we allow for different data distributions, Grubbs’ test also gives poor results, showing it is sensitive to the assumed data distribution.\n\n\n\n\n\n\n\n\nFigure 4.8: True size of Grubbs’ tests (i.e., the probability of a false positive) for different data distributions with \\alpha=0.05.\n\n\n\n\n\nWe can apply the test using the grubbs_anomalies() function to our six examples.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  filter(grubbs_anomalies(Average)) |&gt;\n  select(Player, Country, Average)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   Player     Country   Average\n#&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 DG Bradman Australia    99.9\n\noldfaithful |&gt;\n  filter(grubbs_anomalies(duration))\n\n#&gt; # A tibble: 1 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-12-07 00:09:00     7200    3420\n\nn01b |&gt;\n  filter(grubbs_anomalies(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\nn01 |&gt;\n  filter(grubbs_anomalies(v1))\n\n#&gt; # A tibble: 0 × 10\n#&gt; # ℹ 10 variables: v1 &lt;dbl&gt;, v2 &lt;dbl&gt;, v3 &lt;dbl&gt;, v4 &lt;dbl&gt;, v5 &lt;dbl&gt;, v6 &lt;dbl&gt;,\n#&gt; #   v7 &lt;dbl&gt;, v8 &lt;dbl&gt;, v9 &lt;dbl&gt;, v10 &lt;dbl&gt;\n\nt3 |&gt;\n  filter(grubbs_anomalies(y))\n\n#&gt; # A tibble: 6 × 1\n#&gt;        y\n#&gt;    &lt;dbl&gt;\n#&gt; 1  -7.40\n#&gt; 2  10.5 \n#&gt; 3  -6.61\n#&gt; 4 -11.4 \n#&gt; 5   7.65\n#&gt; 6   6.38\n\nchisq4 |&gt;\n  filter(grubbs_anomalies(y))\n\n#&gt; # A tibble: 1 × 1\n#&gt;       y\n#&gt;   &lt;dbl&gt;\n#&gt; 1  17.0\n\n\nThe clear failure is with the \\text{t}_3 distribution which has no real anomalies. A spurious anomaly is also detected in the \\chi^2_4 example, and real anomalies are missed in the Old Faithful data and in n01b.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#dixons-q-test",
    "href": "04-tests.html#dixons-q-test",
    "title": "4  Statistical tests",
    "section": "4.5 Dixon’s Q test",
    "text": "4.5 Dixon’s Q test\nIf y_{(1)},\\dots,y_{(n)} denote the ordered values of our sample, then Dixon’s Q statistic (Dixon 1950) is given by \n  Q = \\frac{y_{(n)} - y_{(n-1)}}{y_{(n)} - y_{(1)}},\n the ratio of the difference between the two largest values to the range of the data. If the largest value is the only anomaly, then Q will take a larger value than expected.\nThe corresponding test for the minimum to be an anomaly uses y_{(2)}-y_{(1)} in the numerator instead, the difference between the second smallest and minimum observations. Both minimum and maximum values can be tested simultaneously using the two-sided test, where the numerator is the maximum of y_{(n)} - y_{(n-1)} and y_{(2)}-y_{(1)}.\nThe test is clearly flawed for several reasons. First, if the two largest values are both anomalies of similar size, then Q will be small and these anomalies will be missed. Also, if both maximum and minimum values are anomalies, the denominator will be larger than expected, thereby reducing the size of Q.\nAs with the other tests considered here, the test assumes that the underlying data distribution is normal, and anomalies are identified which appear inconsistent with that assumption.\nSimulation can be used to compute the critical values for this test, assuming that the data come from a Normal distribution. We can also use simulation to compute the probability of false positives, giving the results shown in Figure 4.9. As with the other tests we have considered, Dixon’s test is sensitive to the assumed data distribution making it largely useless for real data analysis.\n\n\n\n\n\n\n\n\nFigure 4.9: Probability of a spurious anomaly using Dixon’s test.\n\n\n\n\n\nWe can apply the test using the dixon_anomalies() function, which does a two-sided test of both minimum and maximum observations.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  filter(dixon_anomalies(Average)) |&gt;\n  select(Player, Country, Average)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   Player     Country   Average\n#&gt;   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 DG Bradman Australia    99.9\n\noldfaithful |&gt;\n  filter(dixon_anomalies(duration))\n\n#&gt; # A tibble: 1 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-12-07 00:09:00     7200    3420\n\nn01b |&gt;\n  filter(dixon_anomalies(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\nn01 |&gt;\n  filter(dixon_anomalies(v1))\n\n#&gt; # A tibble: 0 × 10\n#&gt; # ℹ 10 variables: v1 &lt;dbl&gt;, v2 &lt;dbl&gt;, v3 &lt;dbl&gt;, v4 &lt;dbl&gt;, v5 &lt;dbl&gt;, v6 &lt;dbl&gt;,\n#&gt; #   v7 &lt;dbl&gt;, v8 &lt;dbl&gt;, v9 &lt;dbl&gt;, v10 &lt;dbl&gt;\n\nt3 |&gt;\n  filter(dixon_anomalies(y))\n\n#&gt; # A tibble: 1 × 1\n#&gt;       y\n#&gt;   &lt;dbl&gt;\n#&gt; 1 -11.4\n\nchisq4 |&gt;\n  filter(dixon_anomalies(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\n\nFor these examples, the test has worked relatively well apart from n01b, where it is failed to identify either anomaly, and t3 where it has identified a spurious anomaly.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "04-tests.html#summary",
    "href": "04-tests.html#summary",
    "title": "4  Statistical tests",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nWe can summarise the results of the various tests used here in the following table.\n\n\n\n\nTable 4.1: Number of anomalies detected by each test. Cells in red indicate that the test has failed to identify all known anomalies (i.e., it has some false negatives), or it has identified some spurious anomalies (i.e., it has some false positives).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly detection method\n\n\n\nExample\nN observations\nExpected anomalies\nZscore\nPeirce\nChauvenet\nGrubbs\nDixon\n\n\n\n\n1. Cricket batting\n3754\n1\n1\n0\n0\n0\n1\n\n\n2. Old Faithful duration\n2261\n2\n1\n1\n1\n1\n1\n\n\n3. N(0,1) + 2 outliers\n20\n2\n0\n2\n2\n0\n0\n\n\n4. N(0,1)\n1000\n0\n1\n1\n1\n0\n0\n\n\n5. \\text{t}_3\n1000\n0\n18\n10\n12\n6\n1\n\n\n6. \\chi^2_4\n1000\n0\n15\n6\n7\n1\n0\n\n\n\n\n\n\n\n\n\n\nNone of the tests found the two clear anomalies in the Old Faithful data, and most failed to find either of the artificial anomalies in n01b. All tests found at least one spurious anomaly in the large simulated data sets with no real anomalies.\nAs noted at the start of this chapter, anomaly detection methods based on assumed data distributions are unreliable. Until about 1975, they were the only viable methods given the lack of computing facilities available, because they could be implemented using tables and hand calculations. However, there is really no justifiable reason for continuing to use such methods.\nThey are particularly sensitive to the assumed data distribution, and the probability of detecting spurious anomalies is usually much higher in reality than under the ideal conditions in which the tests were conceived.\nThese tests are sometimes applied iteratively, where observations are removed from the data if determined to be anomalies, and the test re-applied to the remaining data. This process continues until no more anomalies are found. However, this procedure will clearly change the size of the test due to the problem of multiple comparisons.\nNext, we will turn out attention to methods that arose in the latter part of the 20th century which were based on quantiles and data depth rather than on any underlying parametric data distribution.\n\n\n\n\nChauvenet, W. 1863. “Appendix: Method of Least Squares.” In A Manual of Spherical and Practical Astronomy, II Theory and use of astronomical instruments:469–566. Philadelphia: Lippincott.\n\n\nDixon, W J. 1950. “Analysis of Extreme Values.” Annals of Mathematical Statistics 21 (4): 488–506. https://projecteuclid.org/euclid.aoms/1177729747.\n\n\nGrubbs, F E. 1950. “Sample Criteria for Testing Outlying Observations.” Annals of Mathematical Statistics 21 (1): 27–58. https://projecteuclid.org/euclid.aoms/1177729885.\n\n\nHawkins, D M. 1980. Identification of Outliers. Springer.\n\n\nPearson, E S, and C C Sekar. 1936. “The Efficiency of Statistical Tools and a Criterion for the Rejection of Outlying Observations.” Biometrika 28 (3/4): 308–20. http://www.jstor.org/stable/2333954.\n\n\nPeirce, B. 1852. “Criterion for the Rejection of Doubtful Observations.” The Astronomical Journal 2 (21): 161–63. http://adsabs.harvard.edu/pdf/1852AJ......2..161P.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical tests</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html",
    "href": "05-boxplots.html",
    "title": "5  Boxplots",
    "section": "",
    "text": "5.1 Univariate data depth\nMost readers will have first come across anomaly detection using boxplots. In this chapter, we will describe the original boxplot method, along with some variations that have been developed to address some of the limitations of the original approach.\nFor univariate data, the depth of an observation is a measure of how deeply buried it is when all observations are ordered. That is, how far would you need to count from either the smallest or largest observation until you encountered the observation of interest. So the minimum and maximum both have depth 1, while the sample median has the largest depth of (n+1)/2.\nThe other sample quantiles as defined in Section 2.4 do not correspond exactly to specific depths, but Tukey (1977) introduced variations for some quantiles that are based on depths. He called these “letter values”.\nLetter values (Tukey 1977; Hoaglin 1983) are order statistics with specific depths, defined recursively starting with the median. The depth of the median is d_1 = (1+n)/2. The depths of successive letter values are defined recursively as d_i = (1+\\lfloor d_{i-1}\\rfloor)/2, i=2,3,\\dots. The corresponding letter values are defined as \n  L_i = y_{(\\lfloor d_i\\rfloor)}\n  \\qquad\\text{and}\\qquad\n  U_i = y_{(\\lfloor n-d_i+1\\rfloor)}\n when the depth is an integer. Otherwise the depth is an integer plus 1/2, and the letter values are given by \n  L_i = (y_{(\\lfloor d_i\\rfloor)} + y_{(\\lfloor d_i\\rfloor+1)})/2\n  \\qquad\\text{and}\\qquad\n  U_i = (y_{(\\lfloor n-d_i+1\\rfloor)} + y_{(\\lfloor n-d_i+1\\rfloor+1)})/2 .\n Rather than label these using integers (L_2,L_3,\\dots), Tukey proposed using letters (L_F,L_E,L_D,\\dots) where F= fourths, E= eighths, D= sixteenths, and so on.\nBecause each depth is roughly half the previous depth, the lower letter values provide estimates of the quantiles with probabilities p=\\frac{1}{2},\\frac14,\\frac18,\\dots, while the upper letter values provide estimates of the quantiles with probabilities p=\\frac{1}{2},\\frac34,\\frac78,\\dots. Hoaglin (1983, p44), showed that \\hat{Q}(p) with type = 8 gives approximately the same result as the corresponding letter value.\nConsider the batting averages from Section 2.4. In this example, n = 1138, so the depths of the first four letter values are given by \n  d_1 = 569.5,\\quad\n  d_2 = 285,\\quad\n  d_3 = 143,\\quad\\text{and}\\quad\n  d_4 = 72,\n and the corresponding letter values are given by \\begin{align*}\nL_1 &= U_1 = (y_{(569)}+ y_{(570)})/2 = 26.63 &\\\\\nL_F &= y_{(285)}  = 16.59 \\qquad\n&U_F &= y_{(854)} = 36.71 \\\\\nL_E &= y_{(143)} = 11.57 \\qquad\n&U_E &= y_{(996)} = 43.28 \\\\\nL_D &= y_{(72)}  = 8.29 \\qquad\n&U_D &= y_{(1067)}  = 47.29\n\\end{align*} We can compute the letter values using the lvtable() function from the lvplot package.\nbatave &lt;- cricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  pull(Average)\nbatave |&gt; lvplot::lvtable(k = 4)\n\n#&gt;    depth     LV   2.5%  97.5%\n#&gt; Dl  72.0  8.286  7.524  8.727\n#&gt; El 143.0 11.568 10.698 11.948\n#&gt; Fl 285.0 16.591 15.583 17.697\n#&gt; M  569.5 26.631 25.732 27.921\n#&gt; Fu 285.0 36.714 35.662 37.649\n#&gt; Eu 143.0 43.278 42.421 44.214\n#&gt; Du  72.0 47.289 46.561 48.227\nThe output also provides 95% confidence intervals based on Equation 2.2. The estimates are similar, but not identical, to the quantiles calculated using the quantile() function.\nbatave |&gt; quantile(prob = c(0.5^(4:1), 1 - 0.5^(2:4)), type = 8)\n\n#&gt;  6.25%  12.5%    25%    50%    75%  87.5% 93.75% \n#&gt;  8.243 11.565 16.589 26.631 36.718 43.346 47.417",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#tukeys-boxplots",
    "href": "05-boxplots.html#tukeys-boxplots",
    "title": "5  Boxplots",
    "section": "5.2 Tukey’s boxplots",
    "text": "5.2 Tukey’s boxplots\nBoxplots were invented by John Tukey as a quick summary of medium sized data sets (Tukey 1975, 1977; Wickham and Stryjewski 2011). They are widely used to identify anomalies, which are shown as separate points in the plot.\nFigure 5.1 shows a boxplot of the cricket batting average data.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  ggplot(aes(x = Average)) +\n  geom_boxplot() +\n  scale_y_discrete() +\n  labs(y = \"\", x = \"Career batting average\")\n\n\n\n\n\n\n\nFigure 5.1: Career batting averages for all men and women to have played test cricket and batted more than 20 times. The anomaly is Don Bradman, who averaged 99.94 over his career.\n\n\n\n\n\nIn the ggplot2 version of the boxplot shown here, the middle line in the box shows the median, and the ends of the box are the quartiles computed using type = 7. The base R version is computed using the boxplot() function, which uses the fourths to delineate the box.\nWhichever variation is used, roughly half of all observations lie within the box. The width of the box is an estimate of the “interquartile range” (IQR). Any points more than 1.5 IQR outside the box are shown as anomalies and appear as separate points in the plot. The “whiskers” that extend out each side of the box show the range of the remaining points.\nOriginally, Tukey proposed two levels of outliers — those more than 1.5 IQR beyond the box were labelled “outside” values, while those more than 3 IQR beyond the box were labelled “far out” values. Most software implementations of boxplots do not distinguish between these groups.\nIn this cricket batting example, the boxplot works well because the data set is not too large or small, and the distribution of points other than the anomaly is unimodal.\nHowever, boxplots can be misleading, and are they are limited in at least two respects.\n\nFor large data sets, boxplots show too many points as anomalies, and it is hard to distinguish them.\nBoxplots assume that the distribution of the data is unimodal.\n\nTo better understand the first problem, imagine if the data comprised n observations from a standard Normal distribution N(0,1). Figure 5.2 shows an example with 10000 points.\n\ntibble(x = rnorm(10000)) |&gt;\n  ggplot(aes(x = x)) +\n  geom_boxplot() +\n  scale_y_discrete() +\n  labs(y = \"\")\n\n\n\n\n\n\n\nFigure 5.2: Boxplot of 10000 draws from a standard Normal distribution.\n\n\n\n\n\nMany anomalies are shown, but since all observations come from a simple distribution, none of them are actually anomalies. For this distribution, Q(0.25) = -0.674, Q(0.75) = 0.674, \\text{IQR} = 1.349, and so for a large sample size, the boxplot whiskers would be approximately -0.674 - 1.5\\times 1.349 = -2.698 and 0.674 + 1.5\\times 1.349 = 2.698. Any points outside the whiskers would be identified as “anomalous” by a boxplot and plotted as separate points. The probability of a standard normal observation being at least 2.698 in absolute value is 0.00349. So with 10000 observations, we would have about 35 anomalies identified, none of which would be a genuine anomaly.\nThe second problem is demonstrated using the Old Faithful eruption duration data, shown in Figure 1.3. As before, we will omit the largest value so we can see the details in the remaining data.\n\noldfaithful |&gt;\n  filter(duration &lt; 6000) |&gt;\n  ggplot(aes(x = duration)) +\n  geom_boxplot() +\n  scale_y_discrete() +\n  labs(y = \"\", x = \"Duration (seconds)\")\n\n\n\n\n\n\n\nFigure 5.3: Boxplot of Old Faithful eruption durations since 2015, omitting the one eruption that lasted nearly two hours.\n\n\n\n\n\nQuite a few anomalies are shown, including the one-second eruption we identified earlier. But the remaining “anomalies” are not particularly unusual observations. All the points below 180 seconds are identified as anomalies, even though we know that observations in the region between 100 and 140 are not unusual for this geyser. Because the boxplot does not allow for more than one mode, all the points in the second smaller cluster are identified as anomalies. The points around 300 seconds are also not really anomalies — these are just values in the upper tail of the distribution for eruptions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#modified-iqr-boxplots",
    "href": "05-boxplots.html#modified-iqr-boxplots",
    "title": "5  Boxplots",
    "section": "5.3 Modified IQR boxplots",
    "text": "5.3 Modified IQR boxplots\nUnder Tukey’s boxplot approach to identifying outliers, a regular outlier is more than 1.5 IQR beyond the quartiles, while an extreme outlier is more than 3 IQR beyond the quartiles. For a Normal distribution, the probability of genuine observations lying beyond these thresholds is 0.0070 and 0.0000023 respectively, so for large sample sizes, many spurious anomalies will be identified. Even with 1000 observations, Tukey’s approach will find at least one spurious anomaly in a Normal distribution with probability 0.9991.\nBarbato et al. (2011) proposed a modification to the boxplot approach to identifying outliers, where the IQR in these thresholds is replaced with IQR[1+0.1\\log(n/10)]. This allows the limits to increase with the sample size, in a way that controls the probability of spurious anomalies. Figure 5.4 shows the probability of identifying at least one spurious anomaly in a Normal distribution, using Tukey’s boxplot approach compared to those obtained using the modified IQR approach of Barbato et al. (2011).\n\n\nCode\ntibble(n = exp(seq(log(3), log(1e6), l = 100))) |&gt;\n  mutate(\n    Tukey1 = 2 * (1 - pnorm(q3 + 1.5 * iqr)),\n    Tukey2 = 2 * (1 - pnorm(q3 + 3 * iqr)),\n    Barbato1 = 2 * (1 - pnorm(q3 + 1.5 * iqr * (1 + 0.1 * log(n / 10)))),\n    Barbato2 = 2 * (1 - pnorm(q3 + 3 * iqr * (1 + 0.1 * log(n / 10))))\n  ) |&gt;\n  tidyr::pivot_longer(Tukey1:Barbato2, names_to = \"method\", values_to = \"probability\") |&gt;\n  mutate(\n    level = stringr::str_extract(method, \"\\\\d\"),\n    level = if_else(level == \"1\", \"Regular outlier\", \"Extreme outlier\"),\n    method = stringr::str_extract(method, \"[A-Za-z]*\"),\n    level = factor(level, levels = c(\"Regular outlier\", \"Extreme outlier\")),\n    method = factor(method, levels = c(\"Tukey\", \"Barbato\")),\n    probability = 1 - (1 - probability)^n\n  ) |&gt;\n  ggplot(aes(x = n, y = probability)) +\n  geom_line() +\n  facet_grid(level ~ method) +\n  labs(y = \"Probability of at least one spurious anomaly\", x = \"Sample size\") +\n  scale_x_log10(\n    limits = c(3, 2e6),\n    breaks = 10^(1:6),\n    minor_breaks = NULL,\n    labels = format(10^(1:6), scientific = FALSE, trim = TRUE)\n  )\n\n\n\n\n\n\n\n\nFigure 5.4: Probability of at least one spurious anomaly identified in a Normal distribution, based on the boxplot approach of Tukey, and the modified IQR approach of Barbato et al. (2011).\n\n\n\n\n\nEven with a huge sample size, the probability of identifying a spurious regular anomaly using this modified approach is less than 1/2, and it is almost impossible to identify a spurious extreme anomaly.\nLet’s apply this approach to the six examples we introduced in Section 4.1. First we will write a short function to implement the idea.\n\nbarbato_anomaly &lt;- function(y, extreme = FALSE) {\n  n &lt;- length(y)\n  q1 &lt;- quantile(y, 0.25, na.rm = TRUE)\n  q3 &lt;- quantile(y, 0.75, na.rm = TRUE)\n  threshold &lt;- (1.5 + 1.5 * extreme) * (q3 - q1) * (1 + log(n / 10))\n  return(y &gt; q3 + threshold | y &lt; q1 - threshold)\n}\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  filter(barbato_anomaly(Average))\n\n#&gt; # A tibble: 0 × 15\n#&gt; # ℹ 15 variables: Player &lt;chr&gt;, Country &lt;chr&gt;, Start &lt;int&gt;, End &lt;int&gt;,\n#&gt; #   Matches &lt;int&gt;, Innings &lt;int&gt;, NotOuts &lt;int&gt;, Runs &lt;int&gt;, HighScore &lt;dbl&gt;,\n#&gt; #   HighScoreNotOut &lt;lgl&gt;, Average &lt;dbl&gt;, Hundreds &lt;int&gt;, Fifties &lt;int&gt;,\n#&gt; #   Ducks &lt;int&gt;, Gender &lt;chr&gt;\n\noldfaithful |&gt; filter(barbato_anomaly(duration))\n\n#&gt; # A tibble: 1 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-12-07 00:09:00     7200    3420\n\nn01b &lt;- tibble(y = c(n01$v2[1:18], 4, 4.5))\nn01b |&gt; filter(barbato_anomaly(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\nn01 |&gt; filter(barbato_anomaly(v1))\n\n#&gt; # A tibble: 0 × 10\n#&gt; # ℹ 10 variables: v1 &lt;dbl&gt;, v2 &lt;dbl&gt;, v3 &lt;dbl&gt;, v4 &lt;dbl&gt;, v5 &lt;dbl&gt;, v6 &lt;dbl&gt;,\n#&gt; #   v7 &lt;dbl&gt;, v8 &lt;dbl&gt;, v9 &lt;dbl&gt;, v10 &lt;dbl&gt;\n\nset.seed(1)\nt3 &lt;- tibble(y = rt(1000, df = 3))\nt3 |&gt; filter(barbato_anomaly(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\nchisq4 &lt;- tibble(y = rchisq(1000, df = 4))\nchisq4 |&gt; filter(barbato_anomaly(y))\n\n#&gt; # A tibble: 0 × 1\n#&gt; # ℹ 1 variable: y &lt;dbl&gt;\n\n\n\nIt misses the anomalies in the cricket batting data, and in the n01b data set.\nOnly the extreme outlier in the duration data is identified as an anomaly.\n\nIn summary, while the modified IQR approach is an improvement on the original boxplot approach, it is not particularly good at finding genuine anomalies in data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#letter-value-plots",
    "href": "05-boxplots.html#letter-value-plots",
    "title": "5  Boxplots",
    "section": "5.4 Letter value plots",
    "text": "5.4 Letter value plots\nThe problem that boxplots have with large data sets was also addressed by Hofmann, Wickham, and Kafadar (2017) who introduced “letter-value” plots, a variation of boxplots that replace the whiskers with a variable number of letter values. In these plots, each pair of letter values marks the boundaries of a box. The box bounded by the fourths is the same as the box of a boxplot; the additional boxes extend to successive letter values until the quantiles corresponding to the letter values can no longer be estimated sufficiently accurately from the available data.\nThese can be produced using the lvplot package.\n\nlibrary(lvplot)\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  ggplot(aes(x = 1, y = Average)) +\n  geom_lv(aes(fill = after_stat(LV))) +\n  scale_x_discrete() +\n  coord_flip() +\n  labs(x = \"\", y = \"Career batting average\") +\n  theme(legend.key.height = unit(.2, \"cm\"))\n\n\n\n\n\n\n\nFigure 5.5: Letter value plot of career batting averages for all men and women who played test cricket and batter more than 20 times.\n\n\n\n\n\nHere the median is given by M, the fourths by F, and so on. The middle box (F) is bounded by the fourths and contains all but 2/4 of the data; the next box (E) is bounded by the eighths and contains all but 2/8 of the data; then D is bounded by the sixteenths and contains all but 2/16 of the data; and so on.\nIn this example, the most extreme box (labelled Z) is bounded by the points that fall within the 1/256 letter values. So it contains all but 2/256 of the data, and shows 2 / 256 \\times 1138 = 9 points as anomalies.\nThe stopping rule used in the letter value plot is to show the boxes up to letter value k, where \n  0.5\\sqrt{2d_k} z_{1-\\alpha/2} &gt; d_{k+1}\n and z_{1-\\alpha/2} is the 1-\\alpha/2 quantile of a standard Normal distribution. This choice is based on the idea that the edges of the boxes are quantile estimates, and the confidence interval for each quantile estimate that is displayed should not overlap the subsequent quantile estimate. The Normal distribution arises because the quantile estimate has an approximate Normal distribution (Equation 2.2). This stopping rule means that, on average, there should be fewer than 2z^2_{1-\\alpha/2} legitimate observations in the tails. By default, \\alpha=0.05, so that, on average, there should be fewer than 2 \\times (1.96)^2 = 7.7 legitimate observations in the tails, regardless of the size of the data set.\nLetter value plots were not designed to detect anomalies, but to be a useful data visualization tool for univariate distributions with large numbers of observations. So the display of legitimate observations in the tails of the distribution is by design, not a flaw.\nIn this cricketing example, it looks like there is one true anomaly (Don Bradman) and the remaining 8 observations displayed directly are simply in the tails of the distribution of the remaining data.\nWhen applied to the remaining examples, we see approximately 10–20 observations shown as individual points in each case.\n\n\nCode\noldfaithful |&gt;\n  filter(duration &lt; 7000) |&gt;\n  ggplot(aes(x = 1, y = duration)) +\n  geom_lv(aes(fill = after_stat(LV))) +\n  scale_x_discrete() +\n  coord_flip() +\n  labs(x = \"\", y = \"Eruption durations (seconds)\") +\n  theme(legend.key.height = unit(.2, \"cm\"))\n\n\n\n\n\n\n\n\nFigure 5.6: Letter value plot of Old Faithful eruption durations, omitting the long 2 hour duration.\n\n\n\n\n\n\n\nCode\nn01 |&gt;\n  ggplot(aes(x = 1, y = v1)) +\n  geom_lv(aes(fill = after_stat(LV))) +\n  scale_x_discrete() +\n  coord_flip() +\n  labs(x = \"\") +\n  theme(legend.key.height = unit(.2, \"cm\"))\n\n\n\n\n\n\n\n\nFigure 5.7: Letter value plot of 1000 N(0,1) observations.\n\n\n\n\n\n\n\nCode\nn01b |&gt;\n  ggplot(aes(x = 1, y = y)) +\n  geom_lv(aes(fill = after_stat(LV))) +\n  scale_x_discrete() +\n  coord_flip() +\n  labs(x = \"\")\n\n\n\n\n\n\n\n\nFigure 5.8: Letter value plot of 19 N(0,1) observations with an anomaly at 4.\n\n\n\n\n\nIn this last example, because there are only 20 observations, there is not enough data to estimate the quantiles beyond the fourths. So only the middle box is shown.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#sec-depth",
    "href": "05-boxplots.html#sec-depth",
    "title": "5  Boxplots",
    "section": "5.5 Multivariate data depth",
    "text": "5.5 Multivariate data depth\nFor multivariate data, there is no unique natural ordering of observations by size, so the concept of depth has to be thought about differently. It is still a measure of how centrally a point is located in a data set, but we need to think about what “central” means when there are multiple variables.\nThere are numerous ways to define depth for multivariate data, nicely summarised in Liu, Parelius, and Singh (1999). Here, we will consider the two simplest approaches.\n\nTukey depth\nThe first approach to this problem was (again) due to John Tukey (Tukey 1975) who defined the depth of a point \\bm{z} in a data set \\{\\bm{y}_1,\\dots,\\bm{y}_n\\} as the smallest number of \\bm{y}_i contained in any halfspace that contains \\bm{z}. This is often called the “Tukey depth” of the point. (The point \\bm{z} does not have to be one of the observations.)\nFor bivariate data, a half space is either of the two parts formed by splitting the plane with a straight line. A diagram will help illustrate the idea. Suppose we have the observations shown in Figure 5.9. These are the first 10 observations from the first two variables of n01. The red line divides the plane into two sections which are called “half spaces”. We are interested in the depth of the orange point (which is not one of the observations).\n\n\nCode\nn01 |&gt;\n  head(10) |&gt;\n  ggplot(aes(x = v1, y = v2)) +\n  geom_point() +\n  geom_abline(aes(intercept = -0.8, slope = -1.8), col = \"red\") +\n  geom_point(data = data.frame(v1 = 0, v2 = -1), col = discrete_colors[1]) +\n  coord_fixed(xlim = c(-0.85, 1.6), ylim = c(-1.85, 1.1))\n\n\n\n\n\n\n\n\nFigure 5.9: A data set of 10 bivariate observations (shown in black). The dividing red line splits the plane into two halfspaces. We are interested in the depth of the orange point.\n\n\n\n\n\nThere are an infinite number of ways of dividing the plane into half spaces, and the number of points in each half plane will vary depending on where the dividing line falls.\nTo find the depth of the orange point, we need to find the line which divides the plane into two sections where the half containing the orange point includes as few observations as possible. In fact, the red line is one such solution which has the orange point in a halfspace containing only 2 observations There are no dividing lines that would put the orange point in a halfspace on its own. So it has a Tukey depth of 2.\nThe depth region D_k is the set of all points \\bm{z} with Tukey depth at least k. These form a series of nested convex hulls where D_{k+1} \\subseteq D_k. The depth regions for our example are shown in Figure 5.10. The blue region is depth 1, the orange region is depth 2, the pink region is depth 3, and the green region is depth 4.\n\n\nCode\nmedian &lt;- aplpack::compute.bagplot(n01[1:10, 1:2])\nz &lt;- tidyr::expand_grid(v1 = seq(-0.9, 1.6, l = 200), v2 = seq(-1.95, 1.15, l = 200)) |&gt;\n  bind_rows(n01[1:10, 1:2])\nz$depth &lt;- round(as.numeric(DepthProc::depthTukey(as.matrix(z), as.matrix(n01[1:10, 1:2]))) * 10)\nregions &lt;- list()\ndepths &lt;- sort(unique(z$depth))\ndepths &lt;- depths[depths &gt; 0]\nfor (i in depths) {\n  tmp &lt;- z |&gt; filter(depth == depths[i])\n  hull &lt;- chull(tmp)\n  regions[[i]] &lt;- tmp[c(hull, hull[1]), ]\n}\ncols &lt;- discrete_colors[c(2, 1, 4, 3)]\np &lt;- n01 |&gt;\n  head(10) |&gt;\n  ggplot(aes(x = v1, y = v2))\nfor (i in depths) {\n  p &lt;- p + geom_polygon(aes(x = v1, y = v2), data = regions[[i]], fill = cols[i], alpha = 0.8)\n}\np + geom_point() +\n  coord_fixed(xlim = c(-0.85, 1.6), ylim = c(-1.85, 1.1)) +\n  geom_point(\n    data = data.frame(v1 = median$center[1], v2 = median$center[2]),\n    col = \"yellow\", size = 2\n  )\n\n\n\n\n\n\n\n\nFigure 5.10: The depth regions for the 10 bivariate observations from Figure 5.9. The depth median is shown as the yellow point in the centre. The 10 observations are shown in black. These lie at the outer edges of the depth regions.\n\n\n\n\n\n\n\nDepth median\nA simple way to define a multivariate median is the “centre of gravity” of all points of maximum depth (Rousseeuw and Ruts 1998). The centre of gravity (or “centroid”) of a shape is the average of all points within the shape. If the shape is convex, it is the point where the shape could be perfectly balanced on the tip of a pin if it were made of a uniform material. For example, the centre of gravity of a rectangle is the point in the middle of the rectangle, and the centre of gravity of a circle is the centre of the circle.\nIn Figure 5.10, the points of maximum depth are shown in the central green region. The centre of gravity of this region is the yellow point in the middle.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#bagplots",
    "href": "05-boxplots.html#bagplots",
    "title": "5  Boxplots",
    "section": "5.6 Bagplots",
    "text": "5.6 Bagplots\nThe bagplot was proposed by Rousseeuw, Ruts, and Tukey (1999) as a bivariate version of a boxplot, constructed using similar principles. Like a univariate boxplot, the bivariate bagplot has a central point (the depth median), an inner region (the “bag”), and an outer region (the “loop”), beyond which outliers are shown as individual points.\nTo define the bag, we first find the smallest depth region D_k containing at least \\lfloor n/2 \\rfloor of the observations. Then the bag is linearly interpolated between D_k and D_{k-1}, with the linear interpolation depending on the number of observations in each depth region. Because the bag is interpolated between depth regions, it is also a convex polygon. The procedure is slightly more complicated for small data sets where only D_1 might contain more than half of the observations.\nTo find the loop, we inflate the bag relative to the median by a factor of 3. This forms the “fence”. Then the loop is the convex hull of the points contained within the fence.\nFigure 5.11 shows the bagplot from the same 10 observations as were used in the illustration of depth in the previous section, along with the observations themselves shown in black.\n\n\nCode\nn01 |&gt;\n  head(10) |&gt;\n  gg_bagplot(v1, v2) +\n  geom_point(aes(x = v1, y = v2), data = n01[1:10, ])\n\n\n\n\n\n\n\n\nFigure 5.11: A bagplot of the 10 bivariate observations from Figure 5.9. The depth median is shown as the blue point in the centre. The darker shaded region is the bag, while the lighter shaded region shows the loop. There are no outliers outside the loop for this data set.\n\n\n\n\n\n\nOld faithful bagplot\nA more interesting example is obtained in Figure 5.12, showing a bagplot of the durations and waiting times of Old Faithful eruptions.\n\n\nCode\noldfaithful |&gt;\n  filter(duration &lt; 7200, waiting &lt; 7200) |&gt;\n  gg_bagplot(duration, waiting) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 5.12: Bagplot of the durations and waiting times of Old Faithful eruptions since 2015.\n\n\n\n\n\nThis demonstrates the drawback of using a bagplot to identify anomalies. The plot has identified all the shorter duration eruptions as anomalous, along with a small number of other eruptions.\nA useful variation of the bagplot, especially for larger data sets, displays a scatterplot of the observations, but colored using the same colors as the bagplot, with the deepest observation in the strongest blue, the points within the bag in a lighter blue, and the points within the loop in the lightest color. Outliers are shown in black if any exist.\n\n\nCode\noldfaithful |&gt;\n  filter(duration &lt; 7200, waiting &lt; 7200) |&gt;\n  gg_bagplot(duration, waiting, scatterplot = TRUE) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 5.13: Bagplot of the durations and waiting times of Old Faithful eruptions since 2015.\n\n\n\n\n\nFigure 5.13 shows the value of this version of a bagplot, as you can see a lot of detail in the bag and loop that would not be visible otherwise.\n\n\nGeneral comments\nThe idea of depth regions and the depth median can be easily generalized to data with more than two dimensions (Rousseeuw and Struyf 1998). Consequently, it would be possible to define a higher-dimensional version of the bagplot, although it would be difficult to plot it in four or more dimensions. In any case, we will consider approaches to handle high-dimensional data in Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#hdr-boxplots",
    "href": "05-boxplots.html#hdr-boxplots",
    "title": "5  Boxplots",
    "section": "5.7 HDR boxplots",
    "text": "5.7 HDR boxplots\nWe can compute HDRs from a kernel density estimate to find regions of the sample space where observations are unlikely to occur.\nLet’s illustrate the idea with univariate data. In Section 2.7, we estimated the density of the duration of Old Faithful eruptions using a kernel density estimate, and plotted it in Figure 2.16. We can find the 50% and 99% HDRs of this density, which can then be used to form an “HDR boxplot” (Hyndman 1996) as shown in Figure 5.14.\n\n\nCode\nof &lt;- oldfaithful |&gt;\n  filter(duration &lt; 7000)\ndist_kde(of$duration, multiplier = 2) |&gt;\n  gg_density(hdr = \"fill\", prob = c(0.5, 0.99), \n    show_points = TRUE, jitter = TRUE, show_mode = TRUE) +\n  scale_y_continuous(breaks = NULL) + labs(y = \"\") + xlim(-10, 320)\n\n\n\n\n\n\n\n\nFigure 5.14: Kernel density estimate of the duration of Old Faithful eruptions since 2015. Below the density estimate is shown an HDR boxplot of the same data, with the 50% and 99% highest density regions shown as shaded regions, and observations outside the 99% region shown as individual points.\n\n\n\n\n\nThe HDR shown here can also be produced using the gg_hdrboxplot() function.\n\n\nCode\nof |&gt; gg_hdrboxplot(duration, show_anomalies = FALSE) +\n  labs(x = \"Duration (seconds)\") +\n  xlim(-10, 320)\n\n\n\n\n\n\n\n\nFigure 5.15: HDR boxplot of the duration of Old Faithful eruptions since 2015.\n\n\n\n\n\nPoints outside the 99% region are shown separately and jittered vertically to reduce overplotting. This type of boxplot has the advantage that it allows for multimodal distributions, and can identify “inliers” that occur in regions of low density between regions of high density.\nOf course, by definition, 1% of points will lie outside the 99% region, so the points shown are not necessarily anomalies, just observations that occur in the lower probability regions of the space. By setting show_anomalies = TRUE, some of the points are highlighted as potential anomalies, using the method described in Section 6.5.\nThe idea naturally extends to higher dimensions. Figure 5.16 shows the bivariate HDR boxplot of the duration and waiting times for the Old Faithful data. Here we have filtered out very long durations and waiting times first.\n\n\nCode\nof2 &lt;- oldfaithful |&gt;\n  filter(duration &lt; 7000, waiting &lt; 7000) |&gt;\n  select(duration, waiting)\nof2 |&gt; gg_hdrboxplot(duration, waiting, show_anomalies = FALSE) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 5.16: Bivariate kernel density estimate of the duration and waiting times of Old Faithful eruptions since 2015, excluding durations and waiting times longer than 2 hours.\n\n\n\n\n\nThere are 2189 observations used to compute this density estimate, and even with that many observations, computing the 99% HDR contour is difficult, as shown by the rather jagged boundary of the HDR region. This is simply a consequence of the sparsity of points in multidimensional space and in the tails of a distribution.\nAs with bagplots, there is a variation that shows the individual points colored according to the HDR region in which they fall.\n\n\nCode\nof2 |&gt; gg_hdrboxplot(duration, waiting, show_anomalies = FALSE, scatterplot = TRUE) +\n  labs(x = \"Duration (seconds)\", y = \"Waiting time (seconds)\")\n\n\n\n\n\n\n\n\nFigure 5.17: Bivariate HDR boxplot of the duration and waiting times of Old Faithful eruptions since 2015, excluding durations and waiting times longer than 2 hours. Individual points are colored according to the HDR region in which they fall.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "05-boxplots.html#summary",
    "href": "05-boxplots.html#summary",
    "title": "5  Boxplots",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nBoxplots, letter value plots, bagplots, and HDR boxplots are all extremely useful tools in exploratory data analysis, and deserve to be widely applied. They are particularly useful in summarising univariate and bivariate data distributions. However, as we have seen, none of them are effective at anomaly detection, and only HDR boxplots allow for data with multiple modes.\nNevertheless, we will find these tools useful when summarising results from anomaly detection algorithms, and in understanding why some points have been labelled as anomalies.\n\n\n\n\nBarbato, G, E M Barini, G Genta, and R Levi. 2011. “Features and Performance of Some Outlier Detection Methods.” Journal of Applied Statistics, no. 922191616: 1–17. https://doi.org/10.1080/02664763.2010.545119.\n\n\nHoaglin, D C. 1983. “Letter Values: A Set of Selected Order Statistics.” In Understanding Robust and Exploratory Data Analysis, edited by David C Hoaglin and Frederick Mosteller, 33–57. New York: John Wiley.\n\n\nHofmann, H, H Wickham, and K Kafadar. 2017. “Letter-Value Plots: Boxplots for Large Data.” Journal of Computational & Ggraphical Statistics 26 (3): 469–77. https://doi.org/10.1080/10618600.2017.1305277.\n\n\nHyndman, R J. 1996. “Computing and Graphing Highest Density Regions.” The American Statistician 50 (2): 120–26. http://www.jstor.org/stable/2684423.\n\n\nLiu, R Y, J M Parelius, and K Singh. 1999. “Multivariate Analysis by Data Depth: Descriptive Statistics, Graphics and Inference.” The Annals of Statistics 27 (3): 783–858. https://doi.org/10.1214/aos/1018031260.\n\n\nRousseeuw, P J, and I Ruts. 1998. “Constructing the Bivariate Tukey Median.” Statistica Sinica 8: 827–39.\n\n\nRousseeuw, P J, I Ruts, and J W Tukey. 1999. “The Bagplot: A Bivariate Boxplot.” The American Statistician 52 (4): 382–87.\n\n\nRousseeuw, P J, and A Struyf. 1998. “Computing Location Depth and Regression Depth in Higher Dimensions.” Statistics and Computing 8 (3): 193–203.\n\n\nTukey, J W. 1975. “Mathematics and the Picturing of Data.” In Proceedings of the International Congress of Mathematicians, 2:523–31.\n\n\n———. 1977. Exploratory Data Analysis. Addison-Wesley.\n\n\nWickham, H, and L Stryjewski. 2011. “40 Years of Boxplots.” https://vita.had.co.nz/papers/boxplots.html.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Boxplots</span>"
    ]
  },
  {
    "objectID": "06-density.html",
    "href": "06-density.html",
    "title": "6  Density-based methods",
    "section": "",
    "text": "6.1 Surprisals\nAnomalies are observations that are unlikely to have come from the same distribution as the rest of the data. One method to identify anomalies is to first estimate the probability distribution of the data, and then find observations that are improbable under that distribution. This is the approach taken by density-based methods, where anomalies are defined as observations with low probability.\nAny density-based method of anomaly detection first requires that we have a density estimate at each observation, which we will denote by f(\\bm{y}_i). This may be an assumed density, or estimated from the data; it may be a parametric function, or a nonparametric estimate; it may be a conditional density or a marginal density. Wherever it comes from, we will treat f as an appropriate probability density function for the data set. When \\bm{y}_i takes values on a discrete space (such as the integers), then f is a probability mass function, but we will call it a “density” for convenience when discussing methods that can apply to both discrete and continuous data. In many cases, f(\\bm{y}_i) is the estimated likelihood of the observation from a given model.\n“Surprisal” is a term coined by American engineer Myron Tribus (1961) in the context of information theory. In the statistical literature it is better known as a “log score”. But since we are considering anomaly detection, “surprisal” seems a more appropriate word to use here.\nThe surprisal of an observation \\bm{y}_i, is defined as \n  s_i = -\\log f(\\bm{y}_i),\n\\tag{6.1} measuring the surprise in seeing the observation. So it is a measure of how anomalous (or surprising) that observation is, given the density f. A large value of s_i indicates that \\bm{y}_i is not a likely value, and so is a potential anomaly. On the other hand, typical values will have low surprisals. In information theory, the average surprisal is known as the entropy of a random variable (Cover and Thomas 2006; Stone 2022), and provides a measure of uncertainty.\nAlthough the term “surprisal” may not be in common use amongst statisticians and other data scientists, the underlying idea is widely used. For example, if f is obtained from a model with parameters that need to be estimated from the data, then the sum of the surprisals is equal to minus the log likelihood of the data. Therefore, finding parameters that minimize the sum of the surprisals is equivalent to maximum likelihood estimation.\nSurprisals are also commonly used in forecasting and prediction problems (where they are called log scores) to assess whether an estimated distribution f provides a good description of the future data values y (Gneiting and Katzfuss 2014). In that context, the data are thought to be a reliable reflection of the underlying processes, and once the data are observed, the surprisal is used to assess how well the estimated distribution f matches what actually happened. In this book, we are using surprisals in reverse — we assume f is a good description of the underlying processes, and then we use the surprisals to identify observations that are unlikely to have come from that distribution.\nLet’s look at some specific examples.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#sec-surprisals",
    "href": "06-density.html#sec-surprisals",
    "title": "6  Density-based methods",
    "section": "",
    "text": "Normal distribution\nSuppose our observations come from a N(0,1) distribution, and that f denotes the corresponding density function. Then \n  s_i = - \\log\\left(\\frac{1}{\\sqrt{2\\pi}} e^{-y_i^2/2}\\right)\n      = \\frac12 y_i^2 + \\frac12\\log(2\\pi).\n So the surprisals are simply equal to half the squared observations plus a constant. This is easily extended to any normal distribution, showing that surprisals are equal to half the squared scaled observations plus a constant.\n\n\n\\chi^2 distribution\nSimilarly, for a \\chi^2_k distribution, a surprisal is given by \n  s_i = -\\log\\left(\\frac{y_i^{k/2-1}e^{-y_i/2}}{2^{k/2}\\Gamma(k/2)}\\right)\n      = \\frac{1}{2} y_i - (k/2-1)\\log(y_i) + c_k\n where c_k is a constant.\n\n\nPoisson distribution\nSurprisals can also be computed for discrete distributions. For a Poisson(\\lambda) distribution, a surprisal is given by \n  s_i = -\\log\\left(\\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\\right)\n  = \\log(y_i!) - y_i\\log(\\lambda) + \\lambda\n\n\n\nMultivariate standard normal distribution\nThe definition of a surprisal also works for multivariate distributions. For a d-dimensional N(\\bm{0}, \\bm{\\Sigma}) distribution, a surprisal is given by \ns_i = -\\log\\left(\n(2\\pi)^{-d/2}|\\bm{\\Sigma}|^{-1/2} \\exp\\Big\\{-\\frac{1}{2}\\bm{y}_i'\\bm{\\Sigma}^{-1}\\bm{y}\\Big\\} \\right) =\n\\frac{1}{2} \\bm{y}_i'\\bm{\\Sigma}^{-1}\\bm{y}_i + \\frac{d}{2}\\log(2\\pi) + \\frac{1}{2} \\log|\\bm{\\Sigma}|\n When \\bm{\\Sigma} = \\bm{I}, this is simply the sum of surprisals computed on each of the marginal distributions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#surprisal-probabilities",
    "href": "06-density.html#surprisal-probabilities",
    "title": "6  Density-based methods",
    "section": "6.2 Surprisal probabilities",
    "text": "6.2 Surprisal probabilities\nWe can use surprisals to compute the probability of an observation being at least as anomalous as \\bm{y}_i under the distribution f: \np_i = \\int f(\\bm{u}) 𝟙(-\\log f(\\bm{u}) &gt; s_i) d\\bm{u} = \\int f(\\bm{u}) 𝟙(f(\\bm{u}) &lt; f(y_i)) d\\bm{u},\n\\tag{6.2} where 𝟙(z) is an indicator function taking value 1 when z is true and 0 otherwise.\nFigure 6.1 shows the calculation for a \\chi^2_5 distribution.\n\n\n\n\n\n\n\n\nFigure 6.1: Top: Density with one observation shown at y_i. The shaded area shows the area where f(y) &lt; f(y_i), corresponding to p_i. In this example, p_i = 0.1, so the region between the orange shaded segments is a 90% HDR. Middle: surprisals for different values of y. Bottom: surprisal probabilities for different values of y.\n\n\n\n\n\nThe surprisals() function takes values of the data \\bm{y}_i and returns values of either s_i or p_i (depending on the probability argument), where the assumed density f can be provided as a distribution object from the distributional package.\n\ntibble(\n  y = -5:5,\n  s = surprisals(y, distribution = dist_normal(), probability = FALSE),\n  p = surprisals(y, distribution = dist_normal(), probability = TRUE)\n)\n\n#&gt; # A tibble: 11 × 3\n#&gt;        y      s           p\n#&gt;    &lt;int&gt;  &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1    -5 13.4   0.000000573\n#&gt;  2    -4  8.92  0.0000633  \n#&gt;  3    -3  5.42  0.00270    \n#&gt;  4    -2  2.92  0.0455     \n#&gt;  5    -1  1.42  0.317      \n#&gt;  6     0  0.919 1.00       \n#&gt;  7     1  1.42  0.317      \n#&gt;  8     2  2.92  0.0455     \n#&gt;  9     3  5.42  0.00270    \n#&gt; 10     4  8.92  0.0000633  \n#&gt; 11     5 13.4   0.000000573\n\n\nThe default value of the probability argument is TRUE.\nThe value of p_i is the probability of an observation being at least as anomalous as \\bm{y}_i​ if the observations come from the distribution f. So if we label points as anomalous when p_i &lt; \\alpha, we will have a false positive rate of \\alpha.\n\nConnection to Highest density regions\nSurprisal probabilities are related to highest density regions discussed in Section 2.5 and Section 3.4. If \\bm{y}_i lies on the boundary of the 100(1-\\alpha)\\% highest density region, then p_i = 1-\\alpha. This should be clear by comparing Equation 6.2 with Equation 3.1.\n\n\nConnection to z-scores\nFor any symmetric univariate distribution, p_i = 2[1 - F(c+|y_i-c|)], where F is the corresponding distribution function centered at c. Therefore, under the Normal distribution N(\\mu, \\sigma^2), p_i = 2[1-\\Phi(|y_i-\\mu|/\\sigma)], where \\Phi is the standard Normal distribution function.\nThis shows that when f is a Normal density, identifying anomalies as observations with p_i &lt; \\alpha is equivalent to a z-score test (Section 4.2) with the threshold set to the 1-\\alpha/2 quantile of the standard Normal distribution.\n\n\nLOO surprisals\nIf the density f has been estimated from the data, then there is a problem of circular reasoning. Any genuine anomalies may affect the estimated density, and then they may not appear to be anomalies. Therefore, it is sometimes useful to consider the leave-one-out (LOO) estimate given by f_{-i}. That is, f_{-i} is the density estimate using all observations other than the ith observation. Then an unusual observation can’t influence the density estimate, giving us a better measure of how anomalous it really is. In this case, we will call the associated surprisal, a “LOO surprisal”, given by s_i = -\\log f_{-i}(\\bm{y}_i).\nThe surprisals() function has a loo argument for specifying whether we want LOO surprisals used in the computation.\n\n\nApproximate GPD surprisal probabilities\nIf we do not know what the distribution of the data should be, or if we think our assumed distribution is incorrect for some reason, there are some ways to find approximate surprisal probabilities that are still accurate even when the assumed distribution is not.\nLet f^* denote the assumed (and possibly incorrect) density function. It might be a poor estimate of the true density, or it might just be a density on the same sample space as the data. Using this assumed density, we can compute the surprisals, s_i = -\\log f^*(\\bm{y}_i).\nThen we can fit a Generalized Pareto Distribution (GPD) to the largest surprisals using the POT approach discussed in Section 2.8. Even when f^* is incorrect or poorly estimated, the extreme surprisals will still follow a Generalized Pareto Distribution.\nLet \\beta denote the tail probability used in the GPD threshold, and let u denote the 1-\\beta sample quantile of the surprisal values. For example, \\beta = 0.1 is a common choice, and then u is the 90^{\\text{th}} percentile of the s_i values. We estimate a GPD distribution using the surprisals above u, and then compute the approximate surprisal probability of each observation with s_i&gt;u by \n\\hat{p}_i = \\beta\\int_{s_i}^\\infty \\hat g(s) ds,\n\\tag{6.3} where \\hat g() denotes the estimated GPD density function. This approximation only gives us surprisal probabilities for the surprisals above the threshold u. For any s_i \\le u, all we can say is that \\hat{p}_i \\ge \\beta.\n\n\n\n\n\n\nNeed some theory here\n\n\n\nWhat can we say about the quality of the approximation for those surprisals above u?\nIs it true that the FTG theorem holds for all surprisal values, or do we need conditions on f?\ne.g., if the surprisal ranks under f and f^* are equal, I think this should work ok. That should be true for any distribution where u is greater than the largest mode of f.\n\n\nThe approach does require a relatively large number of surprisals to be available, in order to adequately estimate the GPD from the top \\beta of the observations. So we would normally need at least a few hundred observations to start with.\nThe surprisals() function has an approximation argument for specifying if we want to use the GPD approximation in computing surprisal probabilities. The argument threshold_probability is used to specify \\beta, with the default value of 0.1.\n\n\nApproximate empirical surprisal probabilities\nAn alternative approach that does not require a GPD is to estimate p_i as the proportion of surprisals larger than s_i. For large n, this provides a quick and convenient estimate of p_i that is accurate even when the assumed distribution f^* is not. Suppose f is the true (unknown) density, and f^* is the assumed (possibly incorrect) density. This approach can work well for moderate values of p_i, but we need a large number of available surprisals in order to estimate small values of p_i.\n\n\n\n\n\n\nNeed some theory here\n\n\n\nWhat can we say about the quality of the approximation?\nIf the surprisal ranks under f and f^* are equal, this is obviously ok. That should be true in the tails of any distribution — i.e., when y_i is greater than the largest mode of f, or less than the smallest mode of f.\nCan we say anything else?\n\n\nThe approximation argument of surprisals() can also be used to specify if we want to use the empirical surprisal probabilities.\n\n\nExample: Standard normal values\nFor example, consider the 1000 values contained in the first column of n01. These were generated from a standard Normal distribution. We can use surprisals() to identify anomalies with probability less than 1%. This is equivalent to z-scores greater than 2.576 in absolute value.\nWe will use all three methods described above, first using the correct density in computing the surprisals, and then repeating the calculation using an incorrect density. For the incorrect density, we will use a t_3 distribution, which has much longer tails than the true N(0,1) distribution.\n\n# Compute surprisal probabilities of v1 assuming a standard Normal distribution\n# and a t4 distribution\nnorm_prob &lt;- tibble(\n  y = n01$v1,\n  p =      surprisals(y, distribution = dist_normal()),\n  p_gpd =  surprisals(y, distribution = dist_normal(), approximation = \"gpd\"),\n  p_emp =  surprisals(y, distribution = dist_normal(), approximation = \"empirical\"),\n  pt =     surprisals(y, distribution = dist_student_t(4)),\n  pt_gpd = surprisals(y, distribution = dist_student_t(4), approximation = \"gpd\"),\n  pt_emp = surprisals(y, distribution = dist_student_t(4), approximation = \"empirical\")\n)\nnorm_prob |&gt; arrange(p)\n\n#&gt; # A tibble: 1,000 × 7\n#&gt;        y        p    p_gpd p_emp     pt   pt_gpd pt_emp\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  3.81 0.000139 0.000135 0.001 0.0191 0.000132  0.001\n#&gt;  2  3.06 0.00225  0.00261  0.002 0.0380 0.00284   0.002\n#&gt;  3 -3.01 0.00263  0.00308  0.003 0.0398 0.00334   0.003\n#&gt;  4 -3.00 0.00273  0.00320  0.004 0.0402 0.00346   0.004\n#&gt;  5 -2.94 0.00328  0.00389  0.005 0.0425 0.00418   0.005\n#&gt;  6 -2.89 0.00387  0.00461  0.006 0.0448 0.00492   0.006\n#&gt;  7  2.68 0.00746  0.00913  0.007 0.0556 0.00948   0.007\n#&gt;  8  2.65 0.00807  0.00991  0.008 0.0572 0.0102    0.008\n#&gt;  9 -2.60 0.00943  0.0116   0.009 0.0604 0.0119    0.009\n#&gt; 10 -2.59 0.00953  0.0118   0.01  0.0607 0.0121    0.01 \n#&gt; # ℹ 990 more rows\n\n\nThe most accurate values are in column p, as this uses the correct distribution applied to the correct surprisals. Notice that p_gpd and p_emp are both relatively accurate: the GPD and empirical approximations work. For the probabilities calculated using the incorrect distribution, pt is relatively inaccurate, with much larger probabilities than they should be, especially in the extreme tails. But pt_gpd and pt_emp are both reasonable estimates, despite being based on an incorrect distribution for computing the surprisals. In fact, p_emp and pt_emp are identical, because the ordering of the surprisals is unchanged despite the incorrect distribution being used.\nThe various estimates are displayed in Figure 6.2 for values of y greater than 2.5, showing how the estimates (other than pt) are particularly accurate in the extreme tails, where we need them for anomaly detection.\n\n\nCode\nnorm_prob |&gt;\n  filter(y &gt; 2.5) |&gt;\n  tidyr::pivot_longer(p:pt_emp, names_to = \"Estimate\", values_to = \"Probability\") |&gt;\n  ggplot(aes(x = y, y = Probability, col = Estimate)) +\n  geom_line()\n\n\n\n\n\n\n\n\nFigure 6.2: Surprisal probabilities estimated using the correct N(0,1) distribution and using an incorrect t_4 distribution. The GPD and empirical estimates of surprisal probabilities are still accurate, even when the wrong distribution is used to compute the surprisal values.\n\n\n\n\n\nIn summary, even when we don’t know the distribution of the data, we can use some incorrect assumed distribution to compute the surprisals, and then apply either a GPD or the empirical distribution to obtain good estimates of the surprisal probabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#sec-regression-log-scores",
    "href": "06-density.html#sec-regression-log-scores",
    "title": "6  Density-based methods",
    "section": "6.3 Linear regression surprisals",
    "text": "6.3 Linear regression surprisals\nSuppose we want to find anomalies amongst n univariate observations y_1,\\dots,y_n, and we have p variables that we think might be useful for predicting y. Then we can write the conditional density as f(y \\mid \\bm{x}), where \\bm{x} is a p-dimensional vector of predictor variables. Anomalies in y are identified as observations that are unlikely to have come from the conditional density f. This is commonly called a “regression model”, regardless of the form of f, or whether the relationship with \\bm{x} is linear or not.\nBy far the most common type of regression model assumes that f is a Normal distribution, and that the conditional mean is a linear function of \\bm{x}. Note that this does not mean that y is Normally distributed, or that \\bm{x} has a Normal distribution. The assumption is that the conditional distribution of y given \\bm{x} is Normal, which can easily be checked by looking at the residuals from the regression model.\nFor a linear Normal regression model, with independent observations and homoscedastic errors, the conditional distribution is given by \n  y \\mid \\bm{x} \\sim N(\\bm{x}_+'\\bm{\\beta}, \\sigma^2),\n\\tag{6.4} where \\bm{x}_+ = [1, \\bm{x}]' is a (p+1)-dimensional vector containing a 1 in the first position and the predictors in the remaining positions, and \\bm{\\beta} is a (p+1)-dimensional vector of regression coefficients.\n\nModel estimation\nThe model can be written in matrix form as \n  \\bm{y} \\sim N(\\bm{X}\\bm{\\beta}, \\sigma^2\\bm{I}),\n where \\bm{X} is an n\\times(p+1) matrix with the first column being a vector of 1s, and the other columns containing the predictor variables, or equivalently as \n  \\bm{\\varepsilon} = \\bm{y} - \\bm{X}\\bm{\\beta} \\sim N(\\bm{0}, \\sigma^2\\bm{I}).\n\\tag{6.5} Provided \\bm{X} is of rank p+1, and the errors \\bm{\\varepsilon} are independent of \\bm{X}, the model can be estimated using ordinary least squares regression (Seber and Lee 2003), resulting in the estimate \n  \\hat{\\bm{\\beta}} = (\\bm{X}'\\bm{X})^{-1}\\bm{X}'\\bm{y}.\n The fitted values (i.e., predicted values for the training data) are given by \n  \\hat{\\bm{y}} = \\bm{X}\\hat{\\bm{\\beta}} = \\bm{H}\\bm{y},\n where \\bm{H} = \\bm{X}(\\bm{X}'\\bm{X})^{-1}\\bm{X}' is known as the “hat”-matrix because it creates the “y-hat” values \\hat{\\bm{y}} from the data \\bm{y}.\nThe diagonals of \\bm{H}, given by h_1,\\dots,h_n, take values between 0 and 1. These are known as the “leverage” values (Faraway 2014, p69), and measure how much each observation influences the corresponding fitted value. High leverage values (close to 1) correspond to observations that have a large influence on the estimated coefficients, and so leaving those observations out will lead to very different values for the fitted values and residuals. On the other hand, small leverage values (close to 0) correspond to observations that have little influence on the estimated coefficients, and so leaving those observations out will lead to similar values for the fitted values and residuals.\n\n\nResiduals\nThe residuals from the model are given by \n  \\bm{e} = \\bm{y} - \\hat{\\bm{y}} = (\\bm{I} - \\bm{H})\\bm{y}.\n\\tag{6.6} Note that the residuals have the distribution \\bm{e}\\mid\\bm{X} \\sim N(\\bm{0}, \\sigma^2(\\bm{I} - \\bm{H})), which is not quite the same as the distribution of the errors given by Equation 6.5. However, as n\\rightarrow\\infty, \\bm{H}\\rightarrow\\bm{I}, so the two distributions are asymptotically equivalent. Often, we need standardized residuals, which are obtained by dividing each residual by its estimated standard deviation, giving  r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}, \\qquad i = 1,\\dots, n,\n where \n\\hat\\sigma^2 = \\frac{1}{n-p-1}\\sum_{i=1}^n e_i^2\n\\tag{6.7} is the estimated residual variance.\nA linear model can be estimated in R using the stats::lm() function. The broom::augment() function will compute the residuals (named .resid), the standardized residuals (named .std.resid), and the leverage values (names .hat).\nThe surprisals under the Gaussian linear regression model Equation 6.4 can be estimated using these standardized residuals, giving \n  s_i = -\\log\\phi(r_i),\n\\tag{6.8} where \\phi(u) = (2\\pi)^{-1/2}e^{-u^2} is the standard normal density. This can be computed as follows, assuming that fit is the output from stats::lm().\nbroom::augment(fit) |&gt;\n  mutate(surprisals = -dnorm(.std.resid, log = TRUE))\nEquivalently, the surprisals() function will compute them:\nsurprisals(fit, probability = FALSE)\n\n\nLOO residuals\nThe leave-one-out residual for the ith observation is defined as the difference between \\bm{y}_i and the predicted value obtained using a model fitted to all observations except the ith observation. In this context, LOO residuals are often called PRESS (prediction error sum of squares) residuals. At first, it appears that calculating LOO residuals involves a lot of computation — estimating n separate models. However, the leave-one-out residuals are easily obtained without actually having to re-estimate the model many times. It can be shown (Montgomery, Peck, and Vining 2012, Appendix C.7) that the leave-one-out (LOO) residuals are given by \n  e_{-i}  = e_{i}/(1-h_{i}),\n\\tag{6.9} where e_{i} is the residual obtained from fitting the model to all observations.\nThe variance of e_{-i} is given by \n  \\frac{\\text{Var}(e_i)}{(1-h_i)^2} = \\frac{\\sigma^2}{1-h_i},\n so we can standardize each LOO residual to obtain \n\\frac{e_i / (1-h_i)}{\\sigma/\\sqrt{1-h_i}} = \\frac{e_i}{\\sigma \\sqrt{1-h_i}}\n To avoid any anomalies affecting the estimate of \\sigma, we consider again the leave-one-out models. If we leave out the ith observation, and fit a regression model to the remaining observations, then the estimated variance of the residuals is given by (Montgomery, Peck, and Vining 2012, Appendix C.8) \n  \\hat\\sigma_{-i}^2 = \\frac{1}{n-p-2}\\left[(n-p-1)\\hat\\sigma^2 - e_{i}^2/(1-h_i)\\right],\n\\tag{6.10} where \\hat\\sigma^2 is given by Equation 6.7. These are computed by broom::augment() and are returned in the column .sigma. Thus the standardized LOO residuals (also known as “externally studentized residuals”) are given by \nr_{-i} = \\frac{e_i}{\\hat\\sigma_{-i} \\sqrt{1-h_i}}.\n Finally, the LOO regression surprisals are given by \n  s_{-i} = -\\log \\phi(r_{-i})\n\\tag{6.11}\nIf fit is the output from stats::lm(), then these quantities can be computed as follows.\nbroom::augment(fit) |&gt;\n  mutate(\n    std_loo_res = .resid / (.sigma * sqrt(1 - .hat)),\n    loo_surprisals = -log(dnorm(std_loo_res, log = TRUE))\n  )\nMore simply, we can just use the surprisals() function again:\nsurprisals(fit, type = \"loo\", probability = FALSE)\n\n\nExample: Shiraz reviews\nFor example, consider the wine reviews of Shiraz (aka Syrah), plotted in Figure 1.6. We can fit a linear regression model to these data to obtain a conditional density estimate of price given the points awarded to each wine. Then, \\bm{X} contains just two columns: a column of 1s, and a column containing the points values. The vector \\bm{y} contains the log prices of the wines. The model can be fitted as follows.\n\nwine_reviews &lt;- fetch_wine_reviews()\n\n\nshiraz &lt;- wine_reviews |&gt; filter(variety %in% c(\"Shiraz\", \"Syrah\"))\nfit_wine &lt;- lm(log(price) ~ points, data = shiraz)\nsummary(fit_wine)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(price) ~ points, data = shiraz)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.6407 -0.3361 -0.0109  0.3052  2.9909 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -6.05913    0.20731   -29.2   &lt;2e-16 ***\n#&gt; points       0.10690    0.00232    46.0   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.485 on 4494 degrees of freedom\n#&gt; Multiple R-squared:  0.32,   Adjusted R-squared:  0.32 \n#&gt; F-statistic: 2.12e+03 on 1 and 4494 DF,  p-value: &lt;2e-16\n\n\nThe fitted model can be written as \n  \\log(\\text{Price}) \\sim N(-6.059 + 0.107 \\times \\text{Points}, 0.485^2),\n and is depicted in Figure 6.3 with 95% prediction intervals.\n\n\nCode\nwine_aug &lt;- broom::augment(fit_wine, data = shiraz, interval = \"prediction\") |&gt;\n  mutate(\n    lwr = exp(.lower),\n    upr = exp(.upper),\n    location = case_when(\n      price &lt; lwr ~ \"below\",\n      price &gt; upr ~ \"above\",\n      TRUE ~ \"within\"\n    )\n  )\nwine_aug |&gt;\n  ggplot(aes(y = price, x = points, col = location)) +\n  geom_jitter(height = 0, width = 0.1, alpha = 0.5) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = \"#cccccc\", alpha = 0.25) +\n  geom_line(aes(y = exp(.fitted)), color = \"#666666\") +\n  scale_y_log10() +\n  guides(fill = \"none\", col = \"none\") +\n  scale_color_manual(values = c(\"#0072B2\", \"#D55E00\", \"#AAAAAA\"))\n\n\n\n\n\n\n\n\nFigure 6.3: Log price of Shiraz as a function of points, with 95% prediction intervals. The points are horizontally jitted to reduce overplotting. Points outside the prediction intervals are colored.\n\n\n\n\n\nThe LOO surprisals obtained from this model are shown in Figure 6.4, using the same colors as Figure 6.3 to indicate whether the observation is below, within, or above, the 95% prediction interval.\n\n\nCode\nwine_aug &lt;- wine_aug |&gt;\n  mutate(\n    std_loo_res = .resid / (.sigma * sqrt(1 - .hat)),\n    surprisals = -dnorm(.std.resid, log = TRUE),\n    loo_surprisals = -dnorm(std_loo_res, log = TRUE)\n  )\nwine_aug |&gt;\n  select(points, std_loo_res, loo_surprisals, location) |&gt;\n  tidyr::pivot_longer(c(std_loo_res, loo_surprisals), names_to = \"variable\", values_to = \"value\") |&gt;\n  mutate(variable = factor(variable,\n    levels = c(\"std_loo_res\", \"loo_surprisals\"),\n    label = c(\"Standardized LOO residuals\", \"LOO surprisals\")\n  )) |&gt;\n  ggplot(aes(x = points, y = value, col = location)) +\n  facet_grid(variable ~ ., scales = \"free_y\") +\n  geom_jitter(height = 0, width = 0.1, alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"#666666\") +\n  labs(x = \"Points\", y = \"\") +\n  guides(fill = \"none\", col = \"none\") +\n  scale_color_manual(values = c(\"#0072B2\", \"#D55E00\", \"#AAAAAA\"))\n\n\n\n\n\n\n\n\nFigure 6.4: Residuals and surprisals for the Shiraz data using a linear regression model. Points are colored to match the 95% prediction intervals in Figure 6.3.\n\n\n\n\n\nThe over-priced wines under this model are shown in blue, while the under-priced wines are shown in orange. This shows that the most anomalous observations are the two with LOO surprisals above 17, and studentized residuals close to 6. The largest LOO surprisal is for the most over-priced wine (under this model), a 2009 Shiraz from the Henschke winery in the Eden Valley region of South Australia, with 91 points and a price of $780.\n\nwine_aug |&gt;\n  filter(loo_surprisals == max(loo_surprisals)) |&gt;\n  select(country:winery, year, points, price, std_loo_res, loo_surprisals)\n\n#&gt; # A tibble: 1 × 9\n#&gt;   country   state  region winery  year points price std_loo_res loo_surprisals\n#&gt;   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 Australia South… Eden … Hensc…  2009     91   780        6.20           20.1\n\n\nThe largest LOO surprisal corresponding to an under-priced wine is for the wine with the lowest residual value, with 85 points and a price of $4. Another good buy, at the higher quality end, is the 2007 Syrah from the Rulo winery in the Columbia Valley in Washington State, USA:\n\nwine_aug |&gt;\n  filter(points &gt; 95) |&gt;\n  filter(std_loo_res == min(std_loo_res)) |&gt;\n  select(country:winery, year, points:price, std_loo_res, loo_surprisals)\n\n#&gt; # A tibble: 1 × 9\n#&gt;   country state    region winery  year points price std_loo_res loo_surprisals\n#&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 US      Washing… Colum… Rulo    2007     96    20       -2.49           4.03\n\n\nThis corresponds to the only orange point in Figure 6.3 that has a point value above 95 and a price below the 95% prediction interval.\n\n\nSurprisal probabilities\nWe will compute the surprisal probabilities using the empirical approximation.\n\nwine_aug |&gt;\n  mutate(prob = rank(-loo_surprisals) / NROW(wine_aug)) |&gt;\n  select(country:winery, year, points, price, loo_surprisals, prob) |&gt;\n  arrange(prob)\n\n#&gt; # A tibble: 4,496 × 9\n#&gt;    country   state     region winery  year points price loo_surprisals    prob\n#&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Australia South Au… Eden … Hensc…  2009     91   780          20.1  2.22e-4\n#&gt;  2 US        Californ… Centr… Law     2013     92   750          18.3  4.45e-4\n#&gt;  3 Australia South Au… Eden … Hensc…  2010     96   820          14.4  6.67e-4\n#&gt;  4 Australia South Au… South… Penfo…  2008     98   850          12.5  8.90e-4\n#&gt;  5 Italy     Tuscany   Tosca… Tua R…  2011     89   300          11.7  1.11e-3\n#&gt;  6 Australia South Au… South… Penfo…  2010     99   850          11.5  1.33e-3\n#&gt;  7 Italy     Tuscany   Tosca… Tua R…  2013     90   300          10.7  1.67e-3\n#&gt;  8 Italy     Tuscany   Tosca… Tua R…  2012     90   300          10.7  1.67e-3\n#&gt;  9 France    Rhône Va… Côte … Domai…  2013     93   391          10.2  2.00e-3\n#&gt; 10 Australia South Au… South… Penfo…  2004     96   500           9.57 2.22e-3\n#&gt; # ℹ 4,486 more rows\n\n\nThose with the ten smallest surprisal probabilities are all wines that appear to be over-priced given their points values.\nThe preceding analysis shows the calculations of surprisal probabilities in detail. They can be more simply computed using surprisals() as follows.\n\nshiraz &lt;- shiraz |&gt;\n  mutate(prob = surprisals(fit_wine, approximation = \"empirical\"))\nshiraz |&gt;\n  select(country:winery, year, points, price, prob) |&gt;\n  arrange(prob)\n\n#&gt; # A tibble: 4,496 × 8\n#&gt;    country   state           region          winery  year points price    prob\n#&gt;    &lt;chr&gt;     &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 Australia South Australia Eden Valley     Hensc…  2009     91   780 2.22e-4\n#&gt;  2 US        California      Central Coast   Law     2013     92   750 4.45e-4\n#&gt;  3 Australia South Australia Eden Valley     Hensc…  2010     96   820 6.67e-4\n#&gt;  4 Australia South Australia South Australia Penfo…  2008     98   850 8.90e-4\n#&gt;  5 Italy     Tuscany         Toscana         Tua R…  2011     89   300 1.11e-3\n#&gt;  6 Australia South Australia South Australia Penfo…  2010     99   850 1.33e-3\n#&gt;  7 Italy     Tuscany         Toscana         Tua R…  2013     90   300 1.67e-3\n#&gt;  8 Italy     Tuscany         Toscana         Tua R…  2012     90   300 1.67e-3\n#&gt;  9 France    Rhône Valley    Côte Rôtie      Domai…  2013     93   391 2.00e-3\n#&gt; 10 Australia South Australia South Australia Penfo…  2004     96   500 2.22e-3\n#&gt; # ℹ 4,486 more rows\n\n\nFigure 6.5 shows the relationship between points and price, with the points colored according to the surprisal probability. The eight blue points are observations with surprisal probabilities less than 0.002.\n\n\nCode\nshiraz |&gt;\n  ggplot(aes(x = points, y = price, color = prob &lt; 0.002)) +\n  geom_jitter(height = 0, width = 0.2) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nFigure 6.5: Price vs points for Shiraz wines. Points are colored according to the surprisal probability. The eight blue points are observations with surprisal probabilities less than 0.002.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#gam-surprisals",
    "href": "06-density.html#gam-surprisals",
    "title": "6  Density-based methods",
    "section": "6.4 GAM surprisals",
    "text": "6.4 GAM surprisals\nIn some applications, it is not appropriate to assume the conditional density is Gaussian, or that the relationships are linear. One useful model that allows for non-Gaussian densities, and non-linear relationships, is a generalized additive model or GAM. Under this model, the conditional density is given by (Wood 2017) \n  y\\mid\\bm{x} \\sim f(\\mu), \\qquad \\ell(\\mu) = \\sum_{k=1}^p g_k(x_{k}),\n where \\mu = \\text{E}(y | \\bm{x}) denotes the conditional mean, \\ell() is a link function, and each g_k function is smooth. If f is Normal, \\ell is the identity, and g_i(u) = \\beta_i u, then this reduces to the linear Gaussian model (Equation 6.4).\nConsider the number of “not outs” for each batter in the cricket_batting data set. A “not out” occurs when a batsman has not been dismissed at the end of the team’s innings. Let’s consider if there are some batters who have an unusually high proportion of not outs. The data set contains results from 91022 innings, of which 11883 were not outs. So the overall proportion of not outs is 11883 / 91022 = 0.131.\nFigure 6.6 shows the proportion of not outs for each batter as a function of the number of innings they played. The unusual patterns on the left of each plot is due to the discrete nature of the data — both the number of not outs and the number of innings must be integers. There is some overplotting that occurs due to batters having the same numbers of not-outs and innings, which results in the higher color density of the corresponding plotted points. Batters who have played only a few innings tend to have a higher proportion of not outs on average, and a higher variance, than those who have played a large number of innings.\n\ndf_no &lt;- cricket_batting |&gt;\n  filter(Innings &gt; 0) |&gt;\n  mutate(prop_no = NotOuts / Innings)\ndf_no |&gt;\n  ggplot(aes(x = Innings, y = NotOuts / Innings)) +\n  geom_point(alpha = 0.15)\n\n\n\n\n\n\n\nFigure 6.6: Proportion of not outs for each batter as a function of the number of innings they played.\n\n\n\n\n\nThis suggests that we can construct a GAM for the number of not outs for each batter as a function of the number of innings they played. It is natural to use a Binomial distribution with a logit link function: \n  \\text{NotOuts} \\mid \\text{Innings} \\sim \\text{Binomial}(n=\\text{Innings},~ p),\n where p denotes the probability of a batter not being dismissed in an innings, and \n  \\log(p / (1- p)) = g(\\text{Innings})\n We can fit this model using the mgcv package.\n\nfit_notouts &lt;- mgcv::gam(prop_no ~ s(Innings),\n  data = df_no,\n  family = binomial(link = logit), weights = Innings\n)\nnotouts_aug &lt;- broom::augment(fit_notouts, data = df_no, type.predict = \"response\")\nnotouts_aug |&gt;\n  ggplot(aes(x = Innings, y = prop_no)) +\n  geom_point(alpha = 0.2) +\n  geom_line(aes(y = .fitted), color = \"#0072B2\") +\n  geom_ribbon(\n    aes(\n      ymin = .fitted - 2 * .se.fit,\n      ymax = .fitted + 2 * .se.fit\n    ),\n    fill = \"#0072B2\", alpha = 0.2\n  ) +\n  labs(y = \"Proportion of not outs\")\n\n\n\n\n\n\n\nFigure 6.7: Proportion of not outs for each batter as a function of the number of innings they played, with a GAM fit using a Binomial distribution. The blue line shows the probability of a batter being not out as a function of the number of Innings they have played.\n\n\n\n\n\nNow we can use the fitted model to compute the surprisals from the Binomial distribution, and find the most anomalous batters. Unfortunately, there is not a convenient way to compute LOO surprisals for GAM models, so we will only consider regular surprisals in this example. While the Binomial distribution is probably a good first approximation, it does assume that the probability of a batter being dismissed does not change over their career, which is an unlikely assumption as it does not allow for the development of skill, the value of experience, or the effect of ageing. So we will use the GPD approximation to compute the surprisal probabilities.\n\nnotouts_aug &lt;- notouts_aug |&gt;\n  mutate(\n    surprisals = surprisals(fit_notouts, probability = FALSE),\n    prob = surprisals(fit_notouts, approximation = \"gpd\")\n  ) |&gt;\n  select(Player:Country, Innings:NotOuts, prop_no:.fitted, surprisals:prob) |&gt;\n  arrange(desc(surprisals))\nnotouts_aug\n\n#&gt; # A tibble: 3,702 × 8\n#&gt;    Player           Country Innings NotOuts prop_no .fitted surprisals    prob\n#&gt;    &lt;chr&gt;            &lt;chr&gt;     &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 JM Anderson      England     233      99   0.425  0.120        71.3 3.10e-4\n#&gt;  2 CS Martin        New Ze…     104      52   0.5    0.121        47.1 7.32e-4\n#&gt;  3 RGD Willis       England     128      55   0.430  0.121        40.8 9.88e-4\n#&gt;  4 CA Walsh         West I…     185      61   0.330  0.0965       40.7 9.91e-4\n#&gt;  5 TA Boult         New Ze…      86      42   0.488  0.121        37.1 1.20e-3\n#&gt;  6 M Muralitharan   ICC/Sr…     164      56   0.341  0.102        36.8 1.22e-3\n#&gt;  7 EJ Chatfield     New Ze…      54      33   0.611  0.131        36.0 1.28e-3\n#&gt;  8 BS Chandrasekhar India        80      39   0.488  0.123        34.1 1.43e-3\n#&gt;  9 GD McGrath       Austra…     138      51   0.370  0.118        31.8 1.65e-3\n#&gt; 10 M Ntini          South …     116      45   0.388  0.122        29.1 1.99e-3\n#&gt; # ℹ 3,692 more rows\n\n\nThe most anomalous batters are all “tail-enders” (i.e., not skilled batters) who played for a long time (so they have a large number of innings). Because they batted last, or nearly last, they are more likely to be not out at the end of the team’s innings.\nThe .fitted value is the expected proportion of not outs for each player given the number of innings they have played, while prop_no gives the actual proportion of not outs they have had. The largest surprisal is for English batter Jimmy Anderson, who has had 99 not outs in 233 innings, which is much higher than the expected number of not outs of 233 \\times 0.120 = 27.9. This anomaly is also seen in Figure 6.7, as being somewhat unusual for that part of the data. Although Jimmy Anderson was not a great batter, he was good at defence, and was able to bat for a long time without being dismissed, leaving the other batter time to score runs.\nWe have identified an anomaly that is not anomalous in the proportion of not-outs, or in the number of innings, and the difference between the actual proportion and the predicted proportion is not anomalous either compared to some of the other values. However, because we have used a statistical model, we have been able to account for the particular features of this data set, such as the discrete nature of the data, and the changing variance, to identify an observation that is anomalous in the context of the model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#sec-kdescores",
    "href": "06-density.html#sec-kdescores",
    "title": "6  Density-based methods",
    "section": "6.5 KDE surprisals",
    "text": "6.5 KDE surprisals\nSuppose, instead of a regression or a GAM, we estimate f using a kernel density estimate. In fact, when the surprisals() functions is applied to data without specifying a probability distribution, a kernel density estimate of the data is used by default. The kernel density estimate at each observation is (Equation 3.4) \n  f_i = \\hat{f}(\\bm{y}_i) = \\frac{1}{n} \\sum_{j=1}^n K_H(\\bm{y}_i-\\bm{y}_j),\n\\tag{6.12} and so the “kde surprisal” at each observation as \n  p_i = -\\log(f_i).\n The largest possible surprisal occurs when an observation has no other observations nearby. Then f_i \\approx K_H(\\bm{0})/n because K_H(\\bm{y}_i-\\bm{y}_j)\\approx 0 when \\|\\bm{y}_i-\\bm{y}_j\\| is large. So the largest possible kde surprisal, when using a Gaussian kernel, is \n  -\\log(K_H(\\bm{0})/n) \\approx \\log(n) + \\frac{d}{2}\\log(2\\pi) + \\frac{1}{2}\\text{log det}(\\bm{H}),\n where \\bm{H} is now the bandwidth matrix. For univariate data, when d=1, this simplifies to \n  -\\log(K_h(0)/n) \\approx \\log(nh\\sqrt{2\\pi}).\n\n\nLeave-one-out kde surprisals\nThe contribution of the ith point to the kernel density estimate at that point is K_H(\\bm{0})/n. Therefore, we can compute leave-one-out kde surprisals as \n  f_{-i} = \\left[nf_i - K_H(\\bm{0})\\right]/(n-1),\n\\tag{6.13} where f_i is the kde estimate at \\bm{y}_i using all data. Thus, we can compute the leave-one-out kernel surprisals without needing to re-estimate the density many times.\nThe largest possible LOO kde surprisal is infinite, occuring when there are no observations nearby, so that nf_i = K_H(\\bm{0}).\n\n\nThe lookout algorithm\nThe “lookout” algorithm (standing for Leave-One-Out Kernel density estimates for OUTlier detection) was proposed by Kandanaarachchi and Hyndman (2022) and uses surprisal probabilities based on a GPD approximation. Unlike the KDE implementation used in this book, it uses a method of bandwidth selection based on topological data analysis. Probabilities obtained using this algorithm can be calculated using the lookout_prob() function.\n\n\nExample: Old Faithful eruption durations\nFor the Old Faithful eruption duration data, we obtain the following results.\n\nof_surprisals &lt;- oldfaithful |&gt;\n  mutate(\n    loo_kde_surprisal = surprisals(duration, loo = TRUE, probability = FALSE),\n    prob = surprisals(duration, loo = TRUE),\n  )\nof_surprisals |&gt; arrange(desc(loo_kde_surprisal))\n\n#&gt; # A tibble: 2,261 × 5\n#&gt;    time                duration waiting loo_kde_surprisal    prob\n#&gt;    &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 2015-12-07 00:09:00     7200    3420            Inf    0      \n#&gt;  2 2018-04-25 19:08:00        1    5700            Inf    0      \n#&gt;  3 2020-10-25 16:31:00      305    6060              7.66 0.00502\n#&gt;  4 2015-03-07 00:50:00      160    5281              7.57 0.00601\n#&gt;  5 2019-03-14 22:43:00      160    5580              7.57 0.00601\n#&gt;  6 2020-09-15 18:01:00      160    5880              7.57 0.00601\n#&gt;  7 2020-09-16 14:44:00      160    6120              7.57 0.00601\n#&gt;  8 2020-05-07 22:55:00      304    6180              7.54 0.00854\n#&gt;  9 2020-05-21 00:27:00      304    5640              7.54 0.00854\n#&gt; 10 2017-09-01 01:18:00      155    4560              7.54 0.00919\n#&gt; # ℹ 2,251 more rows\n\n\nThe two infinite LOO surprisals correspond to the extreme 2 hour duration, and the tiny 1 second duration. These are so improbable given the rest of the data, that the scores are effectively infinite. The prob column contains the LOO surprisal probabilities based on the KDE.\nIf we omit the two extreme values, we obtain different results.\n\nof_surprisals &lt;- oldfaithful |&gt;\n  filter(duration &gt; 1, duration &lt; 7000) |&gt;\n  mutate(\n    loo_kde_surprisal = surprisals(duration, loo = TRUE, probability = FALSE),\n    prob = surprisals(duration, loo = TRUE),\n  )\nof_surprisals |&gt; arrange(desc(loo_kde_surprisal))\n\n#&gt; # A tibble: 2,259 × 5\n#&gt;    time                duration waiting loo_kde_surprisal    prob\n#&gt;    &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 2020-10-25 16:31:00      305    6060              8.11 0.00412\n#&gt;  2 2017-09-01 01:18:00      155    4560              8.01 0.00464\n#&gt;  3 2017-09-07 04:56:00      155    4740              8.01 0.00464\n#&gt;  4 2020-05-07 22:55:00      304    6180              8.01 0.00467\n#&gt;  5 2020-05-21 00:27:00      304    5640              8.01 0.00467\n#&gt;  6 2015-03-07 00:50:00      160    5281              7.93 0.00734\n#&gt;  7 2019-03-14 22:43:00      160    5580              7.93 0.00734\n#&gt;  8 2020-09-15 18:01:00      160    5880              7.93 0.00734\n#&gt;  9 2020-09-16 14:44:00      160    6120              7.93 0.00734\n#&gt; 10 2015-11-21 20:27:00      150    3420              7.91 0.00787\n#&gt; # ℹ 2,249 more rows\n\n\nNow there are five points with probabilities less than 0.005, two at 155, and three above 300.\nFigure 6.8 shows an HDR boxplot of the data (other than the maximum), with those points with surprisal probabilities less than 0.005 highlighted in black.\n\noldfaithful |&gt;\n  filter(duration &gt; 1, duration &lt; 7000) |&gt;\n  gg_hdrboxplot(duration)\n\n\n\n\n\n\n\nFigure 6.8: HDR boxplot of the Old Faithful eruption durations, with the surprisal anomalies highlighted in black.\n\n\n\n\n\nThe same algorithm is easily applied in two dimensions. For example, we can consider the bivariate distribution of Duration and Waiting time, ignoring those observations that are greater than 2 hours in either dimension.\n\nof &lt;- oldfaithful |&gt;\n  select(duration, waiting) |&gt;\n  filter(duration &lt; 7200, waiting &lt; 7200)\nof |&gt;\n  mutate(\n    loo_scores = surprisals(of, loo = TRUE, probability = FALSE),\n    prob = surprisals(of, loo = TRUE)\n  ) |&gt;\n  filter(prob &lt; 0.005) |&gt;\n  arrange(prob, duration)\n\n#&gt; # A tibble: 10 × 4\n#&gt;    duration waiting loo_scores    prob\n#&gt;       &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1        1    5700      Inf   0      \n#&gt;  2      120    6060      Inf   0      \n#&gt;  3      170    3600       17.6 0.00137\n#&gt;  4      170    3840       17.3 0.00182\n#&gt;  5      150    3420       16.5 0.00228\n#&gt;  6       90    4740       16.5 0.00273\n#&gt;  7      160    6120       16.4 0.00319\n#&gt;  8      186    4320       16.3 0.00364\n#&gt;  9      300    5280       16.1 0.00410\n#&gt; 10      160    5880       16.1 0.00455\n\n\nNow, two anomalies are identified, with both of them having infinite LOO surprisals. We can visualize them in an HDR scatterplot, shown in Figure 6.9.\n\nof |&gt;\n  gg_hdrboxplot(duration, waiting, scatterplot = TRUE)\n\n\n\n\n\n\n\nFigure 6.9: HDR scatterplot of the Old Faithful eruption durations and waiting times, with the surprisal anomalies highlighted in black.\n\n\n\n\n\n\n\nMore examples\nLet’s apply the kde surprisals method to the six examples introduced in Section 4.1. For this purpose we will use a threshold of 0.001 for the surprisal probabilities.\n\ncricket_batting |&gt;\n  filter(Innings &gt; 20) |&gt;\n  mutate(prob = surprisals(Average, loo = TRUE)) |&gt;\n  filter(prob &lt; 0.001) |&gt;\n  select(Player, Average, prob)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   Player     Average  prob\n#&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 DG Bradman    99.9     0\n\n\nHere Bradman is a clear anomaly (with a very low surprisal probability), and no-one else is identified as a possible anomaly.\nNext we consider some artificial examples. First, we consider the first 48 rows of the second variable in the n01 data, along with the values 4.0 and 4.5.\n\nn01b &lt;- tibble(y = c(n01$v2[1:48], 4, 4.5))\nn01b |&gt;\n  mutate(prob = surprisals(y, loo = TRUE)) |&gt;\n  filter(prob &lt; 0.05) |&gt;\n  arrange(prob)\n\n#&gt; # A tibble: 2 × 2\n#&gt;       y    prob\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1   4.5 0.00984\n#&gt; 2   4   0.0160\n\n\nAs expected, only the two genuine anomalies have been identified.\nFinally, we consider 1000 simulated observations from each of the distributions, N(0,1), \\text{t}_3 and \\chi^2_4.\n\nn01 |&gt;\n  select(v1) |&gt;\n  mutate(prob = surprisals(v1, loo = TRUE)) |&gt;\n  filter(prob &lt; 0.001) |&gt;\n  arrange(prob, v1)\n\n#&gt; # A tibble: 1 × 2\n#&gt;      v1     prob\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1  3.81 0.000675\n\nset.seed(1)\ntibble(y = rt(1000, df = 3)) |&gt;\n  mutate(prob = surprisals(y, loo = TRUE)) |&gt;\n  filter(prob &lt; 0.001) |&gt;\n  arrange(prob, y)\n\n#&gt; # A tibble: 4 × 2\n#&gt;        y     prob\n#&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 -11.4  0       \n#&gt; 2  10.5  0       \n#&gt; 3   7.65 0.000637\n#&gt; 4  -7.40 0.000939\n\ntibble(y = rchisq(1000, df = 4)) |&gt;\n  mutate(prob = surprisals(y, loo = TRUE)) |&gt;\n  filter(prob &lt; 0.001) |&gt;\n  arrange(prob, y)\n\n#&gt; # A tibble: 1 × 2\n#&gt;       y     prob\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1  17.0 0.000483\n\n\nThe algorithm has found a small number of spurious anomalies in each case, out of the 1000 observations included. Notably, the results do not appear to deteriorate with the heavier-tailed or skewed distributions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "06-density.html#other-density-based-methods",
    "href": "06-density.html#other-density-based-methods",
    "title": "6  Density-based methods",
    "section": "6.6 Other density-based methods",
    "text": "6.6 Other density-based methods\n\n\n\n\nCover, Thomas M, and Joy A Thomas. 2006. Elements of Information Theory. 2nd ed. Hoboken, NJ: Wiley.\n\n\nFaraway, J J. 2014. Linear Models with r. 2nd ed. Chapman; Hall/CRC.\n\n\nGneiting, T, and M Katzfuss. 2014. “Probabilistic Forecasting.” Annual Review of Statistics and Its Application 1 (1): 125–51. https://doi.org/10.1146/annurev-statistics-062713-085831.\n\n\nKandanaarachchi, S, and R J Hyndman. 2022. “Leave-One-Out Kernel Density Estimates for Outlier Detection.” J Computational & Graphical Statistics 31 (2): 586–99. https://doi.org/10.1080/10618600.2021.2000425.\n\n\nMontgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2012. Introduction to Linear Regression Analysis. 5th ed. John Wiley & Sons.\n\n\nSeber, George A F, and Alan J Lee. 2003. Linear Regression Analysis. 2nd ed. John Wiley & Sons.\n\n\nStone, James V. 2022. Information Theory: A Tutorial Introduction. Sheffield, England: Sebtel Press.\n\n\nTribus, Myron. 1961. Thermodynamics and Thermostatics: An Introduction to Energy, Information and States of Matter, with Engineering Applications. New York, USA: D. Van Nostrand.\n\n\nWood, S N. 2017. Generalized Additive Models: An Introduction with R. CRC press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Density-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html",
    "href": "07-distance.html",
    "title": "7  Distance-based methods",
    "section": "",
    "text": "7.1 Multivariate pairwise distances\nDistance-based anomaly detection methods are based on the distances between observations. The pairwise distances between observations are computed, and anomalies are identified as those observations that are far from other observations.\nBefore we discuss some of the methods that fall into this class of algorithms, we will first discuss some of the different ways of measuring distances between observations.\nSuppose our observations are denoted by \\bm{y}_1,\\dots,\\bm{y}_n. In univariate data, the distance between any two observations is simply the absolute difference between the points. But in multivariate data, there are many different ways of measuring distances between observations, even when the data are all numerical.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#sec-distances",
    "href": "07-distance.html#sec-distances",
    "title": "7  Distance-based methods",
    "section": "",
    "text": "Euclidean distance\nPerhaps the most obvious way to define the distance between any two points \\bm{y}_i and \\bm{y}_j is to measure the length of the straight line between the points. This is commonly known as Euclidean distance, named after the famous Greek mathematician Euclid, and can be defined as \\|\\bm{y}_i - \\bm{y}_j\\|_2 = \\sqrt{\\sum_{k=1}^d (y_{ik}-y_{jk})^2}. This is also known as the L_2 distance. For d=1 or d=2, this is the physical distance between the points when plotted on a strip plot or a scatterplot (provided there is no jittering used).\nThe dist() function will return a distance matrix containing all pairwise distances between observations. The argument method specifies the type of distance to compute, with the default being Euclidean distance.\nHere is an example using only the first five observations of the old_faithful data set (omitting the time stamp). Because the distances are symmetric, only the lower triangle of the matrix is computed.\n\nhead(oldfaithful, 5)\n\n#&gt; # A tibble: 5 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-01-02 14:53:00      271    5040\n#&gt; 2 2015-01-09 23:55:00      247    6060\n#&gt; 3 2015-02-07 00:49:00      203    5460\n#&gt; 4 2015-02-14 01:09:00      195    5221\n#&gt; 5 2015-02-21 01:12:00      210    5401\n\nof &lt;- head(oldfaithful, 5) |&gt; select(-time)\nof |&gt; dist()\n\n#&gt;         1       2       3       4\n#&gt; 2 1020.28                        \n#&gt; 3  425.47  601.61                \n#&gt; 4  196.31  840.61  239.13        \n#&gt; 5  366.12  660.04   59.41  180.62\n\n\n\n\nManhattan distance\nThe Manhattan distance (or absolute distance) between two observations is given by \\|\\bm{y}_i - \\bm{y}_j\\|_1 = \\sum_{k=1}^d |y_{ik}-y_{jk}|. This is also known as the L_1 distance. It is called the Manhattan distance as it gives the shortest path between the corners of city blocks (denoted as points on a grid) when those blocks are rectangular, as they mostly are in Manhattan. For the same reason, it is also sometimes called the “taxicab” distance or the “city block” distance.\nLet’s compute the Manhattan distances for the same five observations from the oldfaithful data set.\n\nof |&gt; dist(method = \"manhattan\")\n\n#&gt;      1    2    3    4\n#&gt; 2 1044               \n#&gt; 3  488  644          \n#&gt; 4  257  891  247     \n#&gt; 5  422  696   66  195\n\n\n\n\nMinkowski distance\nThis generalizes the Manhattan and Euclidean distances to use powers of p to define the L_p distance: \\|\\bm{y}_i - \\bm{y}_j\\|_p = \\left(\\sum_{k=1}^d (y_{ik}-y_{jk})^p\\right)^{1/p}. It is named after the German mathematician, Hermann Minkowski. When p=1 this is the Manhattan distance, and when p=2 this is the Euclidean distance.\nHere is the Minkowsi distance with p = 3 for the same five observations from the oldfaithful data set.\n\nof |&gt; dist(method = \"minkowski\", p = 3)\n\n#&gt;         1       2       3       4\n#&gt; 2 1020.00                        \n#&gt; 3  420.59  600.08                \n#&gt; 4  185.36  839.07  239.00        \n#&gt; 5  361.58  659.04   59.03  180.03\n\n\n\n\nChebyshev Distance\nThe maximum distance between any components of \\bm{y}_i and \\bm{y}_j is known as the Chebyshev distance (after the Russian mathematician, Pafnuty Chebyshev), given by \\|\\bm{y}_i - \\bm{y}_j\\|_{\\text{max}} = \\max_k |y_{ik}-y_{jk}|.\n\nof |&gt; dist(method = \"maximum\")\n\n#&gt;      1    2    3    4\n#&gt; 2 1020               \n#&gt; 3  420  600          \n#&gt; 4  181  839  239     \n#&gt; 5  361  659   59  180\n\n\nThis is also known as chessboard distance, as it is the number of moves a king would have to travel on a chessboard to move between two squares. The Chebyshev distance is equivalent to the Minkowski distance with p=\\infty.\nNotice that all four of the distances introduced so far are equal to the absolute difference between observations when d=1. The next distance does not have this property.\n\n\nMahalanobis distance\nWhen the variables have different scales, the variables with the largest ranges will dominate the distance measures. In the Old Faithful example, durations are much longer than waiting times, and so the duration variable is dominating the calculation of distances. Consequently, it is often preferable to scale the data before computing distances.\nSuppose we scale the data using the robust multivariate approach described in Section 3.9, so that the scaled data are given by \\bm{z}_i = \\bm{U} (\\bm{y}_i - \\bm{m}), where \\bm{m} is the pointwise median, and \\bm{U}'\\bm{U} = \\bm{S}_{\\text{OGK}}^{-1} is the Cholesky decomposition of the inverse of \\bm{S}_{\\text{OGK}}, a robust estimate of the covariance matrix of the data. Then the Euclidean distance between \\bm{z}_i and \\bm{z}_j is given by \n  \\| \\bm{z}_i - \\bm{z}_j\\|_2 = \\sqrt{(\\bm{y}_i - \\bm{y}_j)' \\bm{U}' \\bm{U} (\\bm{y}_i - \\bm{y}_j)} =  \\sqrt{(\\bm{y}_i - \\bm{y}_j)' \\bm{S}_{\\text{OGK}}^{-1} (\\bm{y}_i - \\bm{y}_j)}\n This is known as the Mahalanobis distance, named after the Indian statistician Prasanta Chandra Mahalanobis (although he wasn’t using a robust measure of covariance). It is a multivariate generalization of the number of standard deviations between any two observations.\nMahalanobis distance is not an option provided by the dist() function, so we will need to compute the scaled data first.\n\nz &lt;- oldfaithful |&gt;\n  select(-time) |&gt;\n  mvscale()\nz |&gt;\n  head(5) |&gt;\n  dist()\n\n#&gt;        1      2      3      4\n#&gt; 2 2.6920                     \n#&gt; 3 3.5672 1.9719              \n#&gt; 4 3.6898 2.4090 0.4951       \n#&gt; 5 3.1821 1.7619 0.3861 0.6618\n\n\nFor more information about pairwise distances, see Borg and Groenen (2005).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#nearest-neighbours",
    "href": "07-distance.html#nearest-neighbours",
    "title": "7  Distance-based methods",
    "section": "7.2 Nearest neighbours",
    "text": "7.2 Nearest neighbours\nIf there are n observations, then there are n(n-1)/2 pairwise distances to compute, so this is an O(n^2) operation which can take a long time for large n.\nSome algorithms only compute the pairwise distances of the k nearest observations, although finding those observations requires some additional distances to be computed. For some types of distances, efficient solutions are available using kd trees (Bentley 1975; Arya et al. 1998) that find the k nearest neighbours to each observation in O(n\\log(n)) time.\nThe calculation of k nearest neighbours is useful for more than anomaly detection problems. It is also the basis of a popular classification method due to the Berkeley statisticians Evelyn Fix and Joe Hodges (Fix and Hodges 1989) which is often known as the “kNN algorithm”.\nSuppose we use the Old Faithful data to find eruptions that are neighbours in the (duration, waiting) space. The dbscan package uses kd trees to quickly identify the k nearest observations to each eruption. We will use the scaled data computed in the last section to find the 5 nearest neighbours to each eruption.\n\n# Find 5 nearest neighbours to each eruption\nknn &lt;- dbscan::kNN(z, k = 5)\n# First eruption in the data set\noldfaithful[1, ]\n\n#&gt; # A tibble: 1 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2015-01-02 14:53:00      271    5040\n\n# Five closest observations\noldfaithful[knn$id[1, ], ]\n\n#&gt; # A tibble: 5 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2021-04-13 21:48:00      270    5040\n#&gt; 2 2018-09-15 19:32:00      268    5040\n#&gt; 3 2018-08-08 00:47:00      272    5160\n#&gt; 4 2016-01-28 20:36:00      270    5220\n#&gt; 5 2018-09-02 23:25:00      275    5340\n\n\nFor large data sets, approximations are available which speed up the computation even more, but are less accurate in finding the k nearest neighbours. The approx argument specifies a distance tolerance which makes the process faster for large data sets, although the neighbours returned may not be the exact nearest neighbours.\n\n# Find 5 approximate nearest neighbours to each eruption\nkann &lt;- dbscan::kNN(z, k = 5, approx = 2)\n# Five closest observations\noldfaithful[kann$id[1, ], ]\n\n#&gt; # A tibble: 5 × 3\n#&gt;   time                duration waiting\n#&gt;   &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 2021-04-13 21:48:00      270    5040\n#&gt; 2 2018-09-15 19:32:00      268    5040\n#&gt; 3 2018-08-08 00:47:00      272    5160\n#&gt; 4 2016-01-28 20:36:00      270    5220\n#&gt; 5 2018-09-09 20:33:00      266    5280\n\n\nIn this case, the same five observations have been found.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#local-outlier-factors",
    "href": "07-distance.html#local-outlier-factors",
    "title": "7  Distance-based methods",
    "section": "7.3 Local outlier factors",
    "text": "7.3 Local outlier factors\nA popular way of using k-nearest neighbours for anomaly detection is via local outlier factors (Breunig et al. 2000). This is similar to the idea discussed in Section 6.5 of finding points with low probability density estimates, in that it is designed to find points in areas with few surrounding observations.\nSuppose we write the distance between observations \\bm{y}_i and \\bm{y}_j as \\|\\bm{y}_i - \\bm{y}_j\\|, and let d_{i,k} be the distance between observation \\bm{y}_i and its kth nearest neighbour.\nLet N_{i,k} be the set of k nearest neighbours within d_{i,k} of \\bm{y}_i. (If there are multiple observations all exactly d_{i,k} from \\bm{y}_i, then N_{i,k} may contain more than k observations, but we will ignore that issue here.)\nNote that an observation \\bm{y}_j may be within N_{i,k} while \\bm{y}_i does not fall within N_{j,k}, as shown in the diagram below.\n\n\n\n\n\n\n\n\nFigure 7.1: A synthetic data set of 11 observations. Nearest neighbourhoods containing five observations each are shown for \\bm{y}_1 (in orange) and \\bm{y}_2 (in blue). The neighbourhood of \\bm{y}_1 contains \\bm{y}_2 in its nearest five observations, but \\bm{y}_2 does not include \\bm{y}_1 in its nearest five observations.\n\n\n\n\n\nLet r_k(i,j) = \\max(d_{j,k}, \\|\\bm{y}_i-\\bm{y}_j\\|) be the “reachability” of \\bm{y}_i from \\bm{y}_j. Thus, r_k(i,j) is the distance between observations \\bm{y}_i and \\bm{y}_j when they are far from each other, but is equal to d_{j,k} if \\bm{y}_i is one of the k nearest neighbours of \\bm{y}_j. The reachability is a truncated variant of the usual Euclidean distance \\|\\bm{y}_i-\\bm{y}_j\\| so that it is not less than d_{j,k}.\nThe average reachability of \\bm{y}_i from its nearest neighbours is given by \n  \\bar{r}_{i,k} = k^{-1} \\sum_{\\bm{y}_j \\in N_{i,k}} r_k(i,j)\n This is not the same as the average reachability of the neighbours from \\bm{y}_i which, by definition, would be d_{i,k}. An observation will have high average reachability if it is far from its neighbours, whereas it will have low average reachability if it has many close neighbours.\nThen the local outlier factor for observation \\bm{y}_i is given by \n\\ell_{i,k} = \\frac{\\bar{r}_{i,k}}{k}\\sum_{\\bm{y}_j \\in N_{i,k}} \\bar{r}^{-1}_{j,k}.\n Thus, it is the ratio between \\bar{r}_{i,k} and the average value of \\bar{r}^{-1}_{j,k} for points within its neighbourhood.\nThis provides a relative measure of the density of each observation compared to its neighbours. An observation is regarded as an anomaly if it is in a low-density region (far from its neighbours) while its neighbours are in higher density regions (with many neighbours nearby). A value of \\ell_{i,k} much greater than 1 shows that the observation has much larger average reachability compared to its neighbours, and so is regarded as an anomaly.\nOne problem with this definition is that \\ell_{i,k} can be infinite. If the data set contains at least k+1 identical observations, then the average reachability \\bar{r}_k(j,k) will be 0 if \\bm{y}_j is one of the group of identical observations. Consequently, \\ell_{i,k} = \\infty if \\bm{y}_i is a neighbour to one of the group of identical observations. In this book we use a slightly different definition from that used elsewhere and replace these infinite values with zeros.\nAnother problem is that it can be difficult to determine an appropriate value for k.\n\nExample: Old Faithful data\n\n\nCode\nof_scores &lt;- oldfaithful |&gt;\n  mutate(lof = lof_scores(duration, k = 150))\nof_scores |&gt; arrange(desc(lof))\n\n\n#&gt; # A tibble: 2,261 × 4\n#&gt;    time                duration waiting    lof\n#&gt;    &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 2015-12-07 00:09:00     7200    3420 752.  \n#&gt;  2 2018-04-25 19:08:00        1    5700   8.65\n#&gt;  3 2020-10-25 16:31:00      305    6060   3.26\n#&gt;  4 2020-05-07 22:55:00      304    6180   3.17\n#&gt;  5 2020-05-21 00:27:00      304    5640   3.17\n#&gt;  6 2019-03-14 19:28:00      302    5820   2.99\n#&gt;  7 2018-09-14 20:35:00      301    5640   2.90\n#&gt;  8 2019-07-25 06:32:00      300    5280   2.81\n#&gt;  9 2019-09-17 17:22:00      300    6060   2.81\n#&gt; 10 2020-05-07 20:02:00      299    5940   2.72\n#&gt; # ℹ 2,251 more rows\n\n\nCode\nof_scores |&gt;\n  filter(duration &lt; 7000) |&gt;\n  ggplot(aes(x = duration, y = lof)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFigure 7.2: TBC\n\n\n\n\n\nAgain, the two large scores correspond to the extreme 2 hour duration and the tiny 1 second duration. The value of k=150 has been chosen after experimenting with various values.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#stahel-donoho-outlyingness",
    "href": "07-distance.html#stahel-donoho-outlyingness",
    "title": "7  Distance-based methods",
    "section": "7.4 Stahel-Donoho outlyingness",
    "text": "7.4 Stahel-Donoho outlyingness\nThe Stahel-Donoho outlyingness is a robust measure of outlyingness\nStahel (1981), Donoho (1982), Brys, Hubert, and Rousseeuw (2005)\nrobustbase::adjOutlyingness",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#covmcd",
    "href": "07-distance.html#covmcd",
    "title": "7  Distance-based methods",
    "section": "7.5 CovMCD",
    "text": "7.5 CovMCD\nRousseeuw and Van Driessen (1999)\nrobustbase::covMcd",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#detect-deviating-cells-algorithm",
    "href": "07-distance.html#detect-deviating-cells-algorithm",
    "title": "7  Distance-based methods",
    "section": "7.6 Detect deviating cells algorithm",
    "text": "7.6 Detect deviating cells algorithm\nRousseeuw and Van den Bossche (2016)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#stray-and-hdoutliers-algorithms",
    "href": "07-distance.html#stray-and-hdoutliers-algorithms",
    "title": "7  Distance-based methods",
    "section": "7.7 Stray and HDoutliers algorithms",
    "text": "7.7 Stray and HDoutliers algorithms\n\n\nCode\nof_scores &lt;- of_scores |&gt;\n  mutate(stray = stray_scores(duration))\nof_scores |&gt; arrange(desc(stray))\n\n\n#&gt; # A tibble: 2,261 × 5\n#&gt;    time                duration waiting    lof    stray\n#&gt;    &lt;dttm&gt;                 &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 2015-12-07 00:09:00     7200    3420 752.   0.958   \n#&gt;  2 2018-04-25 19:08:00        1    5700   8.65 0.0124  \n#&gt;  3 2017-05-03 06:19:00       90    4740   1.79 0.00139 \n#&gt;  4 2015-11-21 20:27:00      150    3420   1.78 0.00125 \n#&gt;  5 2016-03-22 14:06:00      150    4860   1.78 0.00125 \n#&gt;  6 2016-07-13 21:55:00      150    4980   1.78 0.00125 \n#&gt;  7 2020-06-18 02:00:00       92    3600   1.67 0.00111 \n#&gt;  8 2021-08-14 21:38:06       92    3882   1.67 0.00111 \n#&gt;  9 2020-09-25 20:00:00       93    3540   1.61 0.000972\n#&gt; 10 2021-07-08 00:44:12      173   47628   1.63 0.000972\n#&gt; # ℹ 2,251 more rows\n\n\nCode\nof_scores |&gt;\n  filter(duration &lt; 7000) |&gt;\n  ggplot(aes(x = duration, y = stray)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFigure 7.3\n\n\n\n\n\nTalagala, Hyndman, and Smith-Miles (2019), Wilkinson (2017)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#bacon-algorithm",
    "href": "07-distance.html#bacon-algorithm",
    "title": "7  Distance-based methods",
    "section": "7.8 Bacon algorithm",
    "text": "7.8 Bacon algorithm\nblocked adaptive computationally efficient outlier nominators\nBillor, Hadi, and Velleman (2000)\nrobustX::mvBACON",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#isolation-forest",
    "href": "07-distance.html#isolation-forest",
    "title": "7  Distance-based methods",
    "section": "7.9 Isolation forest",
    "text": "7.9 Isolation forest\nIsolation tree: Randomly partition data until every point is isolated or some depth is reached.\nCombine many isolation trees to form an isolation forest.\nAnomaly score inversely proportional to average depth of isolation trees.\nisotree package\nLiu, Ting, and Zhou (2008)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "07-distance.html#clustering-methods",
    "href": "07-distance.html#clustering-methods",
    "title": "7  Distance-based methods",
    "section": "7.10 Clustering methods",
    "text": "7.10 Clustering methods\ndbscan package\n\nCalled “density-based”, but “density” here does not mean probability density, as in ch6. Instead, it refers to the relative concentration of points in a neighbourhood.\n\n\n\n\n\n\nArya, S, D M Mount, N S Netanyahu, R Silverman, and A Y Wu. 1998. “An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions.” Journal of the ACM 45 (6): 891–923. https://doi.org/10.1145/293347.293348.\n\n\nBentley, J L. 1975. “Multidimensional Binary Search Trees Used for Associative Searching.” Communications of the ACM 18 (9): 509–17. https://doi.org/10.1145/361002.361007.\n\n\nBillor, N, A S Hadi, and P F Velleman. 2000. “BACON: Blocked Adaptive Computationally Efficient Outlier Nominators.” Computational Statistics & Data Analysis 34 (3): 279–98. https://doi.org/10.1016/S0167-9473(99)00101-2.\n\n\nBorg, I, and P J F Groenen. 2005. Modern Multidimensional Scaling: Theory and Applications. 2nd ed. Springer Texts in Statistics. New York: Springer. https://doi.org/10.1007/0-387-28981-X.\n\n\nBreunig, M M, H-P Kriegel, R T Ng, and J Sander. 2000. “LOF: Identifying Density-Based Local Outliers.” ACM SIGMOD Record 29 (2): 93–104. https://doi.org/10.1145/335191.335388.\n\n\nBrys, G, M Hubert, and P J Rousseeuw. 2005. “A Robustification of Independent Component Analysis.” Journal of Chemometrics 19 (5-7): 364–75. https://doi.org/10.1002/cem.940.\n\n\nDonoho, D L. 1982. “Breakdown Properties of Multivariate Location Estimators.”\n\n\nFix, E, and J L Hodges. 1989. “Discriminatory Analysis — Nonparametric Discrimination: Consistency Properties.” International Statistical Review 57 (3): 238–47.\n\n\nLiu, F T, K M Ting, and Z-H Zhou. 2008. “Isolation Forest.” In 2008 Eighth IEEE International Conference on Data Mining, 413–22. IEEE. https://doi.org/10.1109/ICDM.2008.17.\n\n\nRousseeuw, P J, and W Van den Bossche. 2016. “Detecting Deviating Data Cells,” January. http://arxiv.org/abs/1601.07251.\n\n\nRousseeuw, P J, and K Van Driessen. 1999. “A Fast Algorithm for the Minimum Covariance Determinant Estimator.” Technometrics: A Journal of Statistics for the Physical, Chemical, and Engineering Sciences 41 (3): 212–23. https://doi.org/10.1080/00401706.1999.10485670.\n\n\nStahel, W A. 1981. “Robust Estimation: Infinitisimal Optimality and Covariance Matrix Estimators.” PhD, ETH Zurich.\n\n\nTalagala, P D, R J Hyndman, and K Smith-Miles. 2019. “Anomaly Detection in High Dimensional Data.” arXiv Preprint arXiv. https://arxiv.org/abs/1908.04000.\n\n\nWilkinson, L. 2017. “Visualizing Big Data Outliers Through Distributed Aggregation.” IEEE Transactions on Visualization and Computer Graphics, August. https://doi.org/10.1109/TVCG.2017.2744685.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Distance-based methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html",
    "href": "08-highdim.html",
    "title": "8  High-dimensional methods",
    "section": "",
    "text": "8.1 Principal component analysis",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html#multi-dimensional-scaling",
    "href": "08-highdim.html#multi-dimensional-scaling",
    "title": "8  High-dimensional methods",
    "section": "8.2 Multi-dimensional scaling",
    "text": "8.2 Multi-dimensional scaling",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html#pcout",
    "href": "08-highdim.html#pcout",
    "title": "8  High-dimensional methods",
    "section": "8.3 PCout",
    "text": "8.3 PCout\nP. Filzmoser, Maronna, and Werner (2008), Peter Filzmoser and Todorov (2013)\n\nmvoutlier::pcout",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html#fastpcs",
    "href": "08-highdim.html#fastpcs",
    "title": "8  High-dimensional methods",
    "section": "8.4 FastPCS",
    "text": "8.4 FastPCS\nVakili and Schmitt (2014)\n\nFastPCS::FastPCS",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html#dobin",
    "href": "08-highdim.html#dobin",
    "title": "8  High-dimensional methods",
    "section": "8.5 DOBIN",
    "text": "8.5 DOBIN\nKandanaarachchi and Hyndman (2020)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "08-highdim.html#comparisons-using-the-o3-plot",
    "href": "08-highdim.html#comparisons-using-the-o3-plot",
    "title": "8  High-dimensional methods",
    "section": "8.6 Comparisons using the O3 plot",
    "text": "8.6 Comparisons using the O3 plot\n\n\n\n\nFilzmoser, Peter, and Valentin Todorov. 2013. “Robust Tools for the Imperfect World.” Information Sciences 245 (October): 4–20. https://doi.org/10.1016/j.ins.2012.10.017.\n\n\nFilzmoser, P, R Maronna, and M Werner. 2008. “Outlier Identification in High Dimensions.” Computational Statistics & Data Analysis 52 (3): 1694–1711. https://doi.org/10.1016/j.csda.2007.05.018.\n\n\nKandanaarachchi, S, and R J Hyndman. 2020. “Dimension Reduction for Outlier Detection Using DOBIN.” Journal of Computational & Graphical Statistics 30 (1): 204–19. https://doi.org/10.1080/10618600.2020.1807353.\n\n\nVakili, K, and E Schmitt. 2014. “Finding Multivariate Outliers with FastPCS.” Computational Statistics & Data Analysis 69 (January): 54–66. https://doi.org/10.1016/j.csda.2013.07.021.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>High-dimensional methods</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html",
    "href": "09-timeseries.html",
    "title": "9  Time series data",
    "section": "",
    "text": "9.1 Time series anomaly detection paradigms\nFor this chapter, we will load some additional packages designed to work with time series data. To learn more about these packages and their use in time series analysis, see Hyndman and Athanasopoulos (2021).\nTime series data are observations that are collected over time. In this book, we will only consider time series that are observed at regular intervals, such as annually, monthly, or hourly.\nWhen considering time series data, we can distinguish between three different anomaly detection paradigms:\nThese can be illustrated using the following examples.\nFigure 9.1: The three main anomaly detection paradigms. In the top panel, we aim to identify unusual observations within historical data. In the middle panel, we aim to identify an unusual time series within a collection of time series. In the bottom panel, we aim to identify an unusual observation in the next time period.\nWeird times: In the top plot, we are looking for historical anomalies within time series data. This is common in quality control, or in data cleaning. The aim is to find time periods where the observations are different from the rest of the data. Again, it can easily be extended to multivariate time series, where we look for time periods when one or more of the series may display unusual observations.\nWeird series: In the middle plot, we are looking for an anomalous time series within a collection of time series. The observations within each series may all be consistent, but the series as a whole may be unusual. This is, by its nature, a multivariate problem. It is common in finance, where we look for unusual behaviour of a stock within a collection of stocks, or in health, where we look for unusual behaviour of a patient within a collection of patients.\nSurveillance: In the bottom plot, we are looking for an anomaly in the next observation in the time series. This is commonly done in surveillance, when a time is monitored in real time to identify an unusual observation, and appropriate action taken. An anomaly in this context, is an observation that is very different from what was forecast. Although only a single time series is shown in this plot, the idea easily extends to multivariate time series, where we look for something unusual occurring in the next time period across a set of time series.\nWe will discuss each of these paradigms in turn.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html#time-series-anomaly-detection-paradigms",
    "href": "09-timeseries.html#time-series-anomaly-detection-paradigms",
    "title": "9  Time series data",
    "section": "",
    "text": "Weird times: Identifying anomalies within a time series in historical data.\nWeird series: Identifying an anomalous time series within a collection of time series.\nSurveillance: Identifying anomalies within a time series in real time",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html#weird-times",
    "href": "09-timeseries.html#weird-times",
    "title": "9  Time series data",
    "section": "9.2 Weird times",
    "text": "9.2 Weird times\nAs a vehicle of illustration, we will consider French mortality rates, disaggregated by age and sex. These are obtained from the Human Mortality Database (2024). We will consider male and female data from 1816 to 1999 over ages 0 to 85, giving 172 separate time series, each of length 184. In fact, the top plot in Figure 9.1 shows the male mortality rates for 25 year olds over this time period.\n\nfr_mortality\n\n#&gt; # A tibble: 31,648 × 4\n#&gt;     Year   Age Sex    Mortality\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n#&gt;  1  1816     0 Female   0.187  \n#&gt;  2  1816     1 Female   0.0467 \n#&gt;  3  1816     2 Female   0.0339 \n#&gt;  4  1816     3 Female   0.0229 \n#&gt;  5  1816     4 Female   0.0160 \n#&gt;  6  1816     5 Female   0.0138 \n#&gt;  7  1816     6 Female   0.0121 \n#&gt;  8  1816     7 Female   0.0104 \n#&gt;  9  1816     8 Female   0.00891\n#&gt; 10  1816     9 Female   0.00760\n#&gt; # ℹ 31,638 more rows\n\n\nFigure 9.2 shows the data for both sexes and all ages. The mortality rates have improved over time for all ages, especially since 1950. Infant mortality (age 0) is much higher than the rest of childhood, with mortality rates at a minimum at about age 10 in all years. The highest mortality rates are for the oldest age groups. The effect of the two wars are particularly evident in the male mortality rates.\n\nfr_mortality |&gt;\n  ggplot(aes(x = Year, y = Mortality, color = Age, group = Age)) +\n  geom_line() +\n  facet_grid(. ~ Sex) +\n  scale_y_log10()\n\n\n\n\n\n\n\nFigure 9.2: French mortality rates by sex and age from 1816 to 1999. We use a log scale because the rates are vastly different for different age groups.\n\n\n\n\n\nAnomaly detection methods for identifying weird time anomalies are usually based on fitting a smooth function m_t through the observations, estimating the spread in the series s_t, and computing the standardized residuals e_t = (y_t - m_t)/s_t. We then identify anomalies in the e_t series using one of the methods discussed in Chapter 6 or Chapter 7. To illustrate, we will consider two such approaches, but many more are possible depending on how m_t and s_t are estimated, and how anomalies are identified in e_t.\n\nHampel identifier\nThe Hampel identifier is due to a proposal of Frank Hampel (Davies and Gather 1993), and is designed to identify anomalies in a time series based on whether an observation is very different from the neighbouring observations. Suppose we define neighbourhoods comprising observations within h time periods. Then the local median is given by \n\\hat{m}_t = \\text{median}(y_{t-h}, \\dots, y_{t+h}).\n This is also called a “moving median”, a “running median” or a “rolling median”. Similarly, the local median absolute deviation (MAD) is given by \n\\hat{s}_t = \\text{median}(|y_{t-h}-\\hat{m}_t|, \\dots, |y_{t+h}- \\hat{m}_t|).\n This measures the variation of the series in the neighbourhood of time t. The residuals e_t = (y_t-\\hat{m}_t)/s_t provide a measure for how different an observation y_t is from its neighbours. The Hampel identifier denotes an observation as an anomaly if |e_t| &gt; \\tau for some threshold \\tau. A related idea is the Hampel filter, where each anomaly is replaced by the local median \\hat{m}_t.\nThe threshold is often set under the assumption of a Normal distribution, for which the MAD is equal to 0.67445 times the standard deviation. So it is common to set \\tau = k/0.67445 where k is the number of standard deviations permitted before an observation is considered an outlier. Because we apply the rule repeatedly over the length of a time series, we need to be careful of the problem of multiple comparisons, where the probability of a false positive overall is much larger than the probability of a false positive for each individual test. However, it is not straightforward to adjust the probabilities because the tests are not independent, and the dependency between them depends on the window size h, and the characteristics of the time series. Setting k=5 or 6 seems to work quite well for many problems.\nThe hampel_anomalies() function implements this algorithm. The code below applies the algorithm to all 172 time series in fr_mortality.\n\nfr_anomalies &lt;- fr_mortality |&gt;\n  group_by(Age, Sex) |&gt;\n  mutate(\n    hampel = hampel_anomalies(Mortality, bandwidth = 7, k = 6)\n  ) |&gt;\n  ungroup() |&gt;\n  filter(hampel) |&gt;\n  arrange(Year, Age)\n\n\n\nCode\n# Find years containing anomalies\nyrs &lt;- fr_anomalies |&gt;\n  select(Year, Sex) |&gt;\n  distinct()\nfr_anomalies_plot_male2 &lt;- fr_anomalies |&gt;\n  filter(Sex == \"Male\") |&gt;\n  ggplot(aes(x = Year, y = Age)) +\n  facet_grid(. ~ Sex) +\n  scale_x_continuous(\n    breaks = seq(1820, 2000, by = 20),\n    limits = range(yrs$Year)\n  ) +\n  geom_vline(\n    xintercept = unique(yrs$Year[yrs$Sex == \"Male\"]),\n    alpha = 0.5, color = \"grey\"\n  ) +\n  geom_point(col = \"#478cb2\") +\n  ggrepel::geom_text_repel(\n    data = yrs |&gt;\n      filter(Sex == \"Male\", !Year %in% 1915:1918),\n    aes(y = 75, label = Year), col = \"#478cb2\", size = 3, seed = 1967\n  ) +\n  ylim(4, 85)\nfr_anomalies_plot_female2 &lt;- fr_anomalies |&gt;\n  filter(Sex == \"Female\") |&gt;\n  ggplot(aes(x = Year, y = Age)) +\n  facet_grid(. ~ Sex) +\n  scale_x_continuous(\n    breaks = seq(1820, 2000, by = 20),\n    limits = range(yrs$Year)\n  ) +\n  geom_vline(\n    xintercept = unique(yrs$Year[yrs$Sex == \"Female\"]),\n    alpha = 0.5, color = \"grey\"\n  ) +\n  labs(title = \"French mortality anomalies\") +\n  geom_point(col = \"#c1653a\") +\n  ggrepel::geom_text_repel(\n    data = yrs[yrs$Sex == \"Female\", ],\n    aes(y = 75, label = Year), col = \"#c1653a\", size = 3, seed = 1967\n  ) +\n  ylim(4, 85)\npatchwork::wrap_plots(\n  fr_anomalies_plot_female2,\n  fr_anomalies_plot_male2,\n  nrow = 1\n)\n\n\n\n\n\n\n\n\nFigure 9.3: French mortality observations identified as anomalies using the Hampel identifier.\n\n\n\n\n\nFigure 9.3 shows the observations that have been identified as anomalies, showing the wars and epidemics that have occurred over time. The following events in French history are evident in the data:\n\n1832, 1849, 1854: Cholera outbreaks\n1853-1856: Crimean war\n1870: Franco-Prussian war\n1871: Repression of the ‘Commune de Paris’\n1914-1918: World War I\n1918: Spanish flu outbreak\n1940-1944: World War II\n\n\n\nSurprisal anomalies\nThe Hampel identifier worked well for the annual mortality series, but the local median approach will break down when there is seasonality or other systematic patterns in the data. However, the same idea can be adapted by replacing the local median with a more sophisticated way of modelling the signal in the data. One such method is STL (Seasonal Trend decomposition using Loess) due to Cleveland et al. (1990), and extended by Bandara, Hyndman, and Bergmeir (2022) to handle multiple seasonal periods or series with no seasonality. In the latter case, Friedman’s SuperSmoother (Friedman 1984) is used to estimate the signal m_t. This uses local linear regressions rather than local medians, with the window width chosen by cross-validation. Other estimates of m_t are also possible, provided they are robust to outliers.\n\nfr_fit &lt;- fr_mortality |&gt;\n  as_tsibble(index = Year, key = c(Age, Sex)) |&gt;\n  model(stl = STL(Mortality)) |&gt;\n  augment() |&gt;\n  as_tibble() |&gt;\n  select(-.model, -.innov)\nfr_fit\n\n#&gt; # A tibble: 31,648 × 6\n#&gt;      Age Sex     Year Mortality .fitted   .resid\n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     0 Female  1816     0.187   0.192 -0.00503\n#&gt;  2     0 Female  1817     0.182   0.191 -0.00958\n#&gt;  3     0 Female  1818     0.186   0.191 -0.00488\n#&gt;  4     0 Female  1819     0.197   0.190  0.00691\n#&gt;  5     0 Female  1820     0.181   0.189 -0.00826\n#&gt;  6     0 Female  1821     0.182   0.188 -0.00662\n#&gt;  7     0 Female  1822     0.207   0.188  0.0196 \n#&gt;  8     0 Female  1823     0.192   0.187  0.00521\n#&gt;  9     0 Female  1824     0.199   0.186  0.0124 \n#&gt; 10     0 Female  1825     0.194   0.185  0.00909\n#&gt; # ℹ 31,638 more rows\n\n\nThe .fitted column contains the estimates of m_t, while the .resid column contains the differences y_t - \\hat{m}_t. We will estimate s_t using a robust estimate of standard deviation across each series (rather than locally, as was done with the Hampel method). This time, we will use IQR, rather than MAD, to estimate the spread, with a similar adjustment so that it is equivalent to a standard deviation if they data were normally distributed.\n\nfr_sd &lt;- fr_fit |&gt;\n  group_by(Age, Sex) |&gt;\n  summarise(s = IQR(.resid) / 1.349, .groups = \"drop\")\nfr_scores &lt;- fr_fit |&gt;\n  left_join(fr_sd, by = c(\"Age\", \"Sex\")) |&gt;\n  mutate(e = .resid / s)\nfr_scores\n\n#&gt; # A tibble: 31,648 × 8\n#&gt;      Age Sex     Year Mortality .fitted   .resid       s      e\n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1     0 Female  1816     0.187   0.192 -0.00503 0.00786 -0.640\n#&gt;  2     0 Female  1817     0.182   0.191 -0.00958 0.00786 -1.22 \n#&gt;  3     0 Female  1818     0.186   0.191 -0.00488 0.00786 -0.621\n#&gt;  4     0 Female  1819     0.197   0.190  0.00691 0.00786  0.879\n#&gt;  5     0 Female  1820     0.181   0.189 -0.00826 0.00786 -1.05 \n#&gt;  6     0 Female  1821     0.182   0.188 -0.00662 0.00786 -0.843\n#&gt;  7     0 Female  1822     0.207   0.188  0.0196  0.00786  2.49 \n#&gt;  8     0 Female  1823     0.192   0.187  0.00521 0.00786  0.663\n#&gt;  9     0 Female  1824     0.199   0.186  0.0124  0.00786  1.58 \n#&gt; 10     0 Female  1825     0.194   0.185  0.00909 0.00786  1.16 \n#&gt; # ℹ 31,638 more rows\n\n\nAt this point, we could identify a point as an anomaly if it is more than k standard deviations away from m_t, as was done with the Hampel identifier. This is also the approach taken by the tsoutliers function from the forecast package, which uses 3 IQR as the threshold, equivalent to k=4.05 standard deviations. But we will use density scores instead, and compute the corresponding surprisal probabilities assuming the residuals follow a Normal distribution.\n\nfr_scores &lt;- fr_scores |&gt;\n  mutate(\n    s = -dnorm(e, log = TRUE),\n    prob = surprisals(s)\n  )\nfr_scores |&gt; arrange(prob)\n\n#&gt; # A tibble: 31,648 × 9\n#&gt;      Age Sex     Year Mortality .fitted   .resid     s      e     prob\n#&gt;    &lt;int&gt; &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     0 Female  1816     0.187   0.192 -0.00503  1.12 -0.640 0.000100\n#&gt;  2     0 Female  1817     0.182   0.191 -0.00958  1.66 -1.22  0.000100\n#&gt;  3     0 Female  1818     0.186   0.191 -0.00488  1.11 -0.621 0.000100\n#&gt;  4     0 Female  1819     0.197   0.190  0.00691  1.31  0.879 0.000100\n#&gt;  5     0 Female  1820     0.181   0.189 -0.00826  1.47 -1.05  0.000100\n#&gt;  6     0 Female  1821     0.182   0.188 -0.00662  1.27 -0.843 0.000100\n#&gt;  7     0 Female  1822     0.207   0.188  0.0196   4.02  2.49  0.000100\n#&gt;  8     0 Female  1823     0.192   0.187  0.00521  1.14  0.663 0.000100\n#&gt;  9     0 Female  1824     0.199   0.186  0.0124   2.17  1.58  0.000100\n#&gt; 10     0 Female  1825     0.194   0.185  0.00909  1.59  1.16  0.000100\n#&gt; # ℹ 31,638 more rows\n\n\nThe most extreme anomalies all correspond to male deaths due to World War I. A plot similar to Figure 9.3 can be made, showing points with lookout probability less than 0.05. Fewer anomalies have been identified than with the Hampel identifier. Because the lookout probabilities were based on a Generalized Pareto Distribution with a threshold of 90%, and we are identifying anomalies with probability less than 0.05, the overall probability of a false positive is 0.005 per series, so it is unlikely that we have identified any false positives, and likely that we have missed some anomalies.\n\n\nCode\nfr_anomalies &lt;- fr_scores |&gt;\n  filter(prob &lt; 0.05) |&gt;\n  as_tibble() |&gt;\n  select(Year, Sex, Age) |&gt;\n  distinct() |&gt;\n  left_join(fr_mortality, by = c(\"Age\", \"Sex\", \"Year\"))\nyrs &lt;- fr_anomalies |&gt;\n  select(Year, Sex) |&gt;\n  distinct()\nfr_anomalies_plot_male2 &lt;- fr_anomalies |&gt;\n  filter(Sex == \"Male\") |&gt;\n  ggplot(aes(x = Year, y = Age)) +\n  facet_grid(. ~ Sex) +\n  scale_x_continuous(\n    breaks = seq(1820, 2000, by = 20),\n    limits = range(yrs$Year)\n  ) +\n  geom_vline(\n    xintercept = unique(yrs$Year[yrs$Sex == \"Male\"]),\n    alpha = 0.5, color = \"grey\"\n  ) +\n  geom_point(col = \"#478cb2\") +\n  ggrepel::geom_text_repel(\n    data = yrs[yrs$Sex == \"Male\", ],\n    aes(y = 75, label = Year), col = \"#478cb2\", size = 3, seed = 1967\n  ) +\n  ylim(4, 85)\nfr_anomalies_plot_female2 &lt;- fr_anomalies |&gt;\n  filter(Sex == \"Female\") |&gt;\n  ggplot(aes(x = Year, y = Age)) +\n  facet_grid(. ~ Sex) +\n  scale_x_continuous(\n    breaks = seq(1820, 2000, by = 20),\n    limits = range(yrs$Year)\n  ) +\n  geom_vline(\n    xintercept = unique(yrs$Year[yrs$Sex == \"Female\"]),\n    alpha = 0.5, color = \"grey\"\n  ) +\n  labs(title = \"French mortality anomalies\") +\n  geom_point(col = \"#c1653a\") +\n  ggrepel::geom_text_repel(\n    data = yrs[yrs$Sex == \"Female\", ],\n    aes(y = 75, label = Year), col = \"#c1653a\", size = 3, seed = 1967\n  ) +\n  ylim(4, 85)\npatchwork::wrap_plots(\n  fr_anomalies_plot_female2,\n  fr_anomalies_plot_male2,\n  nrow = 1\n)\n\n\n\n\n\n\n\n\nFigure 9.4: French mortality observations identified as anomalies using density scores obtained from applying Friedman’s supersmoother to each series.\n\n\n\n\n\n\n\nMultivariate time series\nA similar can be applied to the whole collection of time series, where we are looking for years that are anomalous across all ages and both sexes. One way to do this is to first take principal components of the series, and then apply the preceding procedure to the series of first principal component scores.\nFirst we convert the series to logs as they are on vastly different scales, then we transform the data to wide form. Finally, we compute the first principal component of the resulting matrix.\n\n\nCode\n# Convert to wide form\nfr_wide &lt;- fr_mortality |&gt;\n  mutate(logm = log(Mortality)) |&gt;\n  mutate(series = paste0(substr(Sex, 1, 1), \":\", Age)) |&gt;\n  select(-Age, -Sex, -Mortality) |&gt;\n  tidyr::pivot_wider(names_from = series, values_from = logm)\n# Compute 1 principal component from mortality rates\npcs &lt;- fr_wide |&gt;\n  select(-Year) |&gt;\n  prcomp(rank = 1) |&gt;\n  broom::augment(fr_wide |&gt; select(Year))\n# Plot principal component scores\npcs |&gt;\n  ggplot(aes(x = Year, y = .fittedPC1)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\nNow we can apply an STL model to find anomalies in this series.\n\n\nCode\npcs |&gt;\n  as_tsibble(index = Year) |&gt;\n  model(stl = STL(.fittedPC1)) |&gt;\n  augment() |&gt;\n  select(-.model, -.innov) |&gt;\n  mutate(e = .resid / IQR(.resid) * 1.349) |&gt;\n  filter(abs(e) &gt; 3)\n\n\n#&gt; # A tsibble: 12 x 5 [1Y]\n#&gt;     Year .fittedPC1 .fitted .resid      e\n#&gt;    &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  1832    -11.9    -9.72   -2.16  -3.40\n#&gt;  2  1849    -11.8    -9.36   -2.46  -3.89\n#&gt;  3  1854    -11.7    -9.18   -2.52  -3.98\n#&gt;  4  1870    -11.1    -8.45   -2.64  -4.17\n#&gt;  5  1871    -14.6    -8.39   -6.21  -9.80\n#&gt;  6  1915     -7.88   -5.18   -2.70  -4.26\n#&gt;  7  1918    -11.4    -4.80   -6.57 -10.4 \n#&gt;  8  1940     -2.26    0.808  -3.06  -4.83\n#&gt;  9  1943     -2.49    1.89   -4.38  -6.91\n#&gt; 10  1944     -6.20    2.47   -8.67 -13.7 \n#&gt; 11  1945     -0.359   3.26   -3.62  -5.70\n#&gt; 12  1948      8.27    6.05    2.22   3.50",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html#weird-series",
    "href": "09-timeseries.html#weird-series",
    "title": "9  Time series data",
    "section": "9.3 Weird series",
    "text": "9.3 Weird series\nNext we consider unusual series within a large collection of time series.\nTo illustrate the ideas we will use a data set of monthly observations on the Australian Pharmaceutical Benefits Scheme (PBS), from July 1991 to June 2008. The PBS involves the Australian government subsidising certain pharmaceutical products, to allow more equitable access to essential medicines. The data set contains monthly sales volumes of those products being subsidised, classified according to the Anatomical Therapeutic Chemical (ATC) classification system.\n\n## Compute features (and omit series with missing features)\nPBS_feat &lt;- PBS |&gt;\n  features(Cost, feature_set(pkgs = \"feasts\")) |&gt;\n  select(-`...26`) |&gt;\n  na.omit()\n\n#&gt; Error in ar.burg.default(x, aic = aic, order.max = order.max, na.action = na.action,  : \n#&gt;   zero-variance series\n#&gt; Error in ar.burg.default(x, aic = aic, order.max = order.max, na.action = na.action,  : \n#&gt;   zero-variance series\n\n## Compute principal components\nPBS_prcomp &lt;- PBS_feat |&gt;\n  select(-Concession, -Type, -ATC1, -ATC2) |&gt;\n  prcomp(scale = TRUE) |&gt;\n  augment(PBS_feat)\n\n## Plot the first two components\nPBS_prcomp |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point()\n\n\n\n\n\n\n\n## Pull out most unusual series from first principal component\noutliers &lt;- PBS_prcomp |&gt;\n  filter(.fittedPC1 &gt; 6)\noutliers |&gt;\n  select(ATC1, ATC2, Type, Concession)\n\n#&gt; # A tibble: 4 × 4\n#&gt;   ATC1  ATC2  Type        Concession  \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;       \n#&gt; 1 J     J06   Safety net  Concessional\n#&gt; 2 C     C05   Co-payments General     \n#&gt; 3 S     S02   Co-payments General     \n#&gt; 4 S     S03   Co-payments General\n\n## Visualise the unusual series\nPBS |&gt;\n  semi_join(outliers, by = c(\"Concession\", \"Type\", \"ATC1\", \"ATC2\")) |&gt;\n  autoplot(Cost) +\n  facet_grid(vars(Concession, Type, ATC1, ATC2)) +\n  labs(title = \"Outlying time series in PC space\")\n\n\n\n\n\n\n\n\nThese series are either all zeros, or have a single non-zero observation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html#surveillance",
    "href": "09-timeseries.html#surveillance",
    "title": "9  Time series data",
    "section": "9.4 Surveillance",
    "text": "9.4 Surveillance\nIn surveillance, we are interested in identifying anomalies in real time. This is common in monitoring systems, where we want to identify unusual behaviour as soon as it occurs. For example, in a manufacturing plant, we may want to identify when a machine is not operating as expected, and take action to prevent further problems. In retail, we may want to identify when sales are unusually high or low, and adjust stock levels if necessary.\nAgain, we will use the PBS data. For this example, we will combine the data into ATC level 2 groups, and look at total sales volumes (measured in thousands of scripts).\n\n\nCode\npbs &lt;- PBS |&gt;\n  arrange(ATC2, Month) |&gt;\n  group_by(ATC2) |&gt;\n  summarise(Scripts = sum(Scripts) / 1e3, .groups = \"drop\") |&gt;\n  mutate(t = as.numeric(Month - min(Month) + 1))\n\n\n\npbs\n\n#&gt; # A tsibble: 17,016 x 4 [1M]\n#&gt; # Key:       ATC2 [84]\n#&gt;    ATC2     Month Scripts     t\n#&gt;    &lt;chr&gt;    &lt;mth&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 A01   1991 Jul    22.6     1\n#&gt;  2 A01   1991 Aug    20.4     2\n#&gt;  3 A01   1991 Sep    21.4     3\n#&gt;  4 A01   1991 Oct    23.7     4\n#&gt;  5 A01   1991 Nov    23.5     5\n#&gt;  6 A01   1991 Dec    26.3     6\n#&gt;  7 A01   1992 Jan    22.0     7\n#&gt;  8 A01   1992 Feb    16.4     8\n#&gt;  9 A01   1992 Mar    17.2     9\n#&gt; 10 A01   1992 Apr    18.8    10\n#&gt; # ℹ 17,006 more rows\n\n\nThe data set is a tsibble object, which is a special type of tibble designed for time series data. Two columns have special purposes: the Month column is the time index, defining when each observation was made; and the ATC2 column is a key variable, defining the separate time series in the data set. This data set contains 84 separate time series, one for each ATC2 value.\nLet’s first look at the time series for just one ATC group: A12 (Mineral supplements). In fact, this is the same series as was shown in the top panel of Figure 9.1.\n\n\nCode\npbs |&gt;\n  filter(ATC2 == \"A12\") |&gt;\n  autoplot(Scripts) +\n  labs(title = \"Scripts for ATC group A12 (Mineral supplements)\")\n\n\n\n\n\n\n\n\nFigure 9.5: The monthly script volume for mineral supplements (ATC group A12) on the Australian PBS.\n\n\n\n\n\nHere there has been a sudden drop in sales at the end of 2005, most likely because of some products no longer being eligible for subsidy. There can also be sudden jumps in sales when a new product becomes available, or a new class of drugs is added to the scheme.\nThe goal of surveillance is to identify these anomalies as soon as they occur. We can do this by fitting a model to the data, and then comparing the observed values to the model’s forecasts. If the observed values are very different from the forecast, then we have an anomaly.\nStatistical forecasting models provide forecasts in the form of probability distributions. Let y_t denote the observation of a time series at time t. Then a forecast can be expressed as a conditional distribution \n  f(y_{t+h} | y_1, \\dots, y_{t}, \\bm{x}_t),\n where y_1,\\dots,y_t denotes the observed history of the series, and \\bm{x}_t contains any other information available at time t that is used in the model. The forecast “horizon” is given by h, denoting the number of time periods into the future that we wish to forecast. Different forecasting methods use different conditioning information, and result in a different form of the forecast distribution. See Hyndman and Athanasopoulos (2021) for a detailed discussion of forecasting methods.\nOnce we observe the value of y_{t+h}, we can calculate the corresponding density scores in the same way as we discussed in Section 6.5.2. Because we are interested in real-time surveillance, we need only consider the one-step-ahead forecast density, f(y_{t+1} | y_1, \\dots, y_{t}). We can then calculate the density score as \n  s_{t+1} = -\\log \\hat{f}(y_{t+1} | y_1, \\dots, y_{t}, \\bm{x}_t).\n This needs to be done iteratively for each time period, updating the model as new data becomes available. Since we need some observations with which to fit a model, we can’t begin the process at time t=1. Instead, we’ll begin at time t=I, where I is the smallest number of observations with which we can reasonably estimate the time series model. The value of I will depend on the complexity of the model being used. For simple models with few parameters, we may be able to set I to around 20, but for complex models with many parameters, I may need to be much larger.\nWe also can’t estimate the Generalized Pareto Distribution until we have computed sufficient density scores. Fortunately, we are often working with many time series, not just one, and we can use the density scores from all series when computing the GPD. Suppose we have m series we are monitoring, then at time t, we will have computed m(t-I) anomaly scores. Usually we would need at least a few hundred anomaly scores before we could reasonably estimate a GPD from the top 10% of the available scores. Suppose we required at least 200 scores, then we could only compute a GPD once t \\ge J where J = 200/m+I.\nWe can summarise the anomaly detection algorithm for surveillance as follows. First, we’ll change the notation slightly to allow for more than one series. Let y_{i,t} denote the observation of the ith series at time t.\nFor each t = I,I+1,\\dots, and for all series i=1,\\dots,m:\n\nFit a time series model to the series y_{i,1},\\dots,y_{i,t}, and estimate the one-step forecast density, f_{i,t+1}(y \\mid y_{i,1},\\dots,y_{i,t}, \\bm{x}_t).\nCompute the anomaly score: s_{i,t+1} = -\\log\\hat{f}_{i,t+1}(y_{i,t+1}\\mid y_{i,1},\\dots,y_{i,t}, \\bm{x}_t).\nIf t\\ge J, fit a Generalized Pareto Distribution S to the top 10% of anomaly scores \\{s_{i,t}\\}, i=1,\\dots,m, t=I+1,I+2,\\dots,t.\nDesignate y_{i,t+1} as an anomaly if P(S &gt; s_{i,t+1}) &lt; 0.05 under the GPD.\n\nTo illustrate, let’s apply this to the pbs data. Starting with I=36 (3 years of data), we will fit ETS models to all available series (Ch 8, Hyndman and Athanasopoulos 2021), and forecast one step ahead in each case. Then we repeat the exercise using 37 observations, then 38 observations, and so on. This is known as a “rolling origin forecast” because the forecast rolls forward by one period each iteration. The process can be illustrated as in Figure 9.6.\n\n\n\n\n\n\n\n\nFigure 9.6: Rolling origin forecasts, with the training sets expanding by one observation at each iteration, and one-step forecasts computed for each training set.\n\n\n\n\n\nLet’s step through the process for the first iteration, with t=36. We fit ETS models to each of the time series, with the specific ETS model selected according to the characteristics of the series. See (Ch8, Hyndman and Athanasopoulos 2021) for details of how this is done.\n\npbs_fit &lt;- pbs |&gt;\n  filter(t &lt;= 36) |&gt;\n  model(ets = ETS(Scripts))\npbs_fit\n\n#&gt; # A mable: 83 x 2\n#&gt; # Key:     ATC2 [83]\n#&gt;    ATC2           ets\n#&gt;    &lt;chr&gt;      &lt;model&gt;\n#&gt;  1 A01   &lt;ETS(M,N,A)&gt;\n#&gt;  2 A02   &lt;ETS(M,A,M)&gt;\n#&gt;  3 A03   &lt;ETS(M,A,M)&gt;\n#&gt;  4 A04   &lt;ETS(M,N,A)&gt;\n#&gt;  5 A06   &lt;ETS(M,A,M)&gt;\n#&gt;  6 A07   &lt;ETS(M,N,M)&gt;\n#&gt;  7 A09   &lt;ETS(M,A,M)&gt;\n#&gt;  8 A10   &lt;ETS(M,A,M)&gt;\n#&gt;  9 A11   &lt;ETS(M,A,M)&gt;\n#&gt; 10 A12   &lt;ETS(M,N,A)&gt;\n#&gt; # ℹ 73 more rows\n\n\nForecasts are produced from all fitted models by applying the forecast() function to the model table.\n\npbs_fc &lt;- forecast(pbs_fit, h = 1)\npbs_fc\n\n#&gt; # A fable: 83 x 5 [1M]\n#&gt; # Key:     ATC2, .model [83]\n#&gt;    ATC2  .model    Month\n#&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;mth&gt;\n#&gt;  1 A01   ets    1994 Jul\n#&gt;  2 A02   ets    1994 Jul\n#&gt;  3 A03   ets    1994 Jul\n#&gt;  4 A04   ets    1994 Jul\n#&gt;  5 A06   ets    1994 Jul\n#&gt;  6 A07   ets    1994 Jul\n#&gt;  7 A09   ets    1994 Jul\n#&gt;  8 A10   ets    1994 Jul\n#&gt;  9 A11   ets    1994 Jul\n#&gt; 10 A12   ets    1994 Jul\n#&gt; # ℹ 73 more rows\n#&gt; # ℹ 2 more variables: Scripts &lt;dist&gt;, .mean &lt;dbl&gt;\n\n\nThe forecasts are in the Scripts column. Note that each of them is a Normal distribution where the mean and variance has been estimated using the ETS model. The mean of the forecast distribution is given as .mean.\nWe can compute the density scores using the log_likelihood() function from the distributional package, and calculate the lookout probabilities.\n\npbs_scores &lt;- pbs_fc |&gt;\n  rename(fcast = Scripts) |&gt;\n  left_join(pbs |&gt; filter(t == 37), by = c(\"Month\", \"ATC2\")) |&gt;\n  mutate(\n    s = -distributional::log_likelihood(fcast, Scripts),\n    prob = surprisals(s)\n  )\npbs_scores\n\n#&gt; # A tsibble: 83 x 9 [?]\n#&gt; # Key:       ATC2, .model [83]\n#&gt;    ATC2  .model    Month\n#&gt;    &lt;chr&gt; &lt;chr&gt;     &lt;mth&gt;\n#&gt;  1 A01   ets    1994 Jul\n#&gt;  2 A02   ets    1994 Jul\n#&gt;  3 A03   ets    1994 Jul\n#&gt;  4 A04   ets    1994 Jul\n#&gt;  5 A06   ets    1994 Jul\n#&gt;  6 A07   ets    1994 Jul\n#&gt;  7 A09   ets    1994 Jul\n#&gt;  8 A10   ets    1994 Jul\n#&gt;  9 A11   ets    1994 Jul\n#&gt; 10 A12   ets    1994 Jul\n#&gt; # ℹ 73 more rows\n#&gt; # ℹ 6 more variables: fcast &lt;dist&gt;, .mean &lt;dbl&gt;, Scripts &lt;dbl&gt;, t &lt;dbl&gt;,\n#&gt; #   s &lt;dbl&gt;, prob &lt;dbl&gt;\n\n\nThis process is repeated as we increase the size of the training set. The following code block carries out the calculations in a loop. Before starting the calculations, we will set up some new columns in which to store the forecasts, scores and probabilities at each iteration.\n\ncompute_pbs_scores &lt;- function(pbs) {\n  pbs &lt;- pbs |&gt;\n    mutate(s = NA_real_, prob = NA_real_, fmean = NA_real_, fvar = NA_real_)\n\n  for (i in 36:(max(pbs$t) - 1)) {\n    # Fit ETS model to all series and compute 1-step forecasts\n    pbs_fc &lt;- pbs |&gt;\n      filter(t &lt;= i) |&gt;\n      model(ets = ETS(Scripts)) |&gt;\n      forecast(h = 1) |&gt;\n      rename(fcast = Scripts) |&gt;\n      filter(!is.na(.mean))\n    # Calculate anomaly scores and probabilities\n    pbs_scores &lt;- pbs |&gt;\n      filter(t == i + 1) |&gt;\n      right_join(pbs_fc, by = c(\"Month\", \"ATC2\")) |&gt;\n      mutate(\n        newmean = mean(fcast),\n        newvar = distributional::variance(fcast),\n        news = -distributional::log_likelihood(fcast, Scripts),\n        newprob = surprisals(news)\n      ) |&gt;\n      select(Month, ATC2, news, newprob, newmean, newvar)\n    # Add scores to pbs data set\n    pbs &lt;- pbs |&gt;\n      left_join(pbs_scores, by = c(\"Month\", \"ATC2\")) |&gt;\n      mutate(\n        fmean = if_else(!is.na(newmean), newmean, fmean),\n        fvar = if_else(!is.na(newvar), newvar, fvar),\n        s = if_else(!is.na(news), news, s),\n        prob = if_else(!is.na(newprob), newprob, prob)\n      ) |&gt;\n      select(-news, -newprob, -newmean, -newvar)\n  }\n  return(pbs)\n}\npbs_scores &lt;- compute_pbs_scores(pbs) |&gt; cache(\"pbs_scores\")\n\n\n\nCode\npbs_scores\n\n\n#&gt; # A tsibble: 17,016 x 8 [1M]\n#&gt; # Key:       ATC2 [84]\n#&gt;    ATC2     Month Scripts     t     s  prob fmean  fvar\n#&gt;    &lt;chr&gt;    &lt;mth&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 A01   1991 Jul    22.6     1    NA    NA    NA    NA\n#&gt;  2 A01   1991 Aug    20.4     2    NA    NA    NA    NA\n#&gt;  3 A01   1991 Sep    21.4     3    NA    NA    NA    NA\n#&gt;  4 A01   1991 Oct    23.7     4    NA    NA    NA    NA\n#&gt;  5 A01   1991 Nov    23.5     5    NA    NA    NA    NA\n#&gt;  6 A01   1991 Dec    26.3     6    NA    NA    NA    NA\n#&gt;  7 A01   1992 Jan    22.0     7    NA    NA    NA    NA\n#&gt;  8 A01   1992 Feb    16.4     8    NA    NA    NA    NA\n#&gt;  9 A01   1992 Mar    17.2     9    NA    NA    NA    NA\n#&gt; 10 A01   1992 Apr    18.8    10    NA    NA    NA    NA\n#&gt; # ℹ 17,006 more rows\n\n\nWe compare the distribution against the observation for February 2006.\n\n\nCode\nobserved &lt;- pbs |&gt; filter(ATC2 == \"A12\", Month == yearmonth(\"2006 Feb\"))\nfc_a12 |&gt;\n  autoplot(a12) +\n  geom_point(data = observed, aes(x = Month, y = Scripts)) +\n  labs(\n    y = \"Scripts (thousands)\",\n    title = \"Forecast of A12 scripts: Feb 2006\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(35, 145)\n\n\n\n\nCode\npbs_anomalies &lt;- pbs |&gt; filter(prob &lt; 0.05)\npbs_anomalies\n\n\n\n\nCode\npbs_plot(pbs, pbs_anomalies, \"L03\")\n\n\n\n\nCode\npbs_plot(pbs, pbs_anomalies, \"N07\")\n\n\nConsecutive anomalies are hard to identify because the preceding anomalies corrupt the model.\n\n\nCode\npbs_plot(pbs, pbs_anomalies, \"R06\")\n\n\nA sequence of near anomalies makes it hard to spot a true anomaly.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "09-timeseries.html#somewhere",
    "href": "09-timeseries.html#somewhere",
    "title": "9  Time series data",
    "section": "9.5 Somewhere",
    "text": "9.5 Somewhere\n\nAO vs IO\ntsoutliers package and tsoutliers() function\nsmooth() function\n\n\n\n\n\n\nBandara, Kasun, Rob J Hyndman, and Christoph Bergmeir. 2022. “MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns.” International J Operational Research. http://robjhyndman.com/publications/mstl/.\n\n\nCleveland, R B, W S Cleveland, J E McRae, and I J Terpenning. 1990. “STL: A Seasonal-Trend Decomposition Procedure Based on Loess.” Journal of Official Statistics 6 (1): 3–33. http://bit.ly/stl1990.\n\n\nDavies, Laurie, and Ursula Gather. 1993. “The Identification of Multiple Outliers.” Journal of the American Statistical Association 88 (September): 782–92. https://doi.org/10.1080/01621459.1993.10476339.\n\n\nFriedman, J H. 1984. “A Variable Span Smoother.” Technical Report No 5. Laboratory for Computational Statistics, Stanford University.\n\n\nHuman Mortality Database. 2024. “University of California, Berkeley (USA); Max Planck Institute for Demographic Research (Germany).” www.mortality.org.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. Melbourne, Australia: OTexts. http://OTexts.org/fpp3.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Time series data</span>"
    ]
  },
  {
    "objectID": "10-functional.html",
    "href": "10-functional.html",
    "title": "10  Functional data",
    "section": "",
    "text": "# Wide version of log Mortality with ages on columns\nfrmort_wide &lt;- fr_mortality |&gt;\n  filter(Sex == \"Male\") |&gt;\n  mutate(logmx = log(Mortality)) |&gt;\n  select(-Mortality, -Sex) |&gt;\n  tidyr::pivot_wider(names_from = Age, values_from = logmx, names_prefix = \"Age\")\n\n# Compute first four principal components\npca &lt;- frmort_wide |&gt;\n  select(-Year) |&gt;\n  prcomp(center = TRUE, scale = FALSE, rank = 4) |&gt;\n  broom::augment(frmort_wide[, \"Year\"]) |&gt;\n  select(-.rownames)\n\n# Time series of first four PCs\npca |&gt;\n  tidyr::pivot_longer(starts_with(\".fittedPC\"),\n    names_to = \"PC\", values_to = \"value\", names_prefix = \".fittedPC\"\n  ) |&gt;\n  ggplot(aes(x = Year, y = value)) +\n  geom_line(aes(colour = PC))\n\n\n\n\n\n\n\n# Scatterplot of first two PCs\npca |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point()\n\n\n\n\n\n\n\n# Find outliers in the PCs\npca_no_year &lt;- pca |&gt; select(-Year)\npca &lt;- pca |&gt;\n  mutate(lookout = surprisals(pca_no_year))\noutliers &lt;- pca |&gt; filter(lookout &lt; 0.05)\npca |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point() +\n  geom_point(data = outliers, color = \"red\") +\n  ggrepel::geom_label_repel(data = outliers, aes(label = Year), )",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Functional data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Arya, S, D M Mount, N S Netanyahu, R Silverman, and A Y Wu. 1998.\n“An Optimal Algorithm for Approximate Nearest Neighbor Searching\nFixed Dimensions.” Journal of the ACM 45 (6): 891–923.\nhttps://doi.org/10.1145/293347.293348.\n\n\nBandara, Kasun, Rob J Hyndman, and Christoph Bergmeir. 2022.\n“MSTL: A Seasonal-Trend Decomposition Algorithm for\nTime Series with Multiple Seasonal Patterns.” International J\nOperational Research. http://robjhyndman.com/publications/mstl/.\n\n\nBarbato, G, E M Barini, G Genta, and R Levi. 2011. “Features and\nPerformance of Some Outlier Detection Methods.” Journal of\nApplied Statistics, no. 922191616: 1–17. https://doi.org/10.1080/02664763.2010.545119.\n\n\nBarnett, V, and T Lewis. 1978. Outliers in Statistical Data.\nJohn Wiley & Sons.\n\n\nBentley, J L. 1975. “Multidimensional Binary Search Trees Used for\nAssociative Searching.” Communications of the ACM 18\n(9): 509–17. https://doi.org/10.1145/361002.361007.\n\n\nBillor, N, A S Hadi, and P F Velleman. 2000. “BACON:\nBlocked Adaptive Computationally Efficient Outlier Nominators.”\nComputational Statistics & Data Analysis 34 (3): 279–98. https://doi.org/10.1016/S0167-9473(99)00101-2.\n\n\nBorg, I, and P J F Groenen. 2005. Modern Multidimensional Scaling:\nTheory and Applications. 2nd ed. Springer Texts in Statistics. New\nYork: Springer. https://doi.org/10.1007/0-387-28981-X.\n\n\nBreunig, M M, H-P Kriegel, R T Ng, and J Sander. 2000. “LOF:\nIdentifying Density-Based Local Outliers.” ACM SIGMOD\nRecord 29 (2): 93–104. https://doi.org/10.1145/335191.335388.\n\n\nBrys, G, M Hubert, and P J Rousseeuw. 2005. “A Robustification of\nIndependent Component Analysis.” Journal of Chemometrics\n19 (5-7): 364–75. https://doi.org/10.1002/cem.940.\n\n\nChacón, J E, and T Duong. 2018. Multivariate Kernel Smoothing and\nIts Applications. Boca Raton, Florida: CRC Press.\n\n\nChauvenet, W. 1863. “Appendix: Method of Least Squares.” In\nA Manual of Spherical and Practical Astronomy, II Theory and\nuse of astronomical instruments:469–566. Philadelphia: Lippincott.\n\n\nChristie, M. 2001. The Ozone Layer: A Philosophy of Science\nPerspective. Cambridge, UK: Cambridge University Press.\n\n\n———. 2004. “Data Collection and the Ozone Hole: Too Much of a Good\nThing?” History of Meteorology 1: 99–105.\n\n\nCleveland, R B, W S Cleveland, J E McRae, and I J Terpenning. 1990.\n“STL: A Seasonal-Trend Decomposition Procedure Based\non Loess.” Journal of Official Statistics 6 (1): 3–33.\nhttp://bit.ly/stl1990.\n\n\nColes, S. 2001. An Introduction to Statistical Modeling of Extreme\nValues. Springer Series in Statistics. London UK: Springer.\n\n\nCover, Thomas M, and Joy A Thomas. 2006. Elements of Information\nTheory. 2nd ed. Hoboken, NJ: Wiley.\n\n\nDavies, Laurie, and Ursula Gather. 1993. “The Identification of\nMultiple Outliers.” Journal of the American Statistical\nAssociation 88 (September): 782–92. https://doi.org/10.1080/01621459.1993.10476339.\n\n\nDixon, W J. 1950. “Analysis of Extreme Values.” Annals\nof Mathematical Statistics 21 (4): 488–506. https://projecteuclid.org/euclid.aoms/1177729747.\n\n\nDonoho, D L. 1982. “Breakdown Properties of Multivariate Location\nEstimators.”\n\n\nFaraway, J J. 2014. Linear Models with r. 2nd ed. Chapman;\nHall/CRC.\n\n\nFarman, J C, B G Gardiner, and J D Shanklin. 1985. “Large Losses\nof Total Ozone in Antarctica Reveal Seasonal ClOx/NOx\nInteraction.” Nature 315 (6016): 207–10.\n\n\nFilzmoser, Peter, and Valentin Todorov. 2013. “Robust Tools for\nthe Imperfect World.” Information Sciences 245\n(October): 4–20. https://doi.org/10.1016/j.ins.2012.10.017.\n\n\nFilzmoser, P, R Maronna, and M Werner. 2008. “Outlier\nIdentification in High Dimensions.” Computational Statistics\n& Data Analysis 52 (3): 1694–1711. https://doi.org/10.1016/j.csda.2007.05.018.\n\n\nFisher, R A, and L H C Tippett. 1928. “Limiting Forms of the\nFrequency Distribution of the Largest or Smallest Member of a\nSample.” Mathematical Proceedings of the Cambridge\nPhilosophical Society 24 (2): 180–90.\n\n\nFix, E, and J L Hodges. 1989. “Discriminatory Analysis —\nNonparametric Discrimination: Consistency Properties.”\nInternational Statistical Review 57 (3): 238–47.\n\n\nForbes, C S, M Evans, N Hastings, and B Peacock. 2011. Statistical\nDistributions. 4th ed. Hoboken, NJ: Wiley-Blackwell. https://doi.org/10.1002/9780470627242.\n\n\nFréchet, M. 1927. “Sur La Loi de Probabilité de l’écart\nMaximum.” Annales de La Société Polonaise de\nMathématique 6: 93–116.\n\n\nFriedman, J H. 1984. “A Variable Span Smoother.” Technical\nReport No 5. Laboratory for Computational Statistics, Stanford\nUniversity.\n\n\nGentle, J E. 2007. Matrix Algebra: Theory, Computations, and\nApplications in Statistics. Springer.\n\n\nGnanadesikan, R., and J. R. Kettenring. 1972. “Robust Estimates,\nResiduals, and Outlier Detection with Multiresponse Data.”\nBiometrics 28 (1): 81–124.\n\n\nGnedenko, B. 1943. “Sur La Distribution Limite Du Terme Maximum\nd’une Serie Aleatoire.” Annals of Mathematics, 423–53.\n\n\nGneiting, T, and M Katzfuss. 2014. “Probabilistic\nForecasting.” Annual Review of Statistics and Its\nApplication 1 (1): 125–51. https://doi.org/10.1146/annurev-statistics-062713-085831.\n\n\nGrubbs, F E. 1950. “Sample Criteria for Testing Outlying\nObservations.” Annals of Mathematical Statistics 21 (1):\n27–58. https://projecteuclid.org/euclid.aoms/1177729885.\n\n\nHawkins, D M. 1980. Identification of Outliers. Springer.\n\n\nHoaglin, D C. 1983. “Letter Values: A Set of Selected Order\nStatistics.” In Understanding Robust and Exploratory Data\nAnalysis, edited by David C Hoaglin and Frederick Mosteller, 33–57.\nNew York: John Wiley.\n\n\nHofmann, H, H Wickham, and K Kafadar. 2017.\n“Letter-Value Plots: Boxplots for Large Data.”\nJournal of Computational & Ggraphical Statistics 26 (3):\n469–77. https://doi.org/10.1080/10618600.2017.1305277.\n\n\nHuman Mortality Database. 2024. “University\nof California, Berkeley (USA); Max Planck Institute for Demographic\nResearch (Germany).” www.mortality.org.\n\n\nHyndman, R J. 1996. “Computing and Graphing Highest Density\nRegions.” The American Statistician 50 (2): 120–26. http://www.jstor.org/stable/2684423.\n\n\nHyndman, R J, and Y Fan. 1996. “Sample Quantiles in Statistical\nPackages.” The American Statistician 50 (4): 361–65.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting:\nPrinciples and Practice. 3rd ed. Melbourne, Australia: OTexts. http://OTexts.org/fpp3.\n\n\nJohnson, N L, S Kotz, and N Balakrishnan. 1994. Continuous\nUnivariate Distributions. 2nd ed. Vol. 1. New York, USA: John Wiley\n& Sons.\n\n\n———. 1995. Continuous Univariate Distributions. 2nd ed. Vol. 2.\nNew York, USA: John Wiley & Sons.\n\n\nKandanaarachchi, S, and R J Hyndman. 2020. “Dimension Reduction\nfor Outlier Detection Using DOBIN.” Journal of\nComputational & Graphical Statistics 30 (1): 204–19. https://doi.org/10.1080/10618600.2020.1807353.\n\n\n———. 2022. “Leave-One-Out Kernel Density Estimates for Outlier\nDetection.” J Computational & Graphical Statistics\n31 (2): 586–99. https://doi.org/10.1080/10618600.2021.2000425.\n\n\nKotz, S, N Balakrishnan, and N L Johnson. 2000. Continuous\nMultivariate Distributions: Models and Applications. 2nd ed. Vol.\n1. New York, USA: John Wiley & Sons.\n\n\nLiu, F T, K M Ting, and Z-H Zhou. 2008. “Isolation Forest.”\nIn 2008 Eighth IEEE International Conference on Data\nMining, 413–22. IEEE. https://doi.org/10.1109/ICDM.2008.17.\n\n\nLiu, R Y, J M Parelius, and K Singh. 1999. “Multivariate Analysis\nby Data Depth: Descriptive Statistics, Graphics and Inference.”\nThe Annals of Statistics 27 (3): 783–858. https://doi.org/10.1214/aos/1018031260.\n\n\nMaechler, M, P Rousseeuw, C Croux, V Todorov, A Ruckstuhl, M\nSalibian-Barrera, T Verbeke, M Koller, E L T Conceicao, and M A di\nPalma. 2023. robustbase: Basic Robust\nStatistics. http://robustbase.r-forge.r-project.org/.\n\n\nMaronna, R. A., and R. H. Zamar. 2002. “Robust Estimates of\nLocation and Dispersion of High-Dimensional Datasets.”\nTechnometrics 44 (4): 307–17.\n\n\nMontgomery, Douglas C, Elizabeth A Peck, and G Geoffrey Vining. 2012.\nIntroduction to Linear Regression Analysis. 5th ed. John Wiley\n& Sons.\n\n\nPearson, E S, and C C Sekar. 1936. “The Efficiency of Statistical\nTools and a Criterion for the Rejection of Outlying\nObservations.” Biometrika 28 (3/4): 308–20. http://www.jstor.org/stable/2333954.\n\n\nPeirce, B. 1852. “Criterion for the Rejection of Doubtful\nObservations.” The Astronomical Journal 2 (21): 161–63.\nhttp://adsabs.harvard.edu/pdf/1852AJ......2..161P.\n\n\nPukelsheim, F. 1990. “Robustness of Statistical Gossip and the\nAntarctic Ozone Hole.” The IMS Bulletin 19\n(4): 540–45.\n\n\nRousseeuw, P J, and I Ruts. 1998. “Constructing the Bivariate\nTukey Median.” Statistica Sinica 8: 827–39.\n\n\nRousseeuw, P J, I Ruts, and J W Tukey. 1999. “The Bagplot: A\nBivariate Boxplot.” The American Statistician 52 (4):\n382–87.\n\n\nRousseeuw, P J, and A Struyf. 1998. “Computing Location Depth and\nRegression Depth in Higher Dimensions.” Statistics and\nComputing 8 (3): 193–203.\n\n\nRousseeuw, P J, and W Van den Bossche. 2016. “Detecting Deviating\nData Cells,” January. http://arxiv.org/abs/1601.07251.\n\n\nRousseeuw, P J, and K Van Driessen. 1999. “A Fast Algorithm for\nthe Minimum Covariance Determinant Estimator.” Technometrics:\nA Journal of Statistics for the Physical, Chemical, and Engineering\nSciences 41 (3): 212–23. https://doi.org/10.1080/00401706.1999.10485670.\n\n\nRousseeuw, Peter J, and Christophe Croux. 1993. “Alternatives to\nthe Median Absolute Deviation.” Journal of the American\nStatistical Association 88: 1273–83. https://doi.org/10.1080/01621459.1993.10476408.\n\n\nScott, D W. 2015. Multivariate Density Estimation: Theory, Practice,\nand Visualization. 2nd ed. Wiley.\n\n\nSearle, S R. 2006. Matrix Algebra Useful for Statistics. 2nd\ned. Wiley.\n\n\nSeber, George A F, and Alan J Lee. 2003. Linear Regression\nAnalysis. 2nd ed. John Wiley & Sons.\n\n\nSheather, S J, and M C Jones. 1991. “A Reliable Data-Based\nBandwidth Selection Method for Kernel Density Estimation.”\nJournal of the Royal Statistical Society. Series B 53 (3):\n683–90.\n\n\nSilverman, B W. 1986. Density Estimation for Statistics and Data\nAnalysis. Chapman; Hall.\n\n\nStahel, W A. 1981. “Robust Estimation: Infinitisimal Optimality\nand Covariance Matrix Estimators.” PhD, ETH Zurich.\n\n\nStolarski, R S, A J Krueger, M R Schoeberl, R D McPeters, P A Newman,\nand J C Alpert. 1986. “Nimbus 7 Satellite Measurements of the\nSpringtime Antarctic Ozone Decrease.”\nNature 322 (6082): 808–11.\n\n\nStone, James V. 2022. Information Theory: A Tutorial\nIntroduction. Sheffield, England: Sebtel Press.\n\n\nTalagala, P D, R J Hyndman, and K Smith-Miles. 2019. “Anomaly\nDetection in High Dimensional Data.” arXiv Preprint\narXiv. https://arxiv.org/abs/1908.04000.\n\n\nTribus, Myron. 1961. Thermodynamics and Thermostatics: An\nIntroduction to Energy, Information and States of Matter, with\nEngineering Applications. New York, USA: D. Van Nostrand.\n\n\nTukey, J W. 1975. “Mathematics and the Picturing of Data.”\nIn Proceedings of the International Congress of Mathematicians,\n2:523–31.\n\n\n———. 1977. Exploratory Data Analysis. Addison-Wesley.\n\n\nVakili, K, and E Schmitt. 2014. “Finding Multivariate Outliers\nwith FastPCS.” Computational Statistics &\nData Analysis 69 (January): 54–66. https://doi.org/10.1016/j.csda.2013.07.021.\n\n\nVan der Vaart, A W. 2000. Asymptotic Statistics. Cambridge\nUniversity Press.\n\n\nvon Mises, R. 1936. “La Distribution de La Plus Grande de n\nValeurs.” Rev. Math. Union Interbalcanique 1: 141–60.\n\n\nWand, M P, and M C Jones. 1995. Kernel Smoothing. New York:\nChapman & Hall.\n\n\nWasserman, L. 2004. All of Statistics: A Concise Course in\nStatistical Inference. Springer Texts in Statistics. New York:\nSpringer. https://doi.org/10.1007/978-0-387-21736-9.\n\n\nWickham, H, and L Stryjewski. 2011. “40 Years of Boxplots.”\nhttps://vita.had.co.nz/papers/boxplots.html.\n\n\nWilkinson, L. 2017. “Visualizing Big Data Outliers Through\nDistributed Aggregation.” IEEE Transactions on Visualization\nand Computer Graphics, August. https://doi.org/10.1109/TVCG.2017.2744685.\n\n\nWood, S N. 2017. Generalized Additive Models: An Introduction with\nR. CRC press.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Matrix algebra",
    "section": "",
    "text": "A.1 Identity matrix\nWorking with multivariate data inevitably involves writing some of the ideas using matrix expressions and matrix algebra. In this appendix, a few of the results that we need are collected for convenient reference. Readers who have not previously studied matrix algebra are recommended to do some self-study using a good introductory textbook such as Gentle (2007) or Searle (2006).\nThe identity matrix is like the number 1: multiplying a matrix by the identity matrix leaves the matrix unchanged. The identity matrix is a square matrix with ones on the diagonal and zeros elsewhere. The identity matrix is often denoted by \\bm{I}. Sometimes we will use subscripts to denote the size of the identity matrix; e.g. \\bm{I}_n is an n \\times n identity matrix.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#matrix-inverse",
    "href": "appendix.html#matrix-inverse",
    "title": "Appendix A — Matrix algebra",
    "section": "A.2 Matrix inverse",
    "text": "A.2 Matrix inverse\nFor a square matrix \\bm{A}, the inverse \\bm{A}^{-1} is defined such that \\bm{A}^{-1}\\bm{A} = \\bm{I}, where \\bm{I} is the identity matrix. The inverse of a matrix can be computed using the solve function in R. Matrices that are not square do not have inverses. Matrices that are square but are not invertible are called singular matrices.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#determinants",
    "href": "appendix.html#determinants",
    "title": "Appendix A — Matrix algebra",
    "section": "A.3 Determinants",
    "text": "A.3 Determinants\nThe determinant of a square matrix \\bm{A} is a scalar value denoted by |\\bm{A}| or \\text{det}(\\bm{A}). It describes the “scale” of the matrix, and is used when computing the inverse of a matrix, and in some other operations. The formula for a determinant is usually computed recursively, using the Laplace expansion, as follows: \n|\\bm{A}| = \\sum_{j=1}^n (-1)^{i+j} a_{ij} |\\bm{A}_{ij}|,\n where a_{ij} is the element in the ith row and jth column of \\bm{A}, and \\bm{A}_{ij} is the matrix obtained by deleting the ith row and jth column of \\bm{A}. The determinant of a 1 \\times 1 matrix is just the value of the element. The determinant of a 2 \\times 2 matrix is computed as \n|\\bm{A}| = a_{11}a_{22} - a_{12}a_{21}.\n Singular matrices have a determinant of zero.\nThe determinant of a matrix can be computed using the det function in R.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#eigenvalues-and-eigenvectors",
    "href": "appendix.html#eigenvalues-and-eigenvectors",
    "title": "Appendix A — Matrix algebra",
    "section": "A.4 Eigenvalues and eigenvectors",
    "text": "A.4 Eigenvalues and eigenvectors\nA matrix \\bm{A} has an eigenvector \\bm{v} and eigenvalue \\lambda if \\bm{A}\\bm{v} = \\lambda \\bm{v}. Thus, eigenvectors can be thought of as “directions” that remain unchanged when the matrix is applied, and the eigenvalues is the amount by which the eigenvector is scaled.\nThe eigenvalues and eigenvectors of a matrix can be computed using the eigen function in R.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#positive-definite-matrices",
    "href": "appendix.html#positive-definite-matrices",
    "title": "Appendix A — Matrix algebra",
    "section": "A.5 Positive definite matrices",
    "text": "A.5 Positive definite matrices\nA positive-definite matrix is a special type of symmetric matrix where all the eigenvalues are positive. A positive-definite matrix is invertible and its inverse is also positive definite.\nA positive semi-definite matrix is a symmetric matrix where all the eigenvalues are non-negative. A positive semi-definite matrix is invertible if and only if it is positive definite.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#sec-cholesky",
    "href": "appendix.html#sec-cholesky",
    "title": "Appendix A — Matrix algebra",
    "section": "A.6 Cholesky decomposition",
    "text": "A.6 Cholesky decomposition\nA Cholesky decomposition is like a square root for a matrix. It is a decomposition of a positive semi-definite matrix \\bm{A} into a product \\bm{A} = \\bm{L}\\bm{L}^T, where \\bm{L} is a lower triangular matrix with non-negative diagonal entries. If \\bm{A} is positive definite, then the diagonal elements of \\bm{L} are positive.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  },
  {
    "objectID": "appendix.html#multivariate-random-variables",
    "href": "appendix.html#multivariate-random-variables",
    "title": "Appendix A — Matrix algebra",
    "section": "A.7 Multivariate random variables",
    "text": "A.7 Multivariate random variables\nRandom variables in multivariate space are usually written as vectors. The mean of a random vector is the vector of means of the components, and the covariance matrix is the matrix of covariances between the components. The covariance matrix is always symmetric and positive semi-definite.\n\n\n\n\nGentle, J E. 2007. Matrix Algebra: Theory, Computations, and Applications in Statistics. Springer.\n\n\nSearle, S R. 2006. Matrix Algebra Useful for Statistics. 2nd ed. Wiley.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Matrix algebra</span>"
    ]
  }
]